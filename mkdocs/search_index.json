{
    "docs": [
        {
            "location": "/", 
            "text": "Overview\n\n\nBullet ...\n\n\n\n\n\n\nIs a real-time query engine that lets you run queries on very large data streams\n\n\n\n\n\n\nDoes not use a \na persistence layer\n. This makes it \nlight-weight, cheap and fast\n\n\n\n\n\n\nIs a \nlook-forward\n query system. Queries are submitted first and they operate on data that arrive after the query is submitted\n\n\n\n\n\n\nIs \nmulti-tenant\n and can scale independently for more queries and for more data in the first order\n\n\n\n\n\n\nProvides a \nUI and Web Service\n that are also pluggable for a full end-to-end solution to your querying needs\n\n\n\n\n\n\nCan be implemented on different Stream processing frameworks. Bullet on \nStorm\n is currently available\n\n\n\n\n\n\nIs \npluggable\n. Any data source that can be read from Storm can be converted into a standard data container letting you query that data. Data is \ntyped\n\n\n\n\n\n\nIs used at scale and in production at Yahoo with hundreds of queries simultaneously on hundreds of thousands of records per second and tested up to millions of records per second\n\n\n\n\n\n\nHow is this useful\n\n\nHow Bullet is used is largely determined by the data source it consumes. Depending on what kind of data you put Bullet on, the types of queries you run on it and your use-cases will change. As a look-forward query system with no persistence, you will not be able to repeat your queries on the same data. The next time you run your query, it will operate on the different data that arrives after that submission. If this usage pattern is what you need and you are looking for a light-weight system that can tap into your streaming data, then Bullet is for you!\n\n\nHow Bullet is used at Yahoo\n\n\nBullet is used in production internally at Yahoo by having it sit on raw user engagement events from Yahoo sites and apps. This lets Yahoo developers automatically validate \nend-to-end\n their instrumentation code in their Continuous Delivery pipelines. Validating instrumentation is critical since it powers pretty much all decisions and products including machine learning, corporate KPIs, analytics, personalization, targeting.\n\n\nThis instance of Bullet also powers other use-cases such as letting analysts validate assumptions about data, product managers verify launches instantly, debug issues and outages, or simply explore and play around with the data.\n\n\n\n\nQuick Start\n\n\nSee \nQuick Start\n to set up Bullet on a local Storm topology. We will generate some fake streaming data that you can then query with Bullet.\n\n\nSetting up Bullet on your streaming data\n\n\nTo set up Bullet on a real data stream, you need:\n\n\n\n\nThe backend set up on a Stream processor:\n\n\nPlug in your source of data. See \nGetting your data into Bullet\n for details\n\n\nConsume your data stream. Currently, we support \nBullet on Storm\n\n\n\n\n\n\nThe \nWeb Service\n set up to convey queries and return results back from the backend\n\n\nThe optional \nUI\n set up to talk to your Web Service. You can skip the UI if all your access is programmatic\n\n\n\n\n\n\nSchema in the UI\n\n\nThe UI also needs an endpoint that provides your data schema to help with query building. The Web Service you set up provides a simple file based schema endpoint that you can point the UI to if that is sufficient for your needs.\n\n\n\n\n\n\nQuerying in Bullet\n\n\nBullet queries allow you to filter, project and aggregate data. It lets you fetch raw (the individual data records) as well as aggregated data.\n\n\nSee the \nUI Usage section\n for using the UI to build Bullet queries.\n\n\nSee the \nAPI section\n for building Bullet API queries.\n\n\nFor examples using the API, see \nExamples\n.\n\n\nTermination conditions\n\n\nA Bullet query terminates and returns whatever has been collected so far when:\n\n\n\n\nA maximum duration is reached. In other words, a query runs for a defined time window\n\n\nA maximum number of records is reached. (only applicable for queries that are fetching raw data records and not aggregating)\n\n\n\n\nFilters\n\n\nBullet supports two kinds of filters:\n\n\n\n\n\n\n\n\nFilter Type\n\n\nMeaning\n\n\n\n\n\n\n\n\n\n\nLogical filter\n\n\nAllow you to combine filter clauses (Logical or Relational) with logical operations like AND, OR and NOTs\n\n\n\n\n\n\nRelational filters\n\n\nAllow you to use comparison operations like equals, not equals, greater than, less than, regex like etc, on fields\n\n\n\n\n\n\n\n\nProjections\n\n\nProjections allow you to pull out only the fields needed and rename them when you are quering for raw data records.\n\n\nAggregations\n\n\nAggregations allow you to perform some operation on the collected records.\n\n\nThe current aggregation types that are supported are:\n\n\n\n\n\n\n\n\nAggregation\n\n\nMeaning\n\n\n\n\n\n\n\n\n\n\nGROUP\n\n\nThe resulting output would be a record containing the result of an operation for each unique value combination in your specified fields\n\n\n\n\n\n\nCOUNT DISTINCT\n\n\nComputes the number of distinct elements in the fields. (May be approximate)\n\n\n\n\n\n\nLIMIT or RAW\n\n\nThe resulting output would be at most the number specified in size.\n\n\n\n\n\n\n\n\nCurrently we support \nGROUP\n aggregations on the following operations:\n\n\n\n\n\n\n\n\nOperation\n\n\nMeaning\n\n\n\n\n\n\n\n\n\n\nCOUNT\n\n\nComputes the number of the elements in the group\n\n\n\n\n\n\nSUM\n\n\nComputes the sum of the elements in the group\n\n\n\n\n\n\nMIN\n\n\nReturns the minimum of the elements in the group\n\n\n\n\n\n\nMAX\n\n\nReturns the maximum of the elements in the group\n\n\n\n\n\n\nAVG\n\n\nComputes the average of the elements in the group\n\n\n\n\n\n\n\n\nResults\n\n\nThe Bullet Web Service returns your query result as well as associated metadata information in a structured JSON format. The UI can display the results in different formats.\n\n\n\n\nApproximate computation\n\n\nIt is often intractable to perform aggregations on an unbounded stream of data and still support arbitrary queries. However, it is possible if an exact answer is not required and the approximate answer's error is exactly quantifiable. There are stochastic algorithms and data structures that let us do this. We use \nData Sketches\n to perform aggregations such as counting uniques, and will be using Sketches to implement some future aggregations.\n\n\nSketches let us be exact in our computation up to configured thresholds and approximate after. The error is very controllable and quantifiable. All Bullet queries that use Sketches return the error bounds with Standard Deviations as part of the results so you can quantify the error exactly. Using Sketches lets us address otherwise hard to solve problems in sublinear space.\n\n\nWe also use Sketches as a way to control high cardinality grouping (group by a natural key column or related) and rely on the Sketching data structure to drop excess groups. It is up to you setting up Bullet to determine to set Sketch sizes large or small enough for to satisfy the queries that will be performed on that instance of Bullet.\n\n\nNew query types coming soon\n\n\nUsing Sketches, we have implemented \nCOUNT DISTINCT\n and \nGROUP\n and are working on other aggregations including but not limited to:\n\n\n\n\n\n\n\n\nAggregation\n\n\nMeaning\n\n\n\n\n\n\n\n\n\n\nTOP K\n\n\nReturns the top K most freqently appearing values in the column\n\n\n\n\n\n\nDISTRIBUTION\n\n\nComputes distributions of the elements in the column. E.g. Find the median value or the 95th percentile of a field or graph the entire distribution as a histogram\n\n\n\n\n\n\n\n\nArchitecture\n\n\nBackend\n\n\n\n\nThe Bullet backend can be split into three main sub-systems:\n\n\n\n\nRequest Processor - receives queries, adds metadata and sends it to the rest of the system\n\n\nData Processor - converts the data from an stream and matches it against queries\n\n\nCombiner - combines results for different queries, perfoms final aggregations and returns results\n\n\n\n\nWeb Service and UI\n\n\nThe rest of the pieces are just the standard other two pieces in a full-stack application:\n\n\n\n\nA Web Service that talks to this backend\n\n\nA UI that talks to this Web Service\n\n\n\n\nThe \nBullet Web Service\n is built using \nJersey\n and the \nUI\n is built in \nEmber\n.\n\n\nThe Web Service can be deployed with your favorite servlet container like \nJetty\n. The UI is a client-side application that can be served using \nNode.js\n\n\nIn the case of Bullet on Storm, the Web Service and UI talk to the backend using \nStorm DRPC\n.\n\n\nEnd-to-End Architecture\n\n\n\n\n\n\nWant to know more?\n\n\nIn practice, the backend is implemented using the basic components that the Stream processing framework provides. See \nStorm Architecture\n for details.\n\n\n\n\nReleases\n\n\nSee the \nReleases\n section where the various Bullet releases are collected in one place.", 
            "title": "Home"
        }, 
        {
            "location": "/#overview", 
            "text": "", 
            "title": "Overview"
        }, 
        {
            "location": "/#bullet", 
            "text": "Is a real-time query engine that lets you run queries on very large data streams    Does not use a  a persistence layer . This makes it  light-weight, cheap and fast    Is a  look-forward  query system. Queries are submitted first and they operate on data that arrive after the query is submitted    Is  multi-tenant  and can scale independently for more queries and for more data in the first order    Provides a  UI and Web Service  that are also pluggable for a full end-to-end solution to your querying needs    Can be implemented on different Stream processing frameworks. Bullet on  Storm  is currently available    Is  pluggable . Any data source that can be read from Storm can be converted into a standard data container letting you query that data. Data is  typed    Is used at scale and in production at Yahoo with hundreds of queries simultaneously on hundreds of thousands of records per second and tested up to millions of records per second", 
            "title": "Bullet ..."
        }, 
        {
            "location": "/#how-is-this-useful", 
            "text": "How Bullet is used is largely determined by the data source it consumes. Depending on what kind of data you put Bullet on, the types of queries you run on it and your use-cases will change. As a look-forward query system with no persistence, you will not be able to repeat your queries on the same data. The next time you run your query, it will operate on the different data that arrives after that submission. If this usage pattern is what you need and you are looking for a light-weight system that can tap into your streaming data, then Bullet is for you!", 
            "title": "How is this useful"
        }, 
        {
            "location": "/#how-bullet-is-used-at-yahoo", 
            "text": "Bullet is used in production internally at Yahoo by having it sit on raw user engagement events from Yahoo sites and apps. This lets Yahoo developers automatically validate  end-to-end  their instrumentation code in their Continuous Delivery pipelines. Validating instrumentation is critical since it powers pretty much all decisions and products including machine learning, corporate KPIs, analytics, personalization, targeting.  This instance of Bullet also powers other use-cases such as letting analysts validate assumptions about data, product managers verify launches instantly, debug issues and outages, or simply explore and play around with the data.", 
            "title": "How Bullet is used at Yahoo"
        }, 
        {
            "location": "/#quick-start", 
            "text": "See  Quick Start  to set up Bullet on a local Storm topology. We will generate some fake streaming data that you can then query with Bullet.", 
            "title": "Quick Start"
        }, 
        {
            "location": "/#setting-up-bullet-on-your-streaming-data", 
            "text": "To set up Bullet on a real data stream, you need:   The backend set up on a Stream processor:  Plug in your source of data. See  Getting your data into Bullet  for details  Consume your data stream. Currently, we support  Bullet on Storm    The  Web Service  set up to convey queries and return results back from the backend  The optional  UI  set up to talk to your Web Service. You can skip the UI if all your access is programmatic    Schema in the UI  The UI also needs an endpoint that provides your data schema to help with query building. The Web Service you set up provides a simple file based schema endpoint that you can point the UI to if that is sufficient for your needs.", 
            "title": "Setting up Bullet on your streaming data"
        }, 
        {
            "location": "/#querying-in-bullet", 
            "text": "Bullet queries allow you to filter, project and aggregate data. It lets you fetch raw (the individual data records) as well as aggregated data.  See the  UI Usage section  for using the UI to build Bullet queries.  See the  API section  for building Bullet API queries.  For examples using the API, see  Examples .", 
            "title": "Querying in Bullet"
        }, 
        {
            "location": "/#termination-conditions", 
            "text": "A Bullet query terminates and returns whatever has been collected so far when:   A maximum duration is reached. In other words, a query runs for a defined time window  A maximum number of records is reached. (only applicable for queries that are fetching raw data records and not aggregating)", 
            "title": "Termination conditions"
        }, 
        {
            "location": "/#filters", 
            "text": "Bullet supports two kinds of filters:     Filter Type  Meaning      Logical filter  Allow you to combine filter clauses (Logical or Relational) with logical operations like AND, OR and NOTs    Relational filters  Allow you to use comparison operations like equals, not equals, greater than, less than, regex like etc, on fields", 
            "title": "Filters"
        }, 
        {
            "location": "/#projections", 
            "text": "Projections allow you to pull out only the fields needed and rename them when you are quering for raw data records.", 
            "title": "Projections"
        }, 
        {
            "location": "/#aggregations", 
            "text": "Aggregations allow you to perform some operation on the collected records.  The current aggregation types that are supported are:     Aggregation  Meaning      GROUP  The resulting output would be a record containing the result of an operation for each unique value combination in your specified fields    COUNT DISTINCT  Computes the number of distinct elements in the fields. (May be approximate)    LIMIT or RAW  The resulting output would be at most the number specified in size.     Currently we support  GROUP  aggregations on the following operations:     Operation  Meaning      COUNT  Computes the number of the elements in the group    SUM  Computes the sum of the elements in the group    MIN  Returns the minimum of the elements in the group    MAX  Returns the maximum of the elements in the group    AVG  Computes the average of the elements in the group", 
            "title": "Aggregations"
        }, 
        {
            "location": "/#results", 
            "text": "The Bullet Web Service returns your query result as well as associated metadata information in a structured JSON format. The UI can display the results in different formats.", 
            "title": "Results"
        }, 
        {
            "location": "/#approximate-computation", 
            "text": "It is often intractable to perform aggregations on an unbounded stream of data and still support arbitrary queries. However, it is possible if an exact answer is not required and the approximate answer's error is exactly quantifiable. There are stochastic algorithms and data structures that let us do this. We use  Data Sketches  to perform aggregations such as counting uniques, and will be using Sketches to implement some future aggregations.  Sketches let us be exact in our computation up to configured thresholds and approximate after. The error is very controllable and quantifiable. All Bullet queries that use Sketches return the error bounds with Standard Deviations as part of the results so you can quantify the error exactly. Using Sketches lets us address otherwise hard to solve problems in sublinear space.  We also use Sketches as a way to control high cardinality grouping (group by a natural key column or related) and rely on the Sketching data structure to drop excess groups. It is up to you setting up Bullet to determine to set Sketch sizes large or small enough for to satisfy the queries that will be performed on that instance of Bullet.", 
            "title": "Approximate computation"
        }, 
        {
            "location": "/#new-query-types-coming-soon", 
            "text": "Using Sketches, we have implemented  COUNT DISTINCT  and  GROUP  and are working on other aggregations including but not limited to:     Aggregation  Meaning      TOP K  Returns the top K most freqently appearing values in the column    DISTRIBUTION  Computes distributions of the elements in the column. E.g. Find the median value or the 95th percentile of a field or graph the entire distribution as a histogram", 
            "title": "New query types coming soon"
        }, 
        {
            "location": "/#architecture", 
            "text": "", 
            "title": "Architecture"
        }, 
        {
            "location": "/#backend", 
            "text": "The Bullet backend can be split into three main sub-systems:   Request Processor - receives queries, adds metadata and sends it to the rest of the system  Data Processor - converts the data from an stream and matches it against queries  Combiner - combines results for different queries, perfoms final aggregations and returns results", 
            "title": "Backend"
        }, 
        {
            "location": "/#web-service-and-ui", 
            "text": "The rest of the pieces are just the standard other two pieces in a full-stack application:   A Web Service that talks to this backend  A UI that talks to this Web Service   The  Bullet Web Service  is built using  Jersey  and the  UI  is built in  Ember .  The Web Service can be deployed with your favorite servlet container like  Jetty . The UI is a client-side application that can be served using  Node.js  In the case of Bullet on Storm, the Web Service and UI talk to the backend using  Storm DRPC .", 
            "title": "Web Service and UI"
        }, 
        {
            "location": "/#end-to-end-architecture", 
            "text": "Want to know more?  In practice, the backend is implemented using the basic components that the Stream processing framework provides. See  Storm Architecture  for details.", 
            "title": "End-to-End Architecture"
        }, 
        {
            "location": "/#releases", 
            "text": "See the  Releases  section where the various Bullet releases are collected in one place.", 
            "title": "Releases"
        }, 
        {
            "location": "/quick-start/", 
            "text": "Quick Start\n\n\nThis section gets you up and running with an mock instance of Bullet to play around with. The instance will be running using \nBullet on Storm\n. Since we do not have an actual data source, we will produce some fake data and convert it into \nBullet Records\n in a\n\ncustom Storm spout\n. When you build Bullet for your data, you will need to do something similar after reading your data.\n\n\nAt the end of this section, you should have a full end-to-end Bullet instance with a Web Service and UI running on a machine. You will:\n\n\n\n\nSetup the Bullet topology using a custom spout on \nbullet-storm-0.3.0\n\n\nSetup the \nWeb Service\n talking to the topology and serving a schema for your UI using \nbullet-service-0.0.1\n\n\nSetup the \nUI\n talking to the Web Service using \nbullet-ui-0.1.0\n\n\n\n\nPrerequisites:\n\n\n\n\nYou will need to be on an Unix-based system (Mac, Ubuntu ...) for most of these commands.\n\n\nYou will need enough CPU and RAM on your machine to run about about 8-10 JVMs. You will be setting up a Storm cluster with multiple components, a couple of Jetty instances and a Node server.\n\n\nYou will need \ngit\n, \nMaven 3\n and \nJDK 8\n installed. The example will walk you through installing \nNode.js\n.\n\n\n\n\nSetting up Storm\n\n\nThis section will deal with setting up Storm. To set up a clean working environment, start with creating an empty working directory.\n\n\nStep 1: Setup directories and examples\n\n\nexport BULLET_HOME=$(pwd)/bullet-quickstart\nmkdir -p $BULLET_HOME/backend/storm\nmkdir -p $BULLET_HOME/service\nmkdir -p $BULLET_HOME/ui\ncd $BULLET_HOME \n git clone git@github.com:yahoo/bullet-docs.git\nexport BULLET_EXAMPLES=$BULLET_HOME/bullet-docs/examples\n\n\n\n\nStep 2: Install Storm 1.0\n\n\ncd $BULLET_HOME/backend\nwget http://apache.org/dist/storm/apache-storm-1.0.3/apache-storm-1.0.3.zip\nunzip apache-storm-1.0.3.zip\nexport PATH=$(pwd)/apache-storm-1.0.3/bin/:$PATH\n\n\n\n\nAdd a DRPC server setting to the Storm config:\n\n\necho 'drpc.servers: [\n127.0.0.1\n]' \n apache-storm-1.0.3/conf/storm.yaml\n\n\n\n\nStep 3: Launch Storm components\n\n\nLaunch each of the following components, in order and wait for the commands to go through. You will see a JVM being launched:\n\n\nstorm dev-zookeeper \n\nstorm nimbus \n\nstorm drpc \n\nstorm ui \n\nstorm logviewer \n\nstorm supervisor \n\n\n\n\n\nOnce everything is up without errors, visit localhost:8080 and see if the Storm UI loads.\n\n\nStep 4: Test Storm (Optional)\n\n\nBefore Bullet, test to see if Storm and DRPC are up and running by launching a example topology that comes with your Storm installation:\n\n\nstorm jar apache-storm-1.0.3/examples/storm-starter/storm-starter-topologies-1.0.3.jar org.apache.storm.starter.BasicDRPCTopology topology\n\n\n\n\nVisit your UI with a browser and see if a topology with name \"topology\" is running. If everything is good, you should be able to ping DRPC with:\n\n\ncurl localhost:3774/drpc/exclamation/foo\n\n\n\n\nand get back a \nfoo!\n. Any string you pass as part of the URL is returned to you with a \"!\" at the end.\n\n\nKill this topology after with:\n\n\nstorm kill topology\n\n\n\n\n\n\nLocal mode cleanup\n\n\nIf you notice any problems while setting up storm or while relaunching a topology, it may be because some state is corrupted. When running Storm in this fashion, states and serializations are stored in \nstorm-local\n. You may want to \nrm -rf storm-local/*\n to remove everything in this directory before relaunching Storm components.\n\n\n\n\nSetting up the example Bullet topology\n\n\nNow that Storm is up and running, we can put Bullet on it. We will build an example Spout that runs on Bullet 0.3.0 on our Storm cluster. The source is available \nhere\n.\n\n\nStep 5: Build the Storm example\n\n\ncd $BULLET_EXAMPLES/storm \n mvn package\ncp $BULLET_EXAMPLES/storm/bin/launch.sh $BULLET_HOME/backend/storm\ncp $BULLET_EXAMPLES/storm/target/*jar-with-dependencies.jar $BULLET_HOME/backend/storm\ncp $BULLET_EXAMPLES/storm/src/main/resources/bullet_settings.yaml $BULLET_HOME/backend/storm\n\n\n\n\n\n\nSettings\n\n\nTake a look at bullet_settings.yaml for the settings that are being overridden for this example. You can add or change settings as you like by referring to \nbullet_defaults.yaml\n.\n\n\n\n\nStep 6: Launch the topology\n\n\ncd $BULLET_HOME/backend/storm \n ./launch.sh\n\n\n\n\nThis script also kills any existing Bullet instances running (you may see an ignorable exception if there is nothing running). There can only be one topology in the cluster with a particular name. Visit the UI and see if the topology is up. You should see the \nDataSource\n spout begin emitting records.\n\n\nTest the Bullet topology by:\n\n\ncurl -s -X POST -d '{}' http://localhost:3774/drpc/bullet\n\n\n\n\nYou should get a random record from Bullet produced by the custom spout that we plugged in.\n\n\nSetting up the Bullet Web Service\n\n\nStep 7: Install Jetty\n\n\ncd $BULLET_HOME/service\nwget http://central.maven.org/maven2/org/eclipse/jetty/jetty-distribution/9.3.16.v20170120/jetty-distribution-9.3.16.v20170120.zip\nunzip jetty-distribution-9.3.16.v20170120.zip\n\n\n\n\nStep 8: Install the Bullet Web Service\n\n\ncd jetty-distribution-9.3.16.v20170120\nwget -O webapps/bullet-service.war http://jcenter.bintray.com/com/yahoo/bullet/bullet-service/0.0.1/bullet-service-0.0.1.war\ncp $BULLET_EXAMPLES/web-service/example_* $BULLET_HOME/service/jetty-distribution-9.3.16.v20170120\n\n\n\n\nStep 9: Launch the Web Service\n\n\ncd $BULLET_HOME/service/jetty-distribution-9.3.16.v20170120\njava -jar -Dbullet.service.configuration.file=\nexample_context.properties\n -Djetty.http.port=9999 start.jar \n logs/out 2\n1 \n\n\n\n\n\nYou can verify that it is up by running the Bullet query and getting the example_columns through the API:\n\n\ncurl -s -X POST -d '{}' http://localhost:9999/bullet-service/api/drpc\ncurl -s http://localhost:9999/bullet-service/api/columns\n\n\n\n\nSetting up the Bullet UI\n\n\nStep 10: Install Node\n\n\nwget -qO- https://raw.githubusercontent.com/creationix/nvm/v0.33.1/install.sh | bash\nsource ~/.bashrc\nnvm install v6.9.4\nnvm use v6.9.4\n\n\n\n\nStep 11: Install the Bullet UI\n\n\ncd $BULLET_HOME/ui\nwget https://github.com/yahoo/bullet-ui/releases/download/v0.1.0/bullet-ui-v0.1.0.tar.gz\ntar -xzf bullet-ui-v0.1.0.tar.gz\ncp $BULLET_EXAMPLES/ui/env-settings.json config/\n\n\n\n\nStep 12: Launch the UI\n\n\nPORT=8800 node express-server.js \n\n\n\n\n\nVisit http://localhost:8800 to query your topology with the UI. See \nUI usage\n for some example queries and interactions using this UI. You see what the Schema means by visiting the Schema section.\n\n\n\n\nRunning it remotely?\n\n\nIf you access the UI from another machine than where your UI is actually running, you will need to edit \nconfig/env-settings.json\n. Since the UI is a client-side app, the machine that your browser is running on will fetch the UI and attempt to use these settings to talk to the Web Service. Since they point to localhost by default, your browser will attempt to connect there and fail. An easy fix is to change \nlocalhost\n in your env-settings.json to point to the hostname where you will hosting the UI. This will be same UI you use in the browser.\n\n\n\n\n\n\nBringing it all down\n\n\nTo kill the Bullet topology, run \nstorm kill bullet\n\n\nTo kill the UI, run \nps aux | grep [e]xpress-server | awk '{print $2}' | xargs kill\n\n\nTo kill the Web Service, run \nps aux | grep [e]xample_context | awk '{print $2}' | xargs kill\n\n\nTo kill Storm, run \nps aux | grep [a]pache-storm-1.0.3 | awk '{print $2}' | xargs kill\n\n\n\n\nWhat did we do?\n\n\nThis section will cover the various custom pieces this example plugged into Bullet, so you can better understand what we did.\n\n\nStorm topology\n\n\nThe topology was the Bullet topology plugged in with a custom spout. This spout is implemented in this \nexample project\n you built from source. This spout produces a maximum number of records in a given period. Both these arguments are configurable. If you examine $BULLET_HOME/backend/storm/launch.sh, you'll see the following:\n\n\nstorm jar bullet-storm-example-1.0-SNAPSHOT-jar-with-dependencies.jar \\\n          com.yahoo.bullet.Topology \\\n          --bullet-conf bullet_settings.yaml \\\n          --bullet-spout com.yahoo.bullet.storm.examples.RandomSpout \\\n          --bullet-spout-parallelism 1 \\\n          ...\n          --bullet-spout-arg 20 \\\n          --bullet-spout-arg 100 \\\n          ...\n\n\n\n\nThis command launches the jar (an uber or \"fat\" jar) containing the custom spout code and all dependencies you built in Step 5. We pass the name of your spout class with \n--bullet-spout com.yahoo.bullet.storm.examples.RandomSpout\n to the Bullet main class \ncom.yahoo.bullet.Topology\n with two arguments \n--bullet-spout-arg 20\n and \n--bullet-spout-arg 100\n. The first argument tells the Spout to generate at most 20 tuples in a period and the second argument says a period is 100 ms long.\n\n\nThe settings defined by \n--bullet-conf bullet_settings.yaml\n and the arguments here run all components in the topology with a parallelism of 1. So there will be one spout that is producing 200 tuples per second.\n\n\n\n\nI thought you said hundreds of thousands of records...\n\n\n200 tuples/s is not Big Data by any stretch of the imagination but this Quick Start is running everything on one machine and is meant to introduce you to what Bullet does. In practice, you would scale and run your components to accommodate for your data volume and querying needs.\n\n\n\n\nLet's look at the \ncustom spout code\n that generates the data.\n\n\n    @Override\n    public void nextTuple() {\n        long timeNow = System.nanoTime();\n        // Only emit if we are still in the interval and haven't gone over our per period max\n        if (timeNow \n= nextIntervalStart \n generatedThisPeriod \n maxPerPeriod) {\n            outputCollector.emit(new Values(generateRecord()), DUMMY_ID);\n            generatedThisPeriod++;\n        }\n        if (timeNow \n nextIntervalStart) {\n            log.info(\nGenerated {} tuples out of {}\n, generatedThisPeriod, maxPerPeriod);\n            nextIntervalStart = timeNow + period;\n            generatedThisPeriod = 0;\n            periodCount++;\n        }\n        // It is courteous to sleep for a short time if you're not emitting anything...\n        try {\n            Thread.sleep(1);\n        } catch (InterruptedException e) {\n            log.error(\nError: \n, e);\n        }\n    }\n\n\n\n\nThis method above emits the tuples. The Storm framework calls this method. This function only emits at most the given maximum tuples per period.\n\n\n\n\nWhy a DUMMY_ID?\n\n\nWhen the spout emits the randomly generated tuple, it attaches a \nDUMMY_ID\n to it. In Storm terms, this is a message ID. By adding a message ID, this tuple can be made to flow reliably. The Bullet component that receives this tuple (Filter bolt) acknowledges or \"acks\" this tuple. If the tuple did not make it to Filter bolt within a configured timeout window, Storm will call a \nfail(Object messageId)\n method on the spout. This particular spout does not define one and hence the usage of a \nDUMMY_ID\n. If your source of data can identify records uniquely and you can re-emit them on a fail, you should attach that actual ID in place of the \nDUMMY_ID\n.\n\n\n\n\n    private BulletRecord generateRecord() {\n        BulletRecord record = new BulletRecord();\n        String uuid = UUID.randomUUID().toString();\n\n        record.setString(STRING, uuid);\n        record.setLong(LONG, (long) generatedThisPeriod);\n        record.setDouble(DOUBLE, random.nextDouble());\n\n        Map\nString, Boolean\n booleanMap = new HashMap\n(4);\n        booleanMap.put(uuid.substring(0, 8), random.nextBoolean());\n        booleanMap.put(uuid.substring(9, 13), random.nextBoolean());\n        booleanMap.put(uuid.substring(14, 18), random.nextBoolean());\n        booleanMap.put(uuid.substring(19, 23), random.nextBoolean());\n        record.setBooleanMap(BOOLEAN_MAP, booleanMap);\n\n        STATS_MAP_VALUE.put(PERIOD_COUNT, periodCount);\n        STATS_MAP_VALUE.put(RECORD_NUMBER, periodCount * maxPerPeriod + generatedThisPeriod);\n        STATS_MAP_VALUE.put(NANO_TIME, System.nanoTime());\n        STATS_MAP_VALUE.put(TIMESTAMP, System.currentTimeMillis());\n        record.setLongMap(STATS_MAP, STATS_MAP_VALUE);\n\n        RANDOM_MAP_VALUE_A.put(RANDOM_MAP_KEY_A, STRING_POOL[random.nextInt(STRING_POOL.length)]);\n        RANDOM_MAP_VALUE_A.put(RANDOM_MAP_KEY_B, STRING_POOL[random.nextInt(STRING_POOL.length)]);\n        RANDOM_MAP_VALUE_B.put(RANDOM_MAP_KEY_A, STRING_POOL[random.nextInt(STRING_POOL.length)]);\n        RANDOM_MAP_VALUE_B.put(RANDOM_MAP_KEY_B, STRING_POOL[random.nextInt(STRING_POOL.length)]);\n        record.setListOfStringMap(LIST, asList(RANDOM_MAP_VALUE_A, RANDOM_MAP_VALUE_B));\n\n        return record;\n    }\n\n\n\n\nThis method generates some fields randomly and inserts them into a BulletRecord. Note that the BulletRecord is typed and all data must be inserted with the proper types. If you put Bullet on your data, you will need to write a Spout (or a topology if your reading is complex), that reads from your data source and emits BulletRecords with the fields you wish to be queryable placed into a BulletRecord.\n\n\n\n\nReusing Maps?\n\n\nThere is no need to reuse maps. This example tries to be as efficient as possible in order to allow variable throughputs.\n\n\n\n\nWeb Service\n\n\nWe launched the Web Service using two custom files - a properties file and JSON schema file.\n\n\nThe JSON columns file contains the schema for our data specified in JSON. Since our schema is not going to change, we use the Web Service to serve it from a file. If your schema changes dynamically, you will need to provide your own endpoint to the UI.\n\n\nThe following is a snippet from the \nJSON file\n. Notice how the types of the fields are specified. Also, if you have generated BulletRecord with Map fields whose keys are known, you can specify them here using \nenumerations\n.\n\n\n[\n    {\n        \nname\n: \nprobability\n,\n        \ntype\n: \nDOUBLE\n,\n        \ndescription\n: \nGenerated from Random#nextDouble\n\n    },\n    ...\n    {\n        \nname\n: \nstats_map\n,\n        \ntype\n: \nMAP\n,\n        \nsubtype\n: \nLONG\n,\n        \ndescription\n: \nThis map contains some numeric information such as the current number of periods etc.\n,\n        \nenumerations\n: [\n            ...\n            {\nname\n: \nnano_time\n, \ndescription\n: \nThe ns time when this record was generated\n}\n        ]\n    },\n    {\n        \nname\n: \nclassifiers\n,\n        \ntype\n: \nLIST\n,\n        \nsubtype\n: \nMAP\n,\n        \ndescription\n: \nThis contains two maps, each with: field_A and field_B whose values are randomly chosen from: foo, bar, baz, qux, quux, norf\n\n    }\n]\n\n\n\n\nThe \nexample properties file\n points to the DRPC host, port, and path as well points to the custom columns file.\n\n\ndrpc.servers=localhost\ndrpc.port=3774\ndrpc.path=drpc/bullet\ndrpc.retry.limit=3\ndrpc.connect.timeout=1000\ncolumns.file=example_columns.json\ncolumns.schema.version=1.0\n\n\n\n\ndrpc.servers\n is a CSV entry that contains the various DRPC servers in your Storm cluster. If you visit the Storm UI and search in the \nNimbus Configuration\n section, you can find the list of DRPC servers for your cluster. Similarly, \ndrpc.port\n in the properties file is \ndrpc.http.port\n in \nNimbus Configuration\n. The \ndrpc.path\n is the constant string \ndrpc/\n followed by the value of the \ntopology.function\n setting in bullet_settings.yaml.\n\n\nUI\n\n\nFinally, we configured the UI with the custom environment specific settings file. We did not add any environments since we only had the one.\n\n\n{\n  \ndefault\n: {\n    \ndrpcHost\n: \nhttp://localhost:9999\n,\n    \ndrpcNamespace\n: \nbullet-service/api\n,\n    \ndrpcPath\n: \ndrpc\n,\n    \nschemaHost\n: \nhttp://localhost:9999\n,\n    \nschemaNamespace\n: \nbullet-service/api\n,\n    \nhelpLinks\n: [\n      {\n        \nname\n: \nExample Docs Page\n,\n        \nlink\n: \n\n      }\n    ],\n    \nbugLink\n: \nhttps://github.com/yahoo/bullet-ui/issues\n,\n    \naggregateDataDefaultSize\n: 512,\n    \nmodelVersion\n: 1\n  }\n}\n\n\n\n\nSince we served our schema through the same Web Service as our queries, both these point to our Web Service. Note that there is no \nschemaPath\n because it must be the constant string \ncolumns\n. If you define a custom endpoint for your schema, you must ensure that it can be obtained by making a GET request to \nschemaHost/schemaNamespace/columns\n.", 
            "title": "Quick Start"
        }, 
        {
            "location": "/quick-start/#quick-start", 
            "text": "This section gets you up and running with an mock instance of Bullet to play around with. The instance will be running using  Bullet on Storm . Since we do not have an actual data source, we will produce some fake data and convert it into  Bullet Records  in a custom Storm spout . When you build Bullet for your data, you will need to do something similar after reading your data.  At the end of this section, you should have a full end-to-end Bullet instance with a Web Service and UI running on a machine. You will:   Setup the Bullet topology using a custom spout on  bullet-storm-0.3.0  Setup the  Web Service  talking to the topology and serving a schema for your UI using  bullet-service-0.0.1  Setup the  UI  talking to the Web Service using  bullet-ui-0.1.0   Prerequisites:   You will need to be on an Unix-based system (Mac, Ubuntu ...) for most of these commands.  You will need enough CPU and RAM on your machine to run about about 8-10 JVMs. You will be setting up a Storm cluster with multiple components, a couple of Jetty instances and a Node server.  You will need  git ,  Maven 3  and  JDK 8  installed. The example will walk you through installing  Node.js .", 
            "title": "Quick Start"
        }, 
        {
            "location": "/quick-start/#setting-up-storm", 
            "text": "This section will deal with setting up Storm. To set up a clean working environment, start with creating an empty working directory.", 
            "title": "Setting up Storm"
        }, 
        {
            "location": "/quick-start/#step-1-setup-directories-and-examples", 
            "text": "export BULLET_HOME=$(pwd)/bullet-quickstart\nmkdir -p $BULLET_HOME/backend/storm\nmkdir -p $BULLET_HOME/service\nmkdir -p $BULLET_HOME/ui\ncd $BULLET_HOME   git clone git@github.com:yahoo/bullet-docs.git\nexport BULLET_EXAMPLES=$BULLET_HOME/bullet-docs/examples", 
            "title": "Step 1: Setup directories and examples"
        }, 
        {
            "location": "/quick-start/#step-2-install-storm-10", 
            "text": "cd $BULLET_HOME/backend\nwget http://apache.org/dist/storm/apache-storm-1.0.3/apache-storm-1.0.3.zip\nunzip apache-storm-1.0.3.zip\nexport PATH=$(pwd)/apache-storm-1.0.3/bin/:$PATH  Add a DRPC server setting to the Storm config:  echo 'drpc.servers: [ 127.0.0.1 ]'   apache-storm-1.0.3/conf/storm.yaml", 
            "title": "Step 2: Install Storm 1.0"
        }, 
        {
            "location": "/quick-start/#step-3-launch-storm-components", 
            "text": "Launch each of the following components, in order and wait for the commands to go through. You will see a JVM being launched:  storm dev-zookeeper  \nstorm nimbus  \nstorm drpc  \nstorm ui  \nstorm logviewer  \nstorm supervisor    Once everything is up without errors, visit localhost:8080 and see if the Storm UI loads.", 
            "title": "Step 3: Launch Storm components"
        }, 
        {
            "location": "/quick-start/#step-4-test-storm-optional", 
            "text": "Before Bullet, test to see if Storm and DRPC are up and running by launching a example topology that comes with your Storm installation:  storm jar apache-storm-1.0.3/examples/storm-starter/storm-starter-topologies-1.0.3.jar org.apache.storm.starter.BasicDRPCTopology topology  Visit your UI with a browser and see if a topology with name \"topology\" is running. If everything is good, you should be able to ping DRPC with:  curl localhost:3774/drpc/exclamation/foo  and get back a  foo! . Any string you pass as part of the URL is returned to you with a \"!\" at the end.  Kill this topology after with:  storm kill topology   Local mode cleanup  If you notice any problems while setting up storm or while relaunching a topology, it may be because some state is corrupted. When running Storm in this fashion, states and serializations are stored in  storm-local . You may want to  rm -rf storm-local/*  to remove everything in this directory before relaunching Storm components.", 
            "title": "Step 4: Test Storm (Optional)"
        }, 
        {
            "location": "/quick-start/#setting-up-the-example-bullet-topology", 
            "text": "Now that Storm is up and running, we can put Bullet on it. We will build an example Spout that runs on Bullet 0.3.0 on our Storm cluster. The source is available  here .", 
            "title": "Setting up the example Bullet topology"
        }, 
        {
            "location": "/quick-start/#step-5-build-the-storm-example", 
            "text": "cd $BULLET_EXAMPLES/storm   mvn package\ncp $BULLET_EXAMPLES/storm/bin/launch.sh $BULLET_HOME/backend/storm\ncp $BULLET_EXAMPLES/storm/target/*jar-with-dependencies.jar $BULLET_HOME/backend/storm\ncp $BULLET_EXAMPLES/storm/src/main/resources/bullet_settings.yaml $BULLET_HOME/backend/storm   Settings  Take a look at bullet_settings.yaml for the settings that are being overridden for this example. You can add or change settings as you like by referring to  bullet_defaults.yaml .", 
            "title": "Step 5: Build the Storm example"
        }, 
        {
            "location": "/quick-start/#step-6-launch-the-topology", 
            "text": "cd $BULLET_HOME/backend/storm   ./launch.sh  This script also kills any existing Bullet instances running (you may see an ignorable exception if there is nothing running). There can only be one topology in the cluster with a particular name. Visit the UI and see if the topology is up. You should see the  DataSource  spout begin emitting records.  Test the Bullet topology by:  curl -s -X POST -d '{}' http://localhost:3774/drpc/bullet  You should get a random record from Bullet produced by the custom spout that we plugged in.", 
            "title": "Step 6: Launch the topology"
        }, 
        {
            "location": "/quick-start/#setting-up-the-bullet-web-service", 
            "text": "", 
            "title": "Setting up the Bullet Web Service"
        }, 
        {
            "location": "/quick-start/#step-7-install-jetty", 
            "text": "cd $BULLET_HOME/service\nwget http://central.maven.org/maven2/org/eclipse/jetty/jetty-distribution/9.3.16.v20170120/jetty-distribution-9.3.16.v20170120.zip\nunzip jetty-distribution-9.3.16.v20170120.zip", 
            "title": "Step 7: Install Jetty"
        }, 
        {
            "location": "/quick-start/#step-8-install-the-bullet-web-service", 
            "text": "cd jetty-distribution-9.3.16.v20170120\nwget -O webapps/bullet-service.war http://jcenter.bintray.com/com/yahoo/bullet/bullet-service/0.0.1/bullet-service-0.0.1.war\ncp $BULLET_EXAMPLES/web-service/example_* $BULLET_HOME/service/jetty-distribution-9.3.16.v20170120", 
            "title": "Step 8: Install the Bullet Web Service"
        }, 
        {
            "location": "/quick-start/#step-9-launch-the-web-service", 
            "text": "cd $BULLET_HOME/service/jetty-distribution-9.3.16.v20170120\njava -jar -Dbullet.service.configuration.file= example_context.properties  -Djetty.http.port=9999 start.jar   logs/out 2 1    You can verify that it is up by running the Bullet query and getting the example_columns through the API:  curl -s -X POST -d '{}' http://localhost:9999/bullet-service/api/drpc\ncurl -s http://localhost:9999/bullet-service/api/columns", 
            "title": "Step 9: Launch the Web Service"
        }, 
        {
            "location": "/quick-start/#setting-up-the-bullet-ui", 
            "text": "", 
            "title": "Setting up the Bullet UI"
        }, 
        {
            "location": "/quick-start/#step-10-install-node", 
            "text": "wget -qO- https://raw.githubusercontent.com/creationix/nvm/v0.33.1/install.sh | bash\nsource ~/.bashrc\nnvm install v6.9.4\nnvm use v6.9.4", 
            "title": "Step 10: Install Node"
        }, 
        {
            "location": "/quick-start/#step-11-install-the-bullet-ui", 
            "text": "cd $BULLET_HOME/ui\nwget https://github.com/yahoo/bullet-ui/releases/download/v0.1.0/bullet-ui-v0.1.0.tar.gz\ntar -xzf bullet-ui-v0.1.0.tar.gz\ncp $BULLET_EXAMPLES/ui/env-settings.json config/", 
            "title": "Step 11: Install the Bullet UI"
        }, 
        {
            "location": "/quick-start/#step-12-launch-the-ui", 
            "text": "PORT=8800 node express-server.js    Visit http://localhost:8800 to query your topology with the UI. See  UI usage  for some example queries and interactions using this UI. You see what the Schema means by visiting the Schema section.   Running it remotely?  If you access the UI from another machine than where your UI is actually running, you will need to edit  config/env-settings.json . Since the UI is a client-side app, the machine that your browser is running on will fetch the UI and attempt to use these settings to talk to the Web Service. Since they point to localhost by default, your browser will attempt to connect there and fail. An easy fix is to change  localhost  in your env-settings.json to point to the hostname where you will hosting the UI. This will be same UI you use in the browser.    Bringing it all down  To kill the Bullet topology, run  storm kill bullet  To kill the UI, run  ps aux | grep [e]xpress-server | awk '{print $2}' | xargs kill  To kill the Web Service, run  ps aux | grep [e]xample_context | awk '{print $2}' | xargs kill  To kill Storm, run  ps aux | grep [a]pache-storm-1.0.3 | awk '{print $2}' | xargs kill", 
            "title": "Step 12: Launch the UI"
        }, 
        {
            "location": "/quick-start/#what-did-we-do", 
            "text": "This section will cover the various custom pieces this example plugged into Bullet, so you can better understand what we did.", 
            "title": "What did we do?"
        }, 
        {
            "location": "/quick-start/#storm-topology", 
            "text": "The topology was the Bullet topology plugged in with a custom spout. This spout is implemented in this  example project  you built from source. This spout produces a maximum number of records in a given period. Both these arguments are configurable. If you examine $BULLET_HOME/backend/storm/launch.sh, you'll see the following:  storm jar bullet-storm-example-1.0-SNAPSHOT-jar-with-dependencies.jar \\\n          com.yahoo.bullet.Topology \\\n          --bullet-conf bullet_settings.yaml \\\n          --bullet-spout com.yahoo.bullet.storm.examples.RandomSpout \\\n          --bullet-spout-parallelism 1 \\\n          ...\n          --bullet-spout-arg 20 \\\n          --bullet-spout-arg 100 \\\n          ...  This command launches the jar (an uber or \"fat\" jar) containing the custom spout code and all dependencies you built in Step 5. We pass the name of your spout class with  --bullet-spout com.yahoo.bullet.storm.examples.RandomSpout  to the Bullet main class  com.yahoo.bullet.Topology  with two arguments  --bullet-spout-arg 20  and  --bullet-spout-arg 100 . The first argument tells the Spout to generate at most 20 tuples in a period and the second argument says a period is 100 ms long.  The settings defined by  --bullet-conf bullet_settings.yaml  and the arguments here run all components in the topology with a parallelism of 1. So there will be one spout that is producing 200 tuples per second.   I thought you said hundreds of thousands of records...  200 tuples/s is not Big Data by any stretch of the imagination but this Quick Start is running everything on one machine and is meant to introduce you to what Bullet does. In practice, you would scale and run your components to accommodate for your data volume and querying needs.   Let's look at the  custom spout code  that generates the data.      @Override\n    public void nextTuple() {\n        long timeNow = System.nanoTime();\n        // Only emit if we are still in the interval and haven't gone over our per period max\n        if (timeNow  = nextIntervalStart   generatedThisPeriod   maxPerPeriod) {\n            outputCollector.emit(new Values(generateRecord()), DUMMY_ID);\n            generatedThisPeriod++;\n        }\n        if (timeNow   nextIntervalStart) {\n            log.info( Generated {} tuples out of {} , generatedThisPeriod, maxPerPeriod);\n            nextIntervalStart = timeNow + period;\n            generatedThisPeriod = 0;\n            periodCount++;\n        }\n        // It is courteous to sleep for a short time if you're not emitting anything...\n        try {\n            Thread.sleep(1);\n        } catch (InterruptedException e) {\n            log.error( Error:  , e);\n        }\n    }  This method above emits the tuples. The Storm framework calls this method. This function only emits at most the given maximum tuples per period.   Why a DUMMY_ID?  When the spout emits the randomly generated tuple, it attaches a  DUMMY_ID  to it. In Storm terms, this is a message ID. By adding a message ID, this tuple can be made to flow reliably. The Bullet component that receives this tuple (Filter bolt) acknowledges or \"acks\" this tuple. If the tuple did not make it to Filter bolt within a configured timeout window, Storm will call a  fail(Object messageId)  method on the spout. This particular spout does not define one and hence the usage of a  DUMMY_ID . If your source of data can identify records uniquely and you can re-emit them on a fail, you should attach that actual ID in place of the  DUMMY_ID .       private BulletRecord generateRecord() {\n        BulletRecord record = new BulletRecord();\n        String uuid = UUID.randomUUID().toString();\n\n        record.setString(STRING, uuid);\n        record.setLong(LONG, (long) generatedThisPeriod);\n        record.setDouble(DOUBLE, random.nextDouble());\n\n        Map String, Boolean  booleanMap = new HashMap (4);\n        booleanMap.put(uuid.substring(0, 8), random.nextBoolean());\n        booleanMap.put(uuid.substring(9, 13), random.nextBoolean());\n        booleanMap.put(uuid.substring(14, 18), random.nextBoolean());\n        booleanMap.put(uuid.substring(19, 23), random.nextBoolean());\n        record.setBooleanMap(BOOLEAN_MAP, booleanMap);\n\n        STATS_MAP_VALUE.put(PERIOD_COUNT, periodCount);\n        STATS_MAP_VALUE.put(RECORD_NUMBER, periodCount * maxPerPeriod + generatedThisPeriod);\n        STATS_MAP_VALUE.put(NANO_TIME, System.nanoTime());\n        STATS_MAP_VALUE.put(TIMESTAMP, System.currentTimeMillis());\n        record.setLongMap(STATS_MAP, STATS_MAP_VALUE);\n\n        RANDOM_MAP_VALUE_A.put(RANDOM_MAP_KEY_A, STRING_POOL[random.nextInt(STRING_POOL.length)]);\n        RANDOM_MAP_VALUE_A.put(RANDOM_MAP_KEY_B, STRING_POOL[random.nextInt(STRING_POOL.length)]);\n        RANDOM_MAP_VALUE_B.put(RANDOM_MAP_KEY_A, STRING_POOL[random.nextInt(STRING_POOL.length)]);\n        RANDOM_MAP_VALUE_B.put(RANDOM_MAP_KEY_B, STRING_POOL[random.nextInt(STRING_POOL.length)]);\n        record.setListOfStringMap(LIST, asList(RANDOM_MAP_VALUE_A, RANDOM_MAP_VALUE_B));\n\n        return record;\n    }  This method generates some fields randomly and inserts them into a BulletRecord. Note that the BulletRecord is typed and all data must be inserted with the proper types. If you put Bullet on your data, you will need to write a Spout (or a topology if your reading is complex), that reads from your data source and emits BulletRecords with the fields you wish to be queryable placed into a BulletRecord.   Reusing Maps?  There is no need to reuse maps. This example tries to be as efficient as possible in order to allow variable throughputs.", 
            "title": "Storm topology"
        }, 
        {
            "location": "/quick-start/#web-service", 
            "text": "We launched the Web Service using two custom files - a properties file and JSON schema file.  The JSON columns file contains the schema for our data specified in JSON. Since our schema is not going to change, we use the Web Service to serve it from a file. If your schema changes dynamically, you will need to provide your own endpoint to the UI.  The following is a snippet from the  JSON file . Notice how the types of the fields are specified. Also, if you have generated BulletRecord with Map fields whose keys are known, you can specify them here using  enumerations .  [\n    {\n         name :  probability ,\n         type :  DOUBLE ,\n         description :  Generated from Random#nextDouble \n    },\n    ...\n    {\n         name :  stats_map ,\n         type :  MAP ,\n         subtype :  LONG ,\n         description :  This map contains some numeric information such as the current number of periods etc. ,\n         enumerations : [\n            ...\n            { name :  nano_time ,  description :  The ns time when this record was generated }\n        ]\n    },\n    {\n         name :  classifiers ,\n         type :  LIST ,\n         subtype :  MAP ,\n         description :  This contains two maps, each with: field_A and field_B whose values are randomly chosen from: foo, bar, baz, qux, quux, norf \n    }\n]  The  example properties file  points to the DRPC host, port, and path as well points to the custom columns file.  drpc.servers=localhost\ndrpc.port=3774\ndrpc.path=drpc/bullet\ndrpc.retry.limit=3\ndrpc.connect.timeout=1000\ncolumns.file=example_columns.json\ncolumns.schema.version=1.0  drpc.servers  is a CSV entry that contains the various DRPC servers in your Storm cluster. If you visit the Storm UI and search in the  Nimbus Configuration  section, you can find the list of DRPC servers for your cluster. Similarly,  drpc.port  in the properties file is  drpc.http.port  in  Nimbus Configuration . The  drpc.path  is the constant string  drpc/  followed by the value of the  topology.function  setting in bullet_settings.yaml.", 
            "title": "Web Service"
        }, 
        {
            "location": "/quick-start/#ui", 
            "text": "Finally, we configured the UI with the custom environment specific settings file. We did not add any environments since we only had the one.  {\n   default : {\n     drpcHost :  http://localhost:9999 ,\n     drpcNamespace :  bullet-service/api ,\n     drpcPath :  drpc ,\n     schemaHost :  http://localhost:9999 ,\n     schemaNamespace :  bullet-service/api ,\n     helpLinks : [\n      {\n         name :  Example Docs Page ,\n         link :  \n      }\n    ],\n     bugLink :  https://github.com/yahoo/bullet-ui/issues ,\n     aggregateDataDefaultSize : 512,\n     modelVersion : 1\n  }\n}  Since we served our schema through the same Web Service as our queries, both these point to our Web Service. Note that there is no  schemaPath  because it must be the constant string  columns . If you define a custom endpoint for your schema, you must ensure that it can be obtained by making a GET request to  schemaHost/schemaNamespace/columns .", 
            "title": "UI"
        }, 
        {
            "location": "/backend/storm-architecture/", 
            "text": "Storm architecture\n\n\nThis section describes how the \nBackend architecture\n is implemented in Storm.\n\n\nStorm DRPC\n\n\nBullet on \nStorm\n is built using \nStorm DRPC\n. DRPC or Distributed Remote Procedure Call, is built into Storm and consist of a set of servers that are part of the Storm cluster. When a Storm topology that uses DRPC is launched, it registers a spout with a unique name (the procedure in the Distributed Remote Procedure Call) with the DRPC infrastructure. The DRPC Servers expose a REST endpoint where data can be POSTed to or a GET request can be made with this unique name. The DRPC infrastructure then sends the request (a query in Bullet) through the spout(s) to the topology (Bullet). The result from Bullet is sent back to the client. We picked Storm to implement Bullet on first not only because it was the most popular Streaming framework at Yahoo but also since DRPC provides us a nice and simple way to handle getting queries into Bullet and sending responses back.\n\n\n\n\nThrift and DRPC servers\n\n\nDRPC also exposes a \nThrift\n endpoint but the Bullet Web Service uses REST for simplicity. When you launch your Bullet Storm topology, you can POST Bullet queries to a DRPC server directly with the function name that you specified in the Bullet configuration. This is a quick way to check if your topology is up and running!\n\n\n\n\nTopology\n\n\nFor Bullet on Storm, the Storm topology implements the backend piece from the full \nArchitecture\n. The topology is implemented with the standard Storm spout and bolt components:\n\n\n\n\nThe components in \nArchitecture\n have direct counterparts here. The DRPC servers, the DRPC spouts, the Prepare Request bolts comprise the Request Processor. The Filter bolts and your plugin for your source of Data make up the Data Processor. The Join bolt and the Return Results bolt make up the Combiner.\n\n\nThe red colored lines are the path for the queries that come in through Storm DRPC and the blue is for the data from your data source. The pattern on the lines denote how the data (Storm tuples) is moved to the next component. Dashed indicates a broadcast (sent to all instances of the component), dotted indicates a key grouping (sent to a particular instance based on hashing on a particular field), and solid indicates a shuffle (randomly sent to an instance).\n\n\n\n\nWhat's a Ticker?\n\n\nThe Ticker component is attached to the Filter and Join Bolts produce Storm tuples at predefined intervals. This is a Storm feature (and is configurable when you launch the Bullet topology). These tuples, called tick tuples, behave like a CPU clock cycles for Bullet. Bullet performs all its system related activities on a tick. This includes purging stale queries, emitting left over data for queries, etc. We could have gone the route of having asynchronous threads that do the same thing but this was a far more simpler and elegant solution. The downside is that Bullet is as fast or as slow as its tick period, which can only go as low at 1 s in Storm.\n\n\nFor example, when the final data is emitted from the Filter bolts when the query has expired, the Join bolt receiving it waits for 3 (this is configurable) ticks after \nits query\n expires to collect all the last intermediate results from the Filter bolts. If the tick period is set as high as 5 s, this means that a query will take 3 * 15 or 15 s to get back after its expiry! Setting it to 1 s, makes it 1 * 3 s. By changing the number of ticks that the Join bolt waits for and the tick period, you can get to any integral delay \n= 1 s.\n\n\n\n\nData processing\n\n\nBullet can accept arbitrary sources of data as long as they can be read from Storm. You can either:\n\n\n\n\nWrite a Storm spout that reads your data from whereever it is (Kafka, etc) and \nconverts it to Bullet Records\n. See \nQuick Start\n for an example.\n\n\nHook up an existing topology that is doing something else directly to Bullet. You will still write and hook up a component that converts your data into Bullet Records in your existing topology.\n\n\n\n\nOption 2 is nice if you do not want to introduce a persistence layer between your existing Streaming pipeline and Bullet. For example, if you just want periodically look at some data within your topology, you could filter them, convert them into Bullet Records and send it into Bullet. You could also sample data. The downside of Option 2 is that you will directly couple your topology with Bullet leaving your topology to be affected by Bullet through Storm features like backpressure (if you are on Storm 1.0) etc. You could also go with Option 2 if you need something more complex than just a spout from Option 1. For example, you may want to process your data in some fashion before emitting to Bullet.\n\n\nYour data is then emitted to the Filter bolt which promptly drops all Bullet Records and does absolutely nothing if you have no queries in your system. If there are queries in the Filter bolt, the record is checked against the \nfilters\n in each query and if it matches, it is processed by\nthe query. Each query can choose to emit matched records in micro-batches. For example, queries that collect raw records (a LIMIT operation) do not micro-batch at all. Every matched record (up to the maximum for the query) is emitted. Queries that aggregate, on the other hand, keep the query around till its\nduration is up and emit the local result.\n\n\nRequest processing\n\n\nStorm DRPC handles receiving REST requests for the whole topology. The DRPC spouts fetch these requests (DRPC knows the request is for the Bullet topology using the unique function name set when launching the topology) and shuffle them to the Prepare Request bolts. The request also contains information about how to return the response back to the DRPC servers. The Prepare Request bolts generate unique identifiers for each request (a Bullet query) and broadcasts them to every Filter bolt. Since every Filter bolt has a copy of every query, the shuffled data from the source of data will match or not match the query no matter which particular Filter bolt it ends up at. Each Filter bolt has access to the unique query id and is able to key group by the id to the Join bolt with the intermediate results for the query.\n\n\nThe Prepare Request bolt also key groups the query and the return information to the Join bolts.\n\n\nCombining\n\n\nSince the data from the Prepare Request bolt (a query and a piece of return information for the query) and the data from all Filter bolts (intermediate results) is key grouped by the unique query id, only one particular Join bolt receives both the query and all the intermediate results for a particular query. The Join bolt can then combine all the intermediate results and produce a final result. This final result is joined (hence the name) with the return information for the query and is shuffled to the Return Results bolt. This bolt then uses the return information to send the results back to a DRPC server, who then returns it back to the requestee.\n\n\n\n\nCombining and operations\n\n\nIn order to be able to combine intermediate results and process data in any order, all aggreations that Bullet does need to be associative and have an identity. In other words, they need to be \nMonoids\n. Luckily for us, the \nDataSketches\n that we use are monoids (actually are commutative monoids). Sketches be unioned and thus all the aggregations we support - SUM, COUNT, MIN, MAX, AVG, COUNT DISTINCTS, DISTINCT - are monoidal. (AVG is monoidal if you store a SUM and a COUNT instead).\n\n\n\n\nScalability\n\n\nThe topology set up this way scales horizontally and has some nice properties:\n\n\n\n\nIf you want to scale for processing more data but the same amount of queries, you only need to scale the components that read your data (the spout reading the data or your custom topology) and the Filter bolts.\n\n\nIf you want to scale for more queries but the same amount of data, you only need to scale up the DRPC spouts, Prepare Request bolts, Join bolts and Return Results bolts (first order). These components generally have low parallelism compared to your data since the data is generally much higher.\n\n\n\n\n\n\nFirst order?\n\n\nIf you send more queries to the Filter bolt, it will be limited by at most how many queries a Filter bolt can store and still process data efficiently. Factors like CPU, memory allocations etc for the Filter bolts come in to the picture in addition to the parallelism. Generally, if you have allocated enough Filter bolts to process your data with enough head room, this should let you run hundreds of queries simultaneously before you run into these issues.", 
            "title": "Storm Architecture"
        }, 
        {
            "location": "/backend/storm-architecture/#storm-architecture", 
            "text": "This section describes how the  Backend architecture  is implemented in Storm.", 
            "title": "Storm architecture"
        }, 
        {
            "location": "/backend/storm-architecture/#storm-drpc", 
            "text": "Bullet on  Storm  is built using  Storm DRPC . DRPC or Distributed Remote Procedure Call, is built into Storm and consist of a set of servers that are part of the Storm cluster. When a Storm topology that uses DRPC is launched, it registers a spout with a unique name (the procedure in the Distributed Remote Procedure Call) with the DRPC infrastructure. The DRPC Servers expose a REST endpoint where data can be POSTed to or a GET request can be made with this unique name. The DRPC infrastructure then sends the request (a query in Bullet) through the spout(s) to the topology (Bullet). The result from Bullet is sent back to the client. We picked Storm to implement Bullet on first not only because it was the most popular Streaming framework at Yahoo but also since DRPC provides us a nice and simple way to handle getting queries into Bullet and sending responses back.   Thrift and DRPC servers  DRPC also exposes a  Thrift  endpoint but the Bullet Web Service uses REST for simplicity. When you launch your Bullet Storm topology, you can POST Bullet queries to a DRPC server directly with the function name that you specified in the Bullet configuration. This is a quick way to check if your topology is up and running!", 
            "title": "Storm DRPC"
        }, 
        {
            "location": "/backend/storm-architecture/#topology", 
            "text": "For Bullet on Storm, the Storm topology implements the backend piece from the full  Architecture . The topology is implemented with the standard Storm spout and bolt components:   The components in  Architecture  have direct counterparts here. The DRPC servers, the DRPC spouts, the Prepare Request bolts comprise the Request Processor. The Filter bolts and your plugin for your source of Data make up the Data Processor. The Join bolt and the Return Results bolt make up the Combiner.  The red colored lines are the path for the queries that come in through Storm DRPC and the blue is for the data from your data source. The pattern on the lines denote how the data (Storm tuples) is moved to the next component. Dashed indicates a broadcast (sent to all instances of the component), dotted indicates a key grouping (sent to a particular instance based on hashing on a particular field), and solid indicates a shuffle (randomly sent to an instance).   What's a Ticker?  The Ticker component is attached to the Filter and Join Bolts produce Storm tuples at predefined intervals. This is a Storm feature (and is configurable when you launch the Bullet topology). These tuples, called tick tuples, behave like a CPU clock cycles for Bullet. Bullet performs all its system related activities on a tick. This includes purging stale queries, emitting left over data for queries, etc. We could have gone the route of having asynchronous threads that do the same thing but this was a far more simpler and elegant solution. The downside is that Bullet is as fast or as slow as its tick period, which can only go as low at 1 s in Storm.  For example, when the final data is emitted from the Filter bolts when the query has expired, the Join bolt receiving it waits for 3 (this is configurable) ticks after  its query  expires to collect all the last intermediate results from the Filter bolts. If the tick period is set as high as 5 s, this means that a query will take 3 * 15 or 15 s to get back after its expiry! Setting it to 1 s, makes it 1 * 3 s. By changing the number of ticks that the Join bolt waits for and the tick period, you can get to any integral delay  = 1 s.", 
            "title": "Topology"
        }, 
        {
            "location": "/backend/storm-architecture/#data-processing", 
            "text": "Bullet can accept arbitrary sources of data as long as they can be read from Storm. You can either:   Write a Storm spout that reads your data from whereever it is (Kafka, etc) and  converts it to Bullet Records . See  Quick Start  for an example.  Hook up an existing topology that is doing something else directly to Bullet. You will still write and hook up a component that converts your data into Bullet Records in your existing topology.   Option 2 is nice if you do not want to introduce a persistence layer between your existing Streaming pipeline and Bullet. For example, if you just want periodically look at some data within your topology, you could filter them, convert them into Bullet Records and send it into Bullet. You could also sample data. The downside of Option 2 is that you will directly couple your topology with Bullet leaving your topology to be affected by Bullet through Storm features like backpressure (if you are on Storm 1.0) etc. You could also go with Option 2 if you need something more complex than just a spout from Option 1. For example, you may want to process your data in some fashion before emitting to Bullet.  Your data is then emitted to the Filter bolt which promptly drops all Bullet Records and does absolutely nothing if you have no queries in your system. If there are queries in the Filter bolt, the record is checked against the  filters  in each query and if it matches, it is processed by\nthe query. Each query can choose to emit matched records in micro-batches. For example, queries that collect raw records (a LIMIT operation) do not micro-batch at all. Every matched record (up to the maximum for the query) is emitted. Queries that aggregate, on the other hand, keep the query around till its\nduration is up and emit the local result.", 
            "title": "Data processing"
        }, 
        {
            "location": "/backend/storm-architecture/#request-processing", 
            "text": "Storm DRPC handles receiving REST requests for the whole topology. The DRPC spouts fetch these requests (DRPC knows the request is for the Bullet topology using the unique function name set when launching the topology) and shuffle them to the Prepare Request bolts. The request also contains information about how to return the response back to the DRPC servers. The Prepare Request bolts generate unique identifiers for each request (a Bullet query) and broadcasts them to every Filter bolt. Since every Filter bolt has a copy of every query, the shuffled data from the source of data will match or not match the query no matter which particular Filter bolt it ends up at. Each Filter bolt has access to the unique query id and is able to key group by the id to the Join bolt with the intermediate results for the query.  The Prepare Request bolt also key groups the query and the return information to the Join bolts.", 
            "title": "Request processing"
        }, 
        {
            "location": "/backend/storm-architecture/#combining", 
            "text": "Since the data from the Prepare Request bolt (a query and a piece of return information for the query) and the data from all Filter bolts (intermediate results) is key grouped by the unique query id, only one particular Join bolt receives both the query and all the intermediate results for a particular query. The Join bolt can then combine all the intermediate results and produce a final result. This final result is joined (hence the name) with the return information for the query and is shuffled to the Return Results bolt. This bolt then uses the return information to send the results back to a DRPC server, who then returns it back to the requestee.   Combining and operations  In order to be able to combine intermediate results and process data in any order, all aggreations that Bullet does need to be associative and have an identity. In other words, they need to be  Monoids . Luckily for us, the  DataSketches  that we use are monoids (actually are commutative monoids). Sketches be unioned and thus all the aggregations we support - SUM, COUNT, MIN, MAX, AVG, COUNT DISTINCTS, DISTINCT - are monoidal. (AVG is monoidal if you store a SUM and a COUNT instead).", 
            "title": "Combining"
        }, 
        {
            "location": "/backend/storm-architecture/#scalability", 
            "text": "The topology set up this way scales horizontally and has some nice properties:   If you want to scale for processing more data but the same amount of queries, you only need to scale the components that read your data (the spout reading the data or your custom topology) and the Filter bolts.  If you want to scale for more queries but the same amount of data, you only need to scale up the DRPC spouts, Prepare Request bolts, Join bolts and Return Results bolts (first order). These components generally have low parallelism compared to your data since the data is generally much higher.    First order?  If you send more queries to the Filter bolt, it will be limited by at most how many queries a Filter bolt can store and still process data efficiently. Factors like CPU, memory allocations etc for the Filter bolts come in to the picture in addition to the parallelism. Generally, if you have allocated enough Filter bolts to process your data with enough head room, this should let you run hundreds of queries simultaneously before you run into these issues.", 
            "title": "Scalability"
        }, 
        {
            "location": "/backend/ingestion/", 
            "text": "Data Ingestion\n\n\nBullet operates on a generic data container that it understands. In order to get Bullet to operate on your data, you need to convert your data records into this format. This conversion is usually done when you plug in your data source into Bullet. Bullet does not make any assumptions on where you get this data from. It could be \nKafka\n, \nRabbitMQ\n, or something else.\n\n\n\n\nIf you are trying to set up Bullet...\n\n\nThe rest of this page gives more information about the Record container and how to depend on it in code directly. If you are setting up Bullet, the Record is already included by default with the Bullet artifact. You can head on over to \nsetting up the Storm topology\n to build the piece that gets your data into the Record container.\n\n\n\n\nBullet Record\n\n\nThe Bullet Record is a serializable data container based on \nAvro\n. It is typed and has a generic schema. You can refer to the \nAvro Schema\n file for details if you wish to see the internals of the data model. The Bullet Record is also lazy and only deserializes itself when you try to read something from it. So, you can pass it around before sending to Bullet with minimal cost. Partial deserialization is being considered if performance is key. This will let you deserialize a much narrower chunk of the Record if you are just looking for a couple of fields.\n\n\nTypes\n\n\nData placed into a Bullet Record is strongly typed. We support these types currently:\n\n\nPrimitives\n\n\n\n\nBoolean\n\n\nLong\n\n\nDouble\n\n\nString\n\n\n\n\nComplex\n\n\n\n\nMap of Strings to any of the \nPrimitives\n\n\nMap of Strings to any Map in 1\n\n\nList of any Map in 1\n\n\n\n\nWith these types, it is unlikely you would have data that cannot be represented as Bullet Record but if you do, please let us know and we are more than willing to accommodate.\n\n\nInstalling the Record directly\n\n\nGenerally, you depend on the Bullet artifact for your Stream Processor when you plug in the piece that gets your data into the Stream processor. The Bullet artifact already brings in the Bullet Record container as well. See the usage for the \nStorm\n.\n\n\nHowever, if you need it, the artifacts are available through JCenter to depend on them in code directly. You will need to add the repository. Below is a Maven example:\n\n\nrepositories\n\n    \nrepository\n\n        \nsnapshots\n\n            \nenabled\nfalse\n/enabled\n\n        \n/snapshots\n\n        \nid\ncentral\n/id\n\n        \nname\nbintray\n/name\n\n        \nurl\nhttp://jcenter.bintray.com\n/url\n\n    \n/repository\n\n\n/repositories\n\n\n\n\n\ndependency\n\n  \ngroupId\ncom.yahoo.bullet\n/groupId\n\n  \nartifactId\nbullet-record\n/artifactId\n\n  \nversion\n${bullet.version}\n/version\n\n\n/dependency\n\n\n\n\n\nIf you just need the jar artifact, you can download it directly from \nJCenter\n.\n\n\nYou can also add \nclassifier\nsources\n/classifier\n  or \nclassifier\njavadoc\n/classifier\n if you want the sources or the javadoc.", 
            "title": "Getting your data into Bullet"
        }, 
        {
            "location": "/backend/ingestion/#data-ingestion", 
            "text": "Bullet operates on a generic data container that it understands. In order to get Bullet to operate on your data, you need to convert your data records into this format. This conversion is usually done when you plug in your data source into Bullet. Bullet does not make any assumptions on where you get this data from. It could be  Kafka ,  RabbitMQ , or something else.   If you are trying to set up Bullet...  The rest of this page gives more information about the Record container and how to depend on it in code directly. If you are setting up Bullet, the Record is already included by default with the Bullet artifact. You can head on over to  setting up the Storm topology  to build the piece that gets your data into the Record container.", 
            "title": "Data Ingestion"
        }, 
        {
            "location": "/backend/ingestion/#bullet-record", 
            "text": "The Bullet Record is a serializable data container based on  Avro . It is typed and has a generic schema. You can refer to the  Avro Schema  file for details if you wish to see the internals of the data model. The Bullet Record is also lazy and only deserializes itself when you try to read something from it. So, you can pass it around before sending to Bullet with minimal cost. Partial deserialization is being considered if performance is key. This will let you deserialize a much narrower chunk of the Record if you are just looking for a couple of fields.", 
            "title": "Bullet Record"
        }, 
        {
            "location": "/backend/ingestion/#types", 
            "text": "Data placed into a Bullet Record is strongly typed. We support these types currently:", 
            "title": "Types"
        }, 
        {
            "location": "/backend/ingestion/#primitives", 
            "text": "Boolean  Long  Double  String", 
            "title": "Primitives"
        }, 
        {
            "location": "/backend/ingestion/#complex", 
            "text": "Map of Strings to any of the  Primitives  Map of Strings to any Map in 1  List of any Map in 1   With these types, it is unlikely you would have data that cannot be represented as Bullet Record but if you do, please let us know and we are more than willing to accommodate.", 
            "title": "Complex"
        }, 
        {
            "location": "/backend/ingestion/#installing-the-record-directly", 
            "text": "Generally, you depend on the Bullet artifact for your Stream Processor when you plug in the piece that gets your data into the Stream processor. The Bullet artifact already brings in the Bullet Record container as well. See the usage for the  Storm .  However, if you need it, the artifacts are available through JCenter to depend on them in code directly. You will need to add the repository. Below is a Maven example:  repositories \n     repository \n         snapshots \n             enabled false /enabled \n         /snapshots \n         id central /id \n         name bintray /name \n         url http://jcenter.bintray.com /url \n     /repository  /repositories   dependency \n   groupId com.yahoo.bullet /groupId \n   artifactId bullet-record /artifactId \n   version ${bullet.version} /version  /dependency   If you just need the jar artifact, you can download it directly from  JCenter .  You can also add  classifier sources /classifier   or  classifier javadoc /classifier  if you want the sources or the javadoc.", 
            "title": "Installing the Record directly"
        }, 
        {
            "location": "/backend/setup-storm/", 
            "text": "Bullet on Storm\n\n\nStorm DRPC\n\n\nBullet on \nStorm\n is built using \nStorm DRPC\n. DRPC comes with Storm installations and generally consist of a set of DRPC servers. When a Storm topology is launched and it uses DRPC, it registers a spout with a unique name with the DRPC infrastructure. The DRPC Servers expose a REST endpoint where data can be POSTed to or a GET request can be made with this unique name. The DRPC infrastructure then sends the request (a query in Bullet) through the spout(s) to the topology (Bullet). Bullet uses the query to filter and joins all records emitted from your (configurable) data source - either a Spout or a topology component according to the query specification. The resulting matched records are aggregated and sent back to the client. We chose to implement Bullet on Storm first since DRPC provides us a nice and simple way to handle getting queries into Bullet and sending responses back.\n\n\nQuery duration in Storm DRPC\n\n\nThe maximum time a query can run for depends on the maximum time Storm DRPC request can last in your Storm topology. Generally the default is set to 10 minutes. This means that the \nlongest query duration possible will be 10 minutes\n. This is up to your cluster maintainers.\n\n\nConfiguration\n\n\nBullet is configured at run-time using settings defined in a file. Settings not overridden will default to the values in \nbullet_defaults.yaml\n. There are too many to list here. You can find out what these settings do in the comments listed in the defaults.\n\n\nInstallation\n\n\nTo use Bullet, you need to implement a way to read from your data source and convert your data into Bullet Records (bullet-record is a transitive dependency for Bullet and can be found \nin Bintray\n. You have two options in how to get your data into Bullet:\n\n\n\n\nYou can implement a Spout that reads from your data source and emits Bullet Record. This spout must have a constructor that takes a List of Strings.\n\n\nYou can pipe your existing Storm topology directly into Bullet. In other words, you convert the data you wish to be queryable through Bullet into Bullet Records from a bolt in your topology.\n\n\n\n\nOption 2 \ndirectly\n couples your topology to Bullet and as such, you would need to watch out for things like backpressure etc.\n\n\nYou need a JVM based project that implements one of the two options above. You include the Bullet artifact and Storm dependencies in your pom.xml or other dependency management system. The artifacts are available through JCenter, so you will need to add the repository.\n\n\nrepositories\n\n    \nrepository\n\n        \nsnapshots\n\n            \nenabled\nfalse\n/enabled\n\n        \n/snapshots\n\n        \nid\ncentral\n/id\n\n        \nname\nbintray\n/name\n\n        \nurl\nhttp://jcenter.bintray.com\n/url\n\n    \n/repository\n\n\n/repositories\n\n\n\n\n\ndependency\n\n  \ngroupId\norg.apache.storm\n/groupId\n\n  \nartifactId\nstorm-core\n/artifactId\n\n  \nversion\n${storm.version}\n/version\n\n  \nscope\nprovided\n/scope\n\n\n/dependency\n\n\n\ndependency\n\n  \ngroupId\ncom.yahoo.bullet\n/groupId\n\n  \nartifactId\nbullet-storm\n/artifactId\n\n  \nversion\n${bullet.version}\n/version\n\n\n/dependency\n\n\n\n\n\nIf you just need the jar artifact directly, you can download it from \nJCenter\n.\n\n\nYou can also add \nclassifier\nsources\n/classifier\n  or \nclassifier\njavadoc\n/classifier\n if you want the sources or javadoc. We also package up our test code where we have some helper classes to deal with \nStorm components\n. If you wish to use these to help with testing your topology, you can add another dependency on bullet-storm with \ntype\ntest-jar\n/type\n.\n\n\nIf you are going to use the second option (directly pipe data into Bullet from your Storm topology), then you will need a main class that directly calls the submit method with your wired up topology and the name of the component that is going to emit Bullet Records in that wired up topology. The submit method can be found in \nTopology.java\n. The submit method submits the topology so it should be the last thing you do in your main.\n\n\nIf you are just implementing a Spout, see the \nLaunch\n section below on how to use the main class in Bullet to create and submit your topology.\n\n\nStorm topologies are generally launched with \"fat\" jars (jar-with-dependencies), excluding storm itself:\n\n\nplugin\n\n    \ngroupId\norg.apache.maven.plugins\n/groupId\n\n    \nartifactId\nmaven-assembly-plugin\n/artifactId\n\n    \nversion\n2.4\n/version\n\n    \nexecutions\n\n        \nexecution\n\n            \nid\nassemble-all\n/id\n\n            \nphase\npackage\n/phase\n\n            \ngoals\n\n                \ngoal\nsingle\n/goal\n\n            \n/goals\n\n        \n/execution\n\n    \n/executions\n\n    \nconfiguration\n\n        \ndescriptorRefs\n\n            \ndescriptorRef\njar-with-dependencies\n/descriptorRef\n\n        \n/descriptorRefs\n\n    \n/configuration\n\n\n/plugin\n\n\n\n\n\nOlder Storm Versions\n\n\nSince package prefixes changed from \nbacktype.storm\n to \norg.apache.storm\n in Storm 1.0 and above, you will need to get the storm-0.10 version of Bullet if\nyour Storm cluster is still not at 1.0 or higher. You change your dependency to:\n\n\ndependency\n\n    \ngroupId\ncom.yahoo.bullet\n/groupId\n\n    \nartifactId\nbullet-storm-0.10\n/artifactId\n\n    \nversion\n${bullet.version}\n/version\n\n\n/dependency\n\n\n\n\n\nThe jar artifact can be downloaded directly from \nJCenter\n.\n\n\nYou can also add \nclassifier\nsources\n/classifier\n  or \nclassifier\njavadoc\n/classifier\n if you want the source or javadoc and \ntype\ntest-jar\n/type\n for the test classes as with bullet-storm.\n\n\nAlso, since storm-metrics and the Resource Aware Scheduler are not in Storm versions less than 1.0, there are changes in the Bullet settings. The settings that set the CPU and memory loads do not exist (so config file does not specify them). The setting to enable the topology scheduler are no longer present (you can still override these settings if you run a custom version of Storm by passing it to the storm jar command. \nSee below\n.) You can take a look the settings file on the storm-0.10 branch in the Git repo.\n\n\nIf for some reason, you are running a version of Storm less than 1.0 that has the RAS backported to it and you wish to set the CPU and other settings, you will your own main class that mirrors the master branch of the main class but with backtype.storm packages instead.\n\n\nLaunch\n\n\nIf you have implemented your own main class (option 2 above), you just pass your main class to the storm executable as usual. If you are implementing a spout, here's an example of how you could launch the topology:\n\n\nstorm jar your-fat-jar-with-dependencies.jar \\\n          com.yahoo.bullet.Topology \\\n          --bullet-conf path/to/the/bullet_settings.yaml \\\n          --bullet-spout full.package.prefix.to.your.spout.implementation \\\n          --bullet-spout-parallelism 64 \\\n          --bullet-spout-cpu-load 200.0 \\\n          --bullet-spout-on-heap-memory-load 512.0 \\\n          --bullet-spout-off-heap-memory-load 256.0 \\\n          --bullet-spout-arg arg-to-your-spout-class-for-ex-a-path-to-a-config-file \\\n          --bullet-spout-arg another-arg-to-your-spout-class \\\n          -c topology.acker.executors=64 \\\n          -c topology.max.spout.pending=10000\n\n\n\n\nYou can pass other arguments to Storm using the -c argument. The example above uses 64 ackers, which is the parallelism of the Filter Bolt. Storm DRPC follows the principle of leaving retries to the DRPC user (in our case, the Bullet web service). As a result, most of the DRPC components do not follow any at least once guarantees. However, you can enable at least once for the hop from your topology (or spout) to the Filter Bolt. This is why this example uses the parallelism of the Filter Bolt as the number of ackers since that is exactly the number of acker tasks we would need. Ackers are lightweight so you need not have the same number of tasks as Filter Bolts but you can tweak it accordingly. The example above also sets max spout pending to control how fast the spout emits. You could use the backpressure mechanisms in Storm in addition or in lieu of as you choose. We have found that max spout pending gives a much more predictable way of throttling our spouts during catch up or data spikes.\n\n\n\n\nMain Class Arguments\n\n\nIf you run the main class without arguments or pass in the \n--help\n argument, you can see what these arguments mean and what others are supported.\n\n\n\n\nTest\n\n\nOnce the topology is up and your data consumption has stabilized, you could post a query to a DRPC server in your cluster. Try a simple query from the \nexamples\n by running a curl from a command line:\n\n\ncurl -s -X POST -d '{}' http://\nDRPC_HOST\n:\nDRPC_PORT\n/drpc/\nYOUR_TOPOLOGY_FUNCTION_FROM_YOUR_BULLET_CONF\n\n\n\n\n\nYou should receive a random record flowing through Bullet instantly (if you left the Raw aggregation micro-batch size at the default of 1).", 
            "title": "Setup on Storm"
        }, 
        {
            "location": "/backend/setup-storm/#bullet-on-storm", 
            "text": "", 
            "title": "Bullet on Storm"
        }, 
        {
            "location": "/backend/setup-storm/#storm-drpc", 
            "text": "Bullet on  Storm  is built using  Storm DRPC . DRPC comes with Storm installations and generally consist of a set of DRPC servers. When a Storm topology is launched and it uses DRPC, it registers a spout with a unique name with the DRPC infrastructure. The DRPC Servers expose a REST endpoint where data can be POSTed to or a GET request can be made with this unique name. The DRPC infrastructure then sends the request (a query in Bullet) through the spout(s) to the topology (Bullet). Bullet uses the query to filter and joins all records emitted from your (configurable) data source - either a Spout or a topology component according to the query specification. The resulting matched records are aggregated and sent back to the client. We chose to implement Bullet on Storm first since DRPC provides us a nice and simple way to handle getting queries into Bullet and sending responses back.", 
            "title": "Storm DRPC"
        }, 
        {
            "location": "/backend/setup-storm/#query-duration-in-storm-drpc", 
            "text": "The maximum time a query can run for depends on the maximum time Storm DRPC request can last in your Storm topology. Generally the default is set to 10 minutes. This means that the  longest query duration possible will be 10 minutes . This is up to your cluster maintainers.", 
            "title": "Query duration in Storm DRPC"
        }, 
        {
            "location": "/backend/setup-storm/#configuration", 
            "text": "Bullet is configured at run-time using settings defined in a file. Settings not overridden will default to the values in  bullet_defaults.yaml . There are too many to list here. You can find out what these settings do in the comments listed in the defaults.", 
            "title": "Configuration"
        }, 
        {
            "location": "/backend/setup-storm/#installation", 
            "text": "To use Bullet, you need to implement a way to read from your data source and convert your data into Bullet Records (bullet-record is a transitive dependency for Bullet and can be found  in Bintray . You have two options in how to get your data into Bullet:   You can implement a Spout that reads from your data source and emits Bullet Record. This spout must have a constructor that takes a List of Strings.  You can pipe your existing Storm topology directly into Bullet. In other words, you convert the data you wish to be queryable through Bullet into Bullet Records from a bolt in your topology.   Option 2  directly  couples your topology to Bullet and as such, you would need to watch out for things like backpressure etc.  You need a JVM based project that implements one of the two options above. You include the Bullet artifact and Storm dependencies in your pom.xml or other dependency management system. The artifacts are available through JCenter, so you will need to add the repository.  repositories \n     repository \n         snapshots \n             enabled false /enabled \n         /snapshots \n         id central /id \n         name bintray /name \n         url http://jcenter.bintray.com /url \n     /repository  /repositories   dependency \n   groupId org.apache.storm /groupId \n   artifactId storm-core /artifactId \n   version ${storm.version} /version \n   scope provided /scope  /dependency  dependency \n   groupId com.yahoo.bullet /groupId \n   artifactId bullet-storm /artifactId \n   version ${bullet.version} /version  /dependency   If you just need the jar artifact directly, you can download it from  JCenter .  You can also add  classifier sources /classifier   or  classifier javadoc /classifier  if you want the sources or javadoc. We also package up our test code where we have some helper classes to deal with  Storm components . If you wish to use these to help with testing your topology, you can add another dependency on bullet-storm with  type test-jar /type .  If you are going to use the second option (directly pipe data into Bullet from your Storm topology), then you will need a main class that directly calls the submit method with your wired up topology and the name of the component that is going to emit Bullet Records in that wired up topology. The submit method can be found in  Topology.java . The submit method submits the topology so it should be the last thing you do in your main.  If you are just implementing a Spout, see the  Launch  section below on how to use the main class in Bullet to create and submit your topology.  Storm topologies are generally launched with \"fat\" jars (jar-with-dependencies), excluding storm itself:  plugin \n     groupId org.apache.maven.plugins /groupId \n     artifactId maven-assembly-plugin /artifactId \n     version 2.4 /version \n     executions \n         execution \n             id assemble-all /id \n             phase package /phase \n             goals \n                 goal single /goal \n             /goals \n         /execution \n     /executions \n     configuration \n         descriptorRefs \n             descriptorRef jar-with-dependencies /descriptorRef \n         /descriptorRefs \n     /configuration  /plugin", 
            "title": "Installation"
        }, 
        {
            "location": "/backend/setup-storm/#older-storm-versions", 
            "text": "Since package prefixes changed from  backtype.storm  to  org.apache.storm  in Storm 1.0 and above, you will need to get the storm-0.10 version of Bullet if\nyour Storm cluster is still not at 1.0 or higher. You change your dependency to:  dependency \n     groupId com.yahoo.bullet /groupId \n     artifactId bullet-storm-0.10 /artifactId \n     version ${bullet.version} /version  /dependency   The jar artifact can be downloaded directly from  JCenter .  You can also add  classifier sources /classifier   or  classifier javadoc /classifier  if you want the source or javadoc and  type test-jar /type  for the test classes as with bullet-storm.  Also, since storm-metrics and the Resource Aware Scheduler are not in Storm versions less than 1.0, there are changes in the Bullet settings. The settings that set the CPU and memory loads do not exist (so config file does not specify them). The setting to enable the topology scheduler are no longer present (you can still override these settings if you run a custom version of Storm by passing it to the storm jar command.  See below .) You can take a look the settings file on the storm-0.10 branch in the Git repo.  If for some reason, you are running a version of Storm less than 1.0 that has the RAS backported to it and you wish to set the CPU and other settings, you will your own main class that mirrors the master branch of the main class but with backtype.storm packages instead.", 
            "title": "Older Storm Versions"
        }, 
        {
            "location": "/backend/setup-storm/#launch", 
            "text": "If you have implemented your own main class (option 2 above), you just pass your main class to the storm executable as usual. If you are implementing a spout, here's an example of how you could launch the topology:  storm jar your-fat-jar-with-dependencies.jar \\\n          com.yahoo.bullet.Topology \\\n          --bullet-conf path/to/the/bullet_settings.yaml \\\n          --bullet-spout full.package.prefix.to.your.spout.implementation \\\n          --bullet-spout-parallelism 64 \\\n          --bullet-spout-cpu-load 200.0 \\\n          --bullet-spout-on-heap-memory-load 512.0 \\\n          --bullet-spout-off-heap-memory-load 256.0 \\\n          --bullet-spout-arg arg-to-your-spout-class-for-ex-a-path-to-a-config-file \\\n          --bullet-spout-arg another-arg-to-your-spout-class \\\n          -c topology.acker.executors=64 \\\n          -c topology.max.spout.pending=10000  You can pass other arguments to Storm using the -c argument. The example above uses 64 ackers, which is the parallelism of the Filter Bolt. Storm DRPC follows the principle of leaving retries to the DRPC user (in our case, the Bullet web service). As a result, most of the DRPC components do not follow any at least once guarantees. However, you can enable at least once for the hop from your topology (or spout) to the Filter Bolt. This is why this example uses the parallelism of the Filter Bolt as the number of ackers since that is exactly the number of acker tasks we would need. Ackers are lightweight so you need not have the same number of tasks as Filter Bolts but you can tweak it accordingly. The example above also sets max spout pending to control how fast the spout emits. You could use the backpressure mechanisms in Storm in addition or in lieu of as you choose. We have found that max spout pending gives a much more predictable way of throttling our spouts during catch up or data spikes.   Main Class Arguments  If you run the main class without arguments or pass in the  --help  argument, you can see what these arguments mean and what others are supported.", 
            "title": "Launch"
        }, 
        {
            "location": "/backend/setup-storm/#test", 
            "text": "Once the topology is up and your data consumption has stabilized, you could post a query to a DRPC server in your cluster. Try a simple query from the  examples  by running a curl from a command line:  curl -s -X POST -d '{}' http:// DRPC_HOST : DRPC_PORT /drpc/ YOUR_TOPOLOGY_FUNCTION_FROM_YOUR_BULLET_CONF   You should receive a random record flowing through Bullet instantly (if you left the Raw aggregation micro-batch size at the default of 1).", 
            "title": "Test"
        }, 
        {
            "location": "/backend/performance/", 
            "text": "Coming soon!", 
            "title": "Performance"
        }, 
        {
            "location": "/backend/performance/#coming-soon", 
            "text": "", 
            "title": "Coming soon!"
        }, 
        {
            "location": "/ws/setup/", 
            "text": "The Web Service\n\n\nThe Web Service is a Java WAR file that you can deploy on a machine to communicate with the Bullet Backend. For Storm, it talks to the Storm DRPC servers. To set up the Bullet backend topology, see \nStorm setup\n.\n\n\nThere are two main purposes for this layer at this time:\n\n\n1) It provides an endpoint that can serve a \nJSON API schema\n for the Bullet UI. Currently, static schemas from a file are supported.\n\n\n2) It proxies a JSON Bullet query to Bullet and wraps errors if the backend is unreachable.\n\n\n\n\nThat's it?\n\n\nThe Web Service essentially just wraps Storm DRPC and provides some helpful endpoints. But the Web Service is there to be a point of abstraction for implementing things like security, monitoring, access-control, rate-limiting, different query formats (e.g. SQL Bullet queries) etc, which are planned in the near future.\n\n\n\n\nPrerequisites\n\n\nIn order for your Web Service to work with Bullet, you should have an instance of the \nbackend\n already set up.\n\n\nInstallation\n\n\nYou can download the WAR file directly from \nJCenter\n.\n\n\nIf you need to depend on the source code directly for any reason, you need to add the JCenter repository and get the artifact through your dependency management system. Maven is shown below.\n\n\nrepositories\n\n    \nrepository\n\n        \nsnapshots\n\n            \nenabled\nfalse\n/enabled\n\n        \n/snapshots\n\n        \nid\ncentral\n/id\n\n        \nname\nbintray\n/name\n\n        \nurl\nhttp://jcenter.bintray.com\n/url\n\n    \n/repository\n\n\n/repositories\n\n\n\n\n\ndependency\n\n  \ngroupId\ncom.yahoo.bullet\n/groupId\n\n  \nartifactId\nbullet-service\n/artifactId\n\n  \nversion\n${bullet.version}\n/version\n\n\n/dependency\n\n\n\n\n\nYou can also add \nclassifier\nsources\n/classifier\n  or \nclassifier\njavadoc\n/classifier\n if you want the source or javadoc.\n\n\nConfiguration\n\n\nYou specify how to talk to your Bullet instance and where to find your schema file (optional for if you want to power the \nUI\n schema with a static file) through a configuration file. See sample at \nbullet_defaults.yaml\n.\n\n\nThe values in the defaults file are used for any missing properties. You can specify a path to your custom configuration using the property:\n\n\nbullet.service.configuration.file=\npath to your configuration file\n\n\n\n\n\nFor example, if you are using Jetty as your servlet container,\n\n\njava -jar -Dbullet.service.configuration.file=/var/bullet-service/context.properties start.jar\n\n\n\n\n\n\nSpring and context\n\n\nThe Web Service uses your passed in configuration properties file to configure its dependency injections using \nSpring\n. See \nApplicationContext.xml\n for how this is loaded.\n\n\n\n\nFile based schema\n\n\nThe Web Service can also provide a endpoint that serves your data schema to your UI. You do not necessarily have to use this to serve your schema. The UI can use any JSON API schema specification. But if your schema is fixed or does not change often, it might be simpler for you to use this endpoint to provide the schema for the UI, instead of creating a new one. The Web Service also takes care to provide the right \nCORS\n headers so that your UI can communicate with it.\n\n\nYou can use \nsample_columns.json\n as a guideline for what your actual schema file should look like or if you want to create your own Web Service that dynamically serves your schema to the UI if it changes frequently.\n\n\nLaunch\n\n\nYou need to deploy the WAR file to a servlet container. We recommend \nJetty\n.\n\n\nQuick start with Jetty\n\n\n\n\nDownload a \nJetty installation\n\n\nUnarchive the installation into a folder\n\n\nDownload the Bullet Service WAR from \nJCenter\n.\n\n\nPlace the WAR into your Jetty installation folder's \nwebapps\n directory.\n\n\nCreate a properties file containing actual values for the settings in the \ndefaults\n. For example, for Bullet on Storm, you will point to your DRPC servers in the properties. If you want to use the file based schema endpoint, you would point to your schema file.\n\n\nLaunch Jetty using \njava -jar -Dbullet.service.configuration.file=/path/to/your/properties/file start.jar\n, where start.jar is in your Jetty installation folder.\n\n\n\n\nYou should tweak and properly install Jetty into a global location with proper logging when you productionize your Web Service.\n\n\nUsage\n\n\nOnce the Web Service is up (defaults to port 8080, you can change it with a \n-Djetty.http.port=\nPORT\n setting), you should be able to test to see if it's able to talk to the Bullet backend:\n\n\nYou can HTTP POST a Bullet query to the API with:\n\n\ncurl -s -H \nContent-Type: application/json\n -X POST -d '{}' http://localhost:8080/bullet-service/api/drpc\n\n\n\n\nYou should receive a random record flowing through Bullet instantly (if you left the Raw aggregation micro-batch size at the default of 1 when launching the Bullet backend).\n\n\n\n\nContext Path\n\n\nThe context path, or \"bullet-service\" in the URL above is the name of the WAR file in Jetty. If you rename it, you will need to change this.\n\n\n\n\nIf you provided a path to a schema file in your configuration file when you \nlaunch\n the Web Service, you can also HTTP GET your schema at \nhttp://localhost:8080/bullet-service/api/columns\n\n\nIf you did not, the schema in \nsample_columns.json\n is the response. The Web Service converts it to a JSON API response and provides the right headers for CORS.", 
            "title": "Setup"
        }, 
        {
            "location": "/ws/setup/#the-web-service", 
            "text": "The Web Service is a Java WAR file that you can deploy on a machine to communicate with the Bullet Backend. For Storm, it talks to the Storm DRPC servers. To set up the Bullet backend topology, see  Storm setup .  There are two main purposes for this layer at this time:  1) It provides an endpoint that can serve a  JSON API schema  for the Bullet UI. Currently, static schemas from a file are supported.  2) It proxies a JSON Bullet query to Bullet and wraps errors if the backend is unreachable.   That's it?  The Web Service essentially just wraps Storm DRPC and provides some helpful endpoints. But the Web Service is there to be a point of abstraction for implementing things like security, monitoring, access-control, rate-limiting, different query formats (e.g. SQL Bullet queries) etc, which are planned in the near future.", 
            "title": "The Web Service"
        }, 
        {
            "location": "/ws/setup/#prerequisites", 
            "text": "In order for your Web Service to work with Bullet, you should have an instance of the  backend  already set up.", 
            "title": "Prerequisites"
        }, 
        {
            "location": "/ws/setup/#installation", 
            "text": "You can download the WAR file directly from  JCenter .  If you need to depend on the source code directly for any reason, you need to add the JCenter repository and get the artifact through your dependency management system. Maven is shown below.  repositories \n     repository \n         snapshots \n             enabled false /enabled \n         /snapshots \n         id central /id \n         name bintray /name \n         url http://jcenter.bintray.com /url \n     /repository  /repositories   dependency \n   groupId com.yahoo.bullet /groupId \n   artifactId bullet-service /artifactId \n   version ${bullet.version} /version  /dependency   You can also add  classifier sources /classifier   or  classifier javadoc /classifier  if you want the source or javadoc.", 
            "title": "Installation"
        }, 
        {
            "location": "/ws/setup/#configuration", 
            "text": "You specify how to talk to your Bullet instance and where to find your schema file (optional for if you want to power the  UI  schema with a static file) through a configuration file. See sample at  bullet_defaults.yaml .  The values in the defaults file are used for any missing properties. You can specify a path to your custom configuration using the property:  bullet.service.configuration.file= path to your configuration file   For example, if you are using Jetty as your servlet container,  java -jar -Dbullet.service.configuration.file=/var/bullet-service/context.properties start.jar   Spring and context  The Web Service uses your passed in configuration properties file to configure its dependency injections using  Spring . See  ApplicationContext.xml  for how this is loaded.", 
            "title": "Configuration"
        }, 
        {
            "location": "/ws/setup/#file-based-schema", 
            "text": "The Web Service can also provide a endpoint that serves your data schema to your UI. You do not necessarily have to use this to serve your schema. The UI can use any JSON API schema specification. But if your schema is fixed or does not change often, it might be simpler for you to use this endpoint to provide the schema for the UI, instead of creating a new one. The Web Service also takes care to provide the right  CORS  headers so that your UI can communicate with it.  You can use  sample_columns.json  as a guideline for what your actual schema file should look like or if you want to create your own Web Service that dynamically serves your schema to the UI if it changes frequently.", 
            "title": "File based schema"
        }, 
        {
            "location": "/ws/setup/#launch", 
            "text": "You need to deploy the WAR file to a servlet container. We recommend  Jetty .", 
            "title": "Launch"
        }, 
        {
            "location": "/ws/setup/#quick-start-with-jetty", 
            "text": "Download a  Jetty installation  Unarchive the installation into a folder  Download the Bullet Service WAR from  JCenter .  Place the WAR into your Jetty installation folder's  webapps  directory.  Create a properties file containing actual values for the settings in the  defaults . For example, for Bullet on Storm, you will point to your DRPC servers in the properties. If you want to use the file based schema endpoint, you would point to your schema file.  Launch Jetty using  java -jar -Dbullet.service.configuration.file=/path/to/your/properties/file start.jar , where start.jar is in your Jetty installation folder.   You should tweak and properly install Jetty into a global location with proper logging when you productionize your Web Service.", 
            "title": "Quick start with Jetty"
        }, 
        {
            "location": "/ws/setup/#usage", 
            "text": "Once the Web Service is up (defaults to port 8080, you can change it with a  -Djetty.http.port= PORT  setting), you should be able to test to see if it's able to talk to the Bullet backend:  You can HTTP POST a Bullet query to the API with:  curl -s -H  Content-Type: application/json  -X POST -d '{}' http://localhost:8080/bullet-service/api/drpc  You should receive a random record flowing through Bullet instantly (if you left the Raw aggregation micro-batch size at the default of 1 when launching the Bullet backend).   Context Path  The context path, or \"bullet-service\" in the URL above is the name of the WAR file in Jetty. If you rename it, you will need to change this.   If you provided a path to a schema file in your configuration file when you  launch  the Web Service, you can also HTTP GET your schema at  http://localhost:8080/bullet-service/api/columns  If you did not, the schema in  sample_columns.json  is the response. The Web Service converts it to a JSON API response and provides the right headers for CORS.", 
            "title": "Usage"
        }, 
        {
            "location": "/ws/api/", 
            "text": "API\n\n\nSee the \nUI Usage section\n for using the UI to build Bullet queries. This section deals with examples of the JSON query format that the API currently exposes (and the UI uses underneath).\n\n\nQuerying\n\n\nBullet queries allow you to filter, project and aggregate data. It lets you fetch raw and aggregated data. Fields inside maps can be accessed using the '.' notation in queries. For example, myMap.key will access the key field inside the myMap map. There is no support for accessing fields inside Lists or inside nested Maps as of yet. Only the entire object can be operated on for now.\n\n\nThe three main sections of a Bullet query are:\n\n\n{\n    \nfilters\n: {},\n    \nprojection\n: {},\n    \naggregation\n: {}.\n    \nduration\n: 20000\n}\n\n\n\n\nThe duration represents how long the query runs for (a window from when you submit it to that many milliseconds into the future). See the \nFilters\n, \nProjections\n and \nAggregation\n sections for their respective specifications. Each of those sections are objects.\n\n\nFilters\n\n\nBullet supports two kinds of filters:\n\n\n\n\nLogical filters\n\n\nRelational filters\n\n\n\n\nLogical Filters\n\n\nLogical filters allow you to combine other filter clauses with logical operations like AND, OR and NOT.\n\n\nThe current logical operators allowed in filters are:\n\n\n\n\n\n\n\n\nLogical Operator\n\n\nMeaning\n\n\n\n\n\n\n\n\n\n\nAND\n\n\nAll filters must be true. The first false filter evaluated left to right will short-circuit the computation.\n\n\n\n\n\n\nOR\n\n\nAny filter must be true. The first true filter evaluated left to right will short-circuit the computation.\n\n\n\n\n\n\nNOT\n\n\nNegates the value of the first filter clause. The filter is satisfied iff the value is true.\n\n\n\n\n\n\n\n\nThe format for a Logical filter is:\n\n\n{\n   \noperation\n: \nAND | OR | NOT\n\n   \nclauses\n: [\n      {\noperation\n: \n...\n, clauses: [{}, ...]},\n      {\nfield\n: \n...\n, \noperation\n: \n, values: [\n...\n]},\n      {\noperation\n: \n...\n, clauses: [{}, ...]}\n      ...\n   ]\n}\n\n\n\n\nAny other type of filter may be provided as a clause in clauses.\n\n\nRelational Filters\n\n\nRelational filters allow you to specify conditions on a field, using a comparison operator and a list of values.\n\n\nThe current comparisons allowed in filters are:\n\n\n\n\n\n\n\n\nComparison\n\n\nMeaning\n\n\n\n\n\n\n\n\n\n\n==\n\n\nEqual to any value in values\n\n\n\n\n\n\n!=\n\n\nNot equal to any value in values\n\n\n\n\n\n\n=\n\n\nLess than or equal to any value in values\n\n\n\n\n\n\n=\n\n\nGreater than or equal to any value in values\n\n\n\n\n\n\n\n\nLess than any value in values\n\n\n\n\n\n\n\n\nGreater than any value in values\n\n\n\n\n\n\nRLIKE\n\n\nMatches using \nJava Regex notation\n, any Regex value in values\n\n\n\n\n\n\n\n\nThese operators are all typed based on the type of the left hand side from the Bullet record. If the elements on the right hand side cannot be\ncasted to the types on the LHS, those items will be ignored for the comparison.\n\n\nThe format for a Relational filter is:\n\n\n{\n    \noperation\n: \n== | != | \n= | \n= | \n | \n | RLIKE\n\n    \nfield\n: \nrecord_field_name | map_field.subfield\n,\n    \nvalues\n: [\n        \nstring values\n,\n        \nthat go here\n,\n        \nwill be casted\n,\n        \nto the\n,\n        \ntype of field\n\n    ]\n}\n\n\n\n\nMultiple top level relational filters behave as if they are ANDed together.\n\n\nProjections\n\n\nProjections allow you to pull out only the fields needed and rename them (renaming is being supported in order to give\nbetter names to fields pulled out from maps). If projections are not specified, the entire record is returned. If you are querying\nfor raw records, you can use projections to help reduce the load on the system and network.\n\n\n{\n    \nprojection\n: {\n        \nfieldA\n: \nnewNameA\n,\n        \nfieldB\n: \nnewNameB\n\n    }\n}\n\n\n\n\nAggregations\n\n\nAggregations allow you to perform some operation on the collected records. They take an optional size to restrict\nthe size of the aggregation (this applies for aggregations high cardinality aggregations and raw records).\n\n\nThe current aggregation types that are supported are:\n\n\n\n\n\n\n\n\nAggregation\n\n\nMeaning\n\n\n\n\n\n\n\n\n\n\nGROUP\n\n\nThe resulting output would be a record containing the result of an operation for each unique group in the specified fields\n\n\n\n\n\n\nCOUNT DISTINCT\n\n\nComputes the number of distinct elements in the fields. (May be approximate)\n\n\n\n\n\n\nLIMIT\n\n\nThe resulting output would be at most the number specified in size.\n\n\n\n\n\n\n\n\nThe current format for an aggregation is (\nnote see above for what is supported at the moment\n):\n\n\n{\n    \ntype\n: \nGROUP | COUNT DISTINCT | TOP | PERCENTILE | RAW\n,\n    \nsize\n: \na limit on the number of resulting records\n,\n    \nfields\n: {\n        \nfields\n: \nnewNameA\n,\n        \nthat go here\n: \nnewNameB\n,\n        \nare what the\n: \nnewNameC\n,\n        \naggregation type applies to\n: \nnewNameD\n\n    },\n    \nattributes\n: {\n        \nthese\n: \nchange\n,\n        \nper\n: [\n           \naggregation type\n\n        ]\n    }\n}\n\n\n\n\nYou can also use LIMIT as an alias for RAW. DISTINCT is also an alias for GROUP. These exist to make some queries read a bit better.\n\n\nCurrently we support GROUP aggregations on the following operations:\n\n\n\n\n\n\n\n\nOperation\n\n\nMeaning\n\n\n\n\n\n\n\n\n\n\nCOUNT\n\n\nComputes the number of the elements in the group\n\n\n\n\n\n\nSUM\n\n\nComputes the sum of the elements in the group\n\n\n\n\n\n\nMIN\n\n\nReturns the minimum of the elements in the group\n\n\n\n\n\n\nMAX\n\n\nReturns the maximum of the elements in the group\n\n\n\n\n\n\nAVG\n\n\nComputes the average of the elements in the group\n\n\n\n\n\n\n\n\nThe following attributes are supported for GROUP:\n\n\nAttributes for GROUP:\n\n\n    \nattributes\n: {\n        \noperations\n: [\n            {\n                \ntype\n: \nCOUNT\n,\n                \nnewName\n: \nresultColumnName\n\n            },\n            {\n                \ntype\n: \nSUM\n,\n                \nfield\n: \nfieldName\n,\n                \nnewName\n: \nresultColumnName\n\n            },\n            {\n                \ntype\n: \nMIN\n,\n                \nfield\n: \nfieldName\n,\n                \nnewName\n: \nresultColumnName\n\n            },\n            {\n                \ntype\n: \nMAX\n,\n                \nfield\n: \nfieldName\n,\n                \nnewName\n: \nresultColumnName\n\n            },\n            {\n                \ntype\n: \nAVG\n,\n                \nfield\n: \nfieldName\n,\n                \nnewName\n: \nresultColumnName\n\n            }\n        ]\n    }\n\n\n\n\nYou can perform SUM, MIN, MAX, AVG on non-numeric fields. \nBullet will attempt to cast the field to a number first.\n If it cannot, that record with the field will be ignored for the operation. For the purposes of AVG, Bullet will\nperform the average across the numeric values for a field only.\n\n\nAttributes for COUNT DISTINCT:\n\n\n    \nattributes\n: {\n        \nnewName\n: \nthe name of the resulting count column\n\n    }\n\n\n\n\nNote that the new names you specify in the fields map for aggregations do not apply. You must use the attributes here to give your resulting output count column a name.\n\n\nSee the \nexamples section\n for a detailed description of how to perform these aggregations.\n\n\nResults\n\n\nBullet results are JSON objects with two fields:\n\n\n\n\n\n\n\n\nField\n\n\nContents\n\n\n\n\n\n\n\n\n\n\nrecords\n\n\nThis field contains the list of matching records\n\n\n\n\n\n\nmeta\n\n\nThis field is a map that contains meta information about the query, such as the time the query was received, error data, etc. These are configurable at launch time.\n\n\n\n\n\n\n\n\nFor example queries and results, see \nExamples\n.", 
            "title": "API"
        }, 
        {
            "location": "/ws/api/#api", 
            "text": "See the  UI Usage section  for using the UI to build Bullet queries. This section deals with examples of the JSON query format that the API currently exposes (and the UI uses underneath).", 
            "title": "API"
        }, 
        {
            "location": "/ws/api/#querying", 
            "text": "Bullet queries allow you to filter, project and aggregate data. It lets you fetch raw and aggregated data. Fields inside maps can be accessed using the '.' notation in queries. For example, myMap.key will access the key field inside the myMap map. There is no support for accessing fields inside Lists or inside nested Maps as of yet. Only the entire object can be operated on for now.  The three main sections of a Bullet query are:  {\n     filters : {},\n     projection : {},\n     aggregation : {}.\n     duration : 20000\n}  The duration represents how long the query runs for (a window from when you submit it to that many milliseconds into the future). See the  Filters ,  Projections  and  Aggregation  sections for their respective specifications. Each of those sections are objects.", 
            "title": "Querying"
        }, 
        {
            "location": "/ws/api/#filters", 
            "text": "Bullet supports two kinds of filters:   Logical filters  Relational filters", 
            "title": "Filters"
        }, 
        {
            "location": "/ws/api/#logical-filters", 
            "text": "Logical filters allow you to combine other filter clauses with logical operations like AND, OR and NOT.  The current logical operators allowed in filters are:     Logical Operator  Meaning      AND  All filters must be true. The first false filter evaluated left to right will short-circuit the computation.    OR  Any filter must be true. The first true filter evaluated left to right will short-circuit the computation.    NOT  Negates the value of the first filter clause. The filter is satisfied iff the value is true.     The format for a Logical filter is:  {\n    operation :  AND | OR | NOT \n    clauses : [\n      { operation :  ... , clauses: [{}, ...]},\n      { field :  ... ,  operation :  , values: [ ... ]},\n      { operation :  ... , clauses: [{}, ...]}\n      ...\n   ]\n}  Any other type of filter may be provided as a clause in clauses.", 
            "title": "Logical Filters"
        }, 
        {
            "location": "/ws/api/#relational-filters", 
            "text": "Relational filters allow you to specify conditions on a field, using a comparison operator and a list of values.  The current comparisons allowed in filters are:     Comparison  Meaning      ==  Equal to any value in values    !=  Not equal to any value in values    =  Less than or equal to any value in values    =  Greater than or equal to any value in values     Less than any value in values     Greater than any value in values    RLIKE  Matches using  Java Regex notation , any Regex value in values     These operators are all typed based on the type of the left hand side from the Bullet record. If the elements on the right hand side cannot be\ncasted to the types on the LHS, those items will be ignored for the comparison.  The format for a Relational filter is:  {\n     operation :  == | != |  = |  = |   |   | RLIKE \n     field :  record_field_name | map_field.subfield ,\n     values : [\n         string values ,\n         that go here ,\n         will be casted ,\n         to the ,\n         type of field \n    ]\n}  Multiple top level relational filters behave as if they are ANDed together.", 
            "title": "Relational Filters"
        }, 
        {
            "location": "/ws/api/#projections", 
            "text": "Projections allow you to pull out only the fields needed and rename them (renaming is being supported in order to give\nbetter names to fields pulled out from maps). If projections are not specified, the entire record is returned. If you are querying\nfor raw records, you can use projections to help reduce the load on the system and network.  {\n     projection : {\n         fieldA :  newNameA ,\n         fieldB :  newNameB \n    }\n}", 
            "title": "Projections"
        }, 
        {
            "location": "/ws/api/#aggregations", 
            "text": "Aggregations allow you to perform some operation on the collected records. They take an optional size to restrict\nthe size of the aggregation (this applies for aggregations high cardinality aggregations and raw records).  The current aggregation types that are supported are:     Aggregation  Meaning      GROUP  The resulting output would be a record containing the result of an operation for each unique group in the specified fields    COUNT DISTINCT  Computes the number of distinct elements in the fields. (May be approximate)    LIMIT  The resulting output would be at most the number specified in size.     The current format for an aggregation is ( note see above for what is supported at the moment ):  {\n     type :  GROUP | COUNT DISTINCT | TOP | PERCENTILE | RAW ,\n     size :  a limit on the number of resulting records ,\n     fields : {\n         fields :  newNameA ,\n         that go here :  newNameB ,\n         are what the :  newNameC ,\n         aggregation type applies to :  newNameD \n    },\n     attributes : {\n         these :  change ,\n         per : [\n            aggregation type \n        ]\n    }\n}  You can also use LIMIT as an alias for RAW. DISTINCT is also an alias for GROUP. These exist to make some queries read a bit better.  Currently we support GROUP aggregations on the following operations:     Operation  Meaning      COUNT  Computes the number of the elements in the group    SUM  Computes the sum of the elements in the group    MIN  Returns the minimum of the elements in the group    MAX  Returns the maximum of the elements in the group    AVG  Computes the average of the elements in the group     The following attributes are supported for GROUP:  Attributes for GROUP:       attributes : {\n         operations : [\n            {\n                 type :  COUNT ,\n                 newName :  resultColumnName \n            },\n            {\n                 type :  SUM ,\n                 field :  fieldName ,\n                 newName :  resultColumnName \n            },\n            {\n                 type :  MIN ,\n                 field :  fieldName ,\n                 newName :  resultColumnName \n            },\n            {\n                 type :  MAX ,\n                 field :  fieldName ,\n                 newName :  resultColumnName \n            },\n            {\n                 type :  AVG ,\n                 field :  fieldName ,\n                 newName :  resultColumnName \n            }\n        ]\n    }  You can perform SUM, MIN, MAX, AVG on non-numeric fields.  Bullet will attempt to cast the field to a number first.  If it cannot, that record with the field will be ignored for the operation. For the purposes of AVG, Bullet will\nperform the average across the numeric values for a field only.  Attributes for COUNT DISTINCT:       attributes : {\n         newName :  the name of the resulting count column \n    }  Note that the new names you specify in the fields map for aggregations do not apply. You must use the attributes here to give your resulting output count column a name.  See the  examples section  for a detailed description of how to perform these aggregations.", 
            "title": "Aggregations"
        }, 
        {
            "location": "/ws/api/#results", 
            "text": "Bullet results are JSON objects with two fields:     Field  Contents      records  This field contains the list of matching records    meta  This field is a map that contains meta information about the query, such as the time the query was received, error data, etc. These are configurable at launch time.     For example queries and results, see  Examples .", 
            "title": "Results"
        }, 
        {
            "location": "/ws/examples/", 
            "text": "Examples\n\n\nRather than sourcing the examples from the \nQuick Start\n, these examples are real-world ones sourced from Bullet running on raw, user events generated by instrumentation on Yahoo sites.\n\n\n\n\nReal data!?\n\n\nThe actual data shown here has been edited and is not how actual Yahoo user events look.\n\n\n\n\n\n\nSQL translation\n\n\nFor each query, we will also rewrite it to a SQL like syntax for readability. Eventually, we plan the API to support this format as input so you could just write this in place of the JSON syntax.\n\n\n\n\nSimplest Query\n\n\nThe simplest query you can write would be:\n\n\nBullet Query\n\n\n{}\n\n\n\n\nWhile not a very useful query - this will get any one event record (no filters =\n any record would be matched, no projection =\n gets the entire record, default aggregation =\n \nLIMIT\nor \nRAW\n, with size 1, default duration =\n 30000 ms), this can be used to quickly test your connection to Bullet.\n\n\nSQL\n\n\nSELECT * FROM WINDOW(30000) LIMIT 1;\n\n\n\n\nSince there is only one data stream in Bullet, the \nFROM\n clause is replaced with a \nWINDOW\n function that sets the time window for the Bullet query.\n\n\nSimple Filtering\n\n\nBullet Query\n\n\n{\n   \nfilters\n:[\n       {\n           \nfield\n:\nid\n,\n           \noperation\n:\n==\n,\n           \nvalues\n:[\n               \nbtsg8l9b234ha\n\n           ]\n       }\n    ]\n}\n\n\n\n\nSQL\n\n\nSELECT * FROM WINDOW(30s) WHERE id = \nbtsg8l9b234ha\n LIMIT 1;\n\n\n\n\nBecause of the default constraints, this query would find at most 1 record with the id matching the value provided. The record would have all its fields.\n\n\nA sample response could be (it has been edited to remove PII and other Yahoo data). The response contains a single matching record, and the associated meta information.\n\n\n{\n   \nrecords\n:[\n       {\n           \nserver_name\n:\nEDITED\n,\n           \npage_uri\n:\n/\n,\n           \nis_page_view\n:true,\n           \ndevice\n:\ntablet\n,\n           \ndebug_codes\n:{\n               \nhttp_status_code\n:\n200\n\n           },\n           \nreferrer_domain\n:\nwww.yahoo.com\n,\n           \nis_logged_in\n:true,\n           \ntimestamp\n:1446842189000,\n           \nevent_family\n:\nview\n,\n           \nid\n:\nbtsg8l9b234ha\n,\n           \nos_name\n:\nmac os\n,\n           \ndemographics\n:{\n               \nage\n : \n25\n,\n               \ngender\n : \nm\n,\n            }\n       }\n    ],\n    \nmeta\n:{\n        \nrule_id\n:1167304238598842449,\n        \nrule_body\n:\n{}\n,\n        \nrule_finish_time\n:1480723799550,\n        \nrule_receive_time\n:1480723799540\n    }\n}\n\n\n\n\nRelational Filters and Projections\n\n\nBullet Query\n\n\n{\n    \nfilters\n:[\n        {\n            \nfield\n:\nid\n,\n            \noperation\n:\n==\n,\n            \nvalues\n:[\n                \nbtsg8l9b234ha\n\n            ]\n        },\n        {\n            \nfield\n:\npage_id\n,\n            \noperation\n:\n!=\n,\n            \nvalues\n:[\n                \nnull\n\n            ]\n        }\n    ],\n    \nprojection\n:{\n        \nfields\n:{\n            \ntimestamp\n:\nts\n,\n            \ndevice_timestamp\n:\ndevice_ts\n,\n            \nevent\n:\nevent\n,\n            \npage_domain\n:\ndomain\n,\n            \nid\n:\nid\n\n        }\n    },\n    \naggregation\n:{\n        \ntype\n:\nRAW\n,\n        \nsize\n:10\n    },\n    \nduration\n:20000\n}\n\n\n\n\nSQL\n\n\nSELECT timestamp AS ts, device_timestamp AS device_ts, event AS event, page_domain AS domain\nFROM WINDOW(20s)\nWHERE id = \nbtsg8l9b234ha\n AND page_id IS NOT NULL\nLIMIT 10;\n\n\n\n\nThe above query finds all events with id set to 'btsg8l9b234ha' and page_id is not null, projects out the fields listed above with their new names (timestamp becomes ts etc) and limits the results to at most 10 such records. \nRAW\n indicates that the complete raw record fields will be returned, and more complicated aggregations such as \nCOUNT\n or \nSUM\n will not be performed. The duration would set the query to wait at most 20 seconds for records to show up.\n\n\nThe resulting response could look like (only 3 events were generated that matched the criteria):\n\n\n{\n    \nrecords\n: [\n        {\n            \ndomain\n: \nhttp://some.url.com\n,\n            \ndevice_ts\n: 1481152233788,\n            \nid\n: 2273844742998,\n            \nevent\n: \npage\n,\n            \nts\n: null\n        },\n        {\n            \ndomain\n: \nwww.yahoo.com\n,\n            \ndevice_ts\n: 1481152233788,\n            \nid\n: 227384472956,\n            \nevent\n: \nclick\n,\n            \nts\n: 1481152233888\n        },\n        {\n            \ndomain\n: \nhttps://news.yahoo.com\n,\n            \ndevice_ts\n: null,\n            \nid\n: 2273844742556,\n            \nevent\n: \npage\n,\n            \nts\n: null\n        }\n    ],\n    \nmeta\n: {\n        \nrule_id\n: -3239746252817510000,\n        \nrule_body\n: \nentire rule body is re-emitted here\n,\n        \nrule_finish_time\n: 1481152233799,\n        \nrule_receive_time\n: 1481152233796\n    }\n}\n\n\n\n\nLogical Filters and Projections\n\n\nBullet Query\n\n\n{\n \nfilters\n: [\n                {\n                    \noperation\n: \nOR\n,\n                    \nclauses\n: [\n                        {\n                            \noperation\n: \nAND\n,\n                            \nclauses\n: [\n                                {\n                                    \nfield\n: \nid\n,\n                                    \noperation\n: \n==\n,\n                                    \nvalues\n: [\nc14plm1begla7\n]\n                                },\n                                {\n                                    \noperation\n: \nOR\n,\n                                    \nclauses\n: [\n                                        {\n                                            \noperation\n: \nAND\n,\n                                            \nclauses\n: [\n                                                {\n                                                    \nfield\n: \nexperience\n,\n                                                    \noperation\n: \n==\n,\n                                                    \nvalues\n: [\nweb\n]\n                                                },\n                                                {\n                                                    \nfield\n: \npage_id\n,\n                                                    \noperation\n: \n==\n,\n                                                    \nvalues\n: [\n18025\n, \n47729\n]\n                                                }\n                                            ]\n                                        },\n                                        {\n                                            \nfield\n: \nlink_id\n,\n                                            \noperation\n: \nRLIKE\n,\n                                            \nvalues\n: [\n2.*\n]\n                                        }\n                                    ]\n                                }\n                            ]\n                        },\n                        {\n                            \noperation\n: \nAND\n,\n                            \nclauses\n: [\n                                {\n                                    \nfield\n: \ntags.player\n,\n                                    \noperation\n: \n==\n,\n                                    \nvalues\n: [\ntrue\n]\n                                },\n                                {\n                                    \nfield\n: \ndemographics.age\n,\n                                    \noperation\n: \n,\n                                    \nvalues\n: [\n65\n]\n                                }\n                            ]\n                        }\n                    ]\n                }\n            ],\n \nprojection\n : {\n    \nfields\n: {\n        \nid\n: \nid\n,\n        \nexperience\n: \nexperience\n,\n        \npage_id\n: \npid\n,\n        \nlink_id\n: \nlid\n,\n        \ntags\n: \ntags\n,\n        \ndemographics.age\n: \nage\n\n    }\n },\n \naggregation\n: {\ntype\n : \nRAW\n, \nsize\n : 1},\n \nduration\n: 60000\n}\n\n\n\n\nSQL\n\n\nSELECT id AS id, experience AS experience, page_id AS pid,\n       link_id AS lid, tags AS tags, demographics[\nage\n] AS age\nFROM WINDOW(1min)\nWHERE (id = \nc14plm1begla7\n AND ((experience = \nweb\n AND page_id IN [\n18025\n, \n47729\n])\n                                 OR link_id MATCHES \n2.*\n))\n      OR\n      (tags[\nplayer\n] AND demographics[\nage\n] \n \n65\n)\nLIMIT 1;\n\n\n\n\n\n\nTyping\n\n\nIf demographics[\"age\"] was of type Long, then Bullet will convert 85 to be an Long, but in this example, we are pretending that it is String.  So, no conversion is made. Similarly for link_id, id, experience and page_id. tags is a Map of String to Boolean so Bullet converts \n\"true\"\n to the Boolean \ntrue\n.\n\n\n\n\nThis query is looking for a single event with a specific id and either the page_id is in two specific pages on the \"web\" experience or with a link_id that starts with 2, or a player event where the age is greater than \"65\". In other words, it is looking for senior citizens who generate video player events or a particular person's (based on id) events on two specific pages or a group of pages that have link that have ids that start with 2. It then projects out only these fields with different names.\n\n\nA sample result could look like (it matched because of tags.player was true and demographics.age was \n 65):\n\n\n{\n    \nrecords\n: [\n        {\n            \npid\n:\n158\n,\n            \nid\n:\n0qcgofdbfqs9s\n,\n            \nexperience\n:\nweb\n,\n            \nlid\n:\n978500434\n,\n            \nage\n:\n66\n,\n            \ntags\n:{\nplayer\n:true}\n        }\n    ],\n    \nmeta\n: {\n        \nrule_id\n: 3239746252812284004,\n        \nrule_body\n: \nRULE_BODY_EDITED_OUT\n,\n        \nrule_finish_time\n: 1481152233805,\n        \nrule_receive_time\n: 1481152233881\n    }\n}\n\n\n\n\nGROUP ALL COUNT Aggregation\n\n\nAn example of a query performing a COUNT all records aggregation would look like:\n\n\nBullet Query\n\n\n{\n   \nfilters\n:[\n      {\n         \nfield\n: \ndemographics.age\n,\n         \noperation\n: \n,\n         \nvalues\n: [\n65\n]\n      }\n   ],\n   \naggregation\n:{\n      \ntype\n: \nGROUP\n,\n      \nattributes\n: {\n         \noperations\n: [\n            {\n               \ntype\n: \nCOUNT\n,\n               \nnewName\n: \nnumSeniors\n\n            }\n         ]\n      }\n   },\n   \nduration\n: 20000\n}\n\n\n\n\nSQL\n\n\nSELECT COUNT(*) AS numSeniors\nFROM WINDOW(20s)\nWHERE demographics[\nage\n] \n \n65\n;\n\n\n\n\nThis query will count the number events for which demographics.age \n 65. The aggregation type GROUP indicates that it is a group aggregation. To group by a key, the \nfields\n key needs to be set in the \naggregation\n part of the query. If \nfields\n is empty or is omitted (as it is in the query above) and the \ntype\n is \nGROUP\n, it is as if all the records are collapsed into a single group - a \nGROUP ALL\n. Adding a \nCOUNT\n in the \noperations\n part of the \nattributes\n indicates that the number of records in this group will be counted, and the \"newName\" key denotes the name the resulting column \"numSeniors\" in the result. Setting the duration to 20000 counts matching records for\nthis duration.\n\n\nA sample result would look like:\n\n\n{\n    \nrecords\n: [\n        {\n            \nnumSeniors\n: 363201\n        }\n    ],\n    \nmeta\n: {}\n}\n\n\n\n\nThis result indicates that 363,201 records were counted with demographics.age \n 65 during the 20 s the query was running.\n\n\nGROUP ALL Multiple Aggregations\n\n\nCOUNT is the only GROUP operation for which you can omit a \"field\".\n\n\nBullet Query\n\n\n{\n   \nfilters\n:[\n      {\n         \nfield\n: \ndemographics.state\n,\n         \noperation\n: \n==\n,\n         \nvalues\n: [\ncalifornia\n]\n      }\n   ],\n   \naggregation\n:{\n      \ntype\n: \nGROUP\n,\n      \nattributes\n: {\n         \noperations\n: [\n            {\n               \ntype\n: \nCOUNT\n,\n               \nnewName\n: \nnumCalifornians\n\n            },\n            {\n               \ntype\n: \nAVG\n,\n               \nfield\n: \ndemographics.age\n,\n               \nnewName\n: \navgAge\n\n            },\n            {\n               \ntype\n: \nMIN\n,\n               \nfield\n: \ndemographics.age\n,\n               \nnewName\n: \nminAge\n\n            },\n            {\n               \ntype\n: \nMAX\n,\n               \nfield\n: \ndemographics.age\n,\n               \nnewName\n: \nmaxAge\n\n            }\n         ]\n      }\n   },\n   \nduration\n: 20000\n}\n\n\n\n\nSQL\n\n\nSELECT COUNT(*) AS numCalifornians, AVG(demographics[\nage\n]) AS avgAge,\n       MIN(demographics[\nage\n]) AS minAge, MAX(demographics[\nage\n]) AS maxAge,\nFROM WINDOW(20s)\nWHERE demographics[\nstate\n] = \ncalifornia\n;\n\n\n\n\nA sample result would look like:\n\n\n{\n    \nrecords\n: [\n        {\n            \nmaxAge\n: 94.0,\n            \nnumCalifornians\n: 188451,\n            \nminAge\n: 6.0,\n            \navgAge\n: 33.71828\n        }\n    ],\n    \nmeta\n: {\n        \nrule_id\n: 8051040987827161000,\n        \nrule_body\n: \nRULE BODY HERE\n,\n        \nrule_finish_time\n: 1482371927435,\n        \nrule_receive_time\n: 1482371916625\n    }\n}\n\n\n\n\nThis result indicates that, among the records observed during the 20 s this query ran, there were 188,451 users with demographics.state equal to \"california\". Among these users the average age was 33.71828, the max age observed was 94, and the minimum age observed was 6.\n\n\nExact COUNT DISTINCT Aggregation\n\n\nBullet Query\n\n\n{\n  \naggregation\n: {\n      \ntype\n: \nCOUNT DISTINCT\n,\n      \nfields\n: {\n          \nbrowser_name\n: \n,\n          \nbrowser_version\n: \n\n      }\n    }\n}\n\n\n\n\nSQL\n\n\nSELECT COUNT(*) AS \nCOUNT DISTINCT\n\nFROM (SELECT browser_name, browser_version\n      FROM WINDOW(30s)\n      GROUP BY browser_name, browser_version) tmp;\n\n\n\n\nThis gets the count of the unique browser names and versions in the next 30 s (default duration). Note that we do not specify values for the keys in fields. This is because they are not relevant\n\n\n{\n    \nrecords\n: [\n        {\n            \nCOUNT DISTINCT\n: 158.0\n        }\n    ],\n    \nmeta\n: {\n        \nrule_id\n: 4451146261377394443,\n        \naggregation\n: {\n            \nstandardDeviations\n: {\n                \n1\n: {\n                    \nupperBound\n: 158.0,\n                    \nlowerBound\n: 158.0\n                },\n                \n2\n: {\n                    \nupperBound\n: 158.0,\n                    \nlowerBound\n: 158.0\n                },\n                \n3\n: {\n                    \nupperBound\n: 158.0,\n                    \nlowerBound\n: 158.0\n                }\n            },\n            \nwasEstimated\n: false,\n            \nsketchFamily\n: \nCOMPACT\n,\n            \nsketchTheta\n: 1.0,\n            \nsketchSize\n: 1280\n        },\n        \nrule_body\n: \nRULE BODY HERE\n,\n        \nrule_finish_time\n: 1484084869073,\n        \nrule_receive_time\n: 1484084832684\n    }\n}\n\n\n\n\nThere were 158 unique combinations on browser names and versions in our dataset for those 30 s. Note the new \naggregation\n object in the meta. It has various metadata about the result and Sketches. In particular, the \nwasEstimated\n key denotes where the result\nwas estimated or not. The \nstandardDeviations\n key denotes the confidence at various sigmas: 1 (1 sigma = ~68% confidence, 2 sigma = ~95% confidence, 3 sigma = ~99% confidence). Since this result was not estimated, the result is the same as the upper and lower bounds for the result.\n\n\nApproximate COUNT DISTINCT Aggregation\n\n\nBullet Query\n\n\n{\n  \naggregation\n: {\n      \ntype\n: \nCOUNT DISTINCT\n,\n      \nfields\n: {\n          \nip_address\n: \n\n      },\n      \nattributes\n: {\n          \nnewName\n: \nuniqueIPs\n\n      }\n    },\n    \nduration\n: 10000\n}\n\n\n\n\nSQL\n\n\nSELECT COUNT(*) AS uniqueIPs\nFROM (SELECT ip_address\n      FROM WINDOW(10s)\n      GROUP BY ip_address) tmp;\n\n\n\n\nThis query gets us the unique IP addresses in the next 10 s. It renames the result column from \"COUNT DISTINCT\" to \"uniqueIPs\".\n\n\n{\n    \nrecords\n: [\n        {\n            \nuniqueIPs\n: 130551.07952805843\n        }\n    ],\n    \nmeta\n: {\n        \nrule_id\n: 5377782455857451480,\n        \naggregation\n: {\n            \nstandardDeviations\n: {\n                \n1\n: {\n                    \nupperBound\n: 131512.85413760383,\n                    \nlowerBound\n: 129596.30223107953\n                },\n                \n2\n: {\n                    \nupperBound\n: 132477.15103015225,\n                    \nlowerBound\n: 128652.93906100772\n                },\n                \n3\n: {\n                    \nupperBound\n: 133448.49248615955,\n                    \nlowerBound\n: 127716.46773622213\n                }\n            },\n            \nwasEstimated\n: true,\n            \nsketchFamily\n: \nCOMPACT\n,\n            \nsketchTheta\n: 0.12549877074343688,\n            \nsketchSize\n: 131096\n        },\n        \nrule_body\n: \nRULE BODY HERE\n,\n        \nrule_finish_time\n: 1484090240812,\n        \nrule_receive_time\n: 1484090223351\n    }\n}\n\n\n\n\nThe number of unique IPs in our dataset was 130551 in those 10 s (approximately) with the true value between (129596, 131512) at 68% confidence, (128652, 132477) at 95% confidence and (127716, 133448) at 99% confidence. In the \nworst\n case at 3 sigma (99% confidence),\nour error is 2.17%. The final result was computed with 131096 bytes or ~128 KiB as denoted by \nsketchSize\n. This happens to be maximum size the the COUNT DISTINCT sketch will take up at the default nominal entries, so even if we had billions of unique IPs, the size will be the same and the error may be higher (depends on the distribution). For example, the error when the same query was run for 30 s was 2.28% at 99% confidence (actual unique IPs: 559428, upper bound: 572514). In fact, the worst the error can get at this\nSketch size is 2.34% as defined \nhere\n, \nregardless of the number of unique entries added to the Sketch!\n.\n\n\nDISTINCT Aggregation\n\n\nBullet Query\n\n\n{\n   \naggregation\n:{\n      \ntype\n: \nDISTINCT\n,\n      \nsize\n: 10,\n      \nfields\n: {\n         \nbrowser_name\n: \nbrowser\n\n      }\n   }\n}\n\n\n\n\nSQL\n\n\nSELECT browser_name AS browser\nFROM WINDOW(30s)\nGROUP BY browser_name\nLIMIT 10;\n\n\n\n\nThis query gets the distinct values for the browser_name field and limit the results to 10. It runs for 30 s.\n\n\n{\n  \nrecords\n:[\n    {\n      \nbrowser\n:\nopera\n\n    },\n    {\n      \nbrowser\n:\nflock\n\n    },\n    {\n      \nbrowser\n:\nlinks\n\n    },\n    {\n      \nbrowser\n:\nmozilla firefox\n\n    },\n    {\n      \nbrowser\n:\ndolfin\n\n    },\n    {\n      \nbrowser\n:\nlynx\n\n    },\n    {\n      \nbrowser\n:\nchrome\n\n    },\n    {\n      \nbrowser\n:\nmicrosoft internet explorer\n\n    },\n    {\n      \nbrowser\n:\naol browser\n\n    },\n    {\n      \nbrowser\n:\nedge\n\n    }\n  ],\n  \nmeta\n:{\n    \nrule_id\n:-4872093887360741287,\n    \naggregation\n:{\n      \nstandardDeviations\n:{\n        \n1\n:{\n          \nupperBound\n:28.0,\n          \nlowerBound\n:28.0\n        },\n        \n2\n:{\n          \nupperBound\n:28.0,\n          \nlowerBound\n:28.0\n        },\n        \n3\n:{\n          \nupperBound\n:28.0,\n          \nlowerBound\n:28.0\n        }\n      },\n      \nwasEstimated\n:false,\n      \nuniquesEstimate\n:28.0,\n      \nsketchTheta\n:1.0\n    },\n    \nrule_body\n:\nRULE_BODY_HERE\n,\n    \nrule_finish_time\n:1485469087971,\n    \nrule_receive_time\n:1485469054070\n  }\n}\n\n\n\n\nThere were 28 unique results but we asked for 10, so the query returned a uniform sample across the 28 distinct values.\n\n\nDISTINCT is just an alias for GROUP. A GROUP by with no operations is exactly a DISTINCT.\n\n\nGROUP by Aggregation\n\n\nBullet Query\n\n\n{\n   \nfilters\n:[\n      {\n         \nfield\n: \ndemographics\n,\n         \noperation\n: \n!=\n,\n         \nvalues\n: [\nnull\n]\n      }\n   ],\n   \naggregation\n:{\n      \ntype\n: \nGROUP\n,\n      \nsize\n: 50,\n      \nfields\n: {\n          \ndemographics.country\n: \ncountry\n,\n          \ndevice\n: \n\n      },\n      \nattributes\n: {\n         \noperations\n: [\n            {\n               \ntype\n: \nCOUNT\n,\n               \nnewName\n: \ncount\n\n            },\n            {\n               \ntype\n: \nAVG\n,\n               \nfield\n: \ndemographics.age\n,\n               \nnewName\n: \naverageAge\n\n            },\n            {\n               \ntype\n: \nAVG\n,\n               \nfield\n: \ntimespent\n,\n               \nnewName\n: \naverageTimespent\n\n            }\n         ]\n      }\n   },\n   \nduration\n: 20000\n}\n\n\n\n\nSQL\n\n\nSELECT demographics[\ncountry\n] AS country, device AS device,\n       COUNT(*) AS count, AVG(demographics[\nage\n]) AS averageAge,\n       AVG(timespent) AS averageTimespent\nFROM WINDOW(20s)\nWHERE demographics IS NOT NULL\nGROUP BY demographics[\ncountry\n], device\nLIMIT 50;\n\n\n\n\nThis query groups by the country and the device and for each unique group gets the count, average age and time spent by the users for the next 20 seconds. It renames demographics.country to country and does not rename device. It limits the groups to 50. If there were more than\n50 groups, the results would be a uniform sampling of the groups (but each group in the result would have the correct result). These parameters can all be tweaked \nin the configuration\n.\n\n\n{\n  \nrecords\n:[\n    {\n      \ncountry\n:\nuk\n,\n      \ndevice\n:\ndesktop\n,\n      \ncount\n:203034,\n      \naverageAge\n:32.42523,\n      \naverageTimespent\n:1.342\n    },\n    {\n      \ncountry\n:\nus\n,\n      \ndevice\n:\ndesktop\n,\n      \ncount\n:1934030,\n      \naverageAge\n:29.42523,\n      \naverageTimespent\n:3.234520\n    },\n    \n...and 41 other such records here\n\n  ],\n  \nmeta\n:{\n    \nrule_id\n:1705911449584057747,\n    \naggregation\n:{\n      \nstandardDeviations\n:{\n        \n1\n:{\n          \nupperBound\n:43.0,\n          \nlowerBound\n:43.0\n        },\n        \n2\n:{\n          \nupperBound\n:43.0,\n          \nlowerBound\n:43.0\n        },\n        \n3\n:{\n          \nupperBound\n:43.0,\n          \nlowerBound\n:43.0\n        }\n      },\n      \nwasEstimated\n:false,\n      \nuniquesEstimate\n:43.0,\n      \nsketchTheta\n:1.0\n    },\n    \nrule_body\n:\nRULE_BODY_HERE\n,\n    \nrule_finish_time\n:1485217172780,\n    \nrule_receive_time\n:1485217148840\n  }\n}\n\n\n\n\nWe received 43 rows for this result. The maximum groups that was allowed for the instance of Bullet was 512. If there were more groups than the maximum specified by your configuration, \na uniform sample\n across them would be chosen\nfor the result. However, for each group, the values computed (average, count) would be exact. The standard deviations, whether the result was estimated and the number of approximate uniques in the metadata would reflect the change.\n\n\nIf you asked for 50 rows in the aggregation (as the query did above) but there were more than 50 in the result (but \n 512), the metadata would reflect the fact that the result was not estimated. You would still get a uniform sample\nbut by increasing your aggregation size higher, you could get the rest.\n\n\nFor readability, if you were just trying to get the unique values for a field or a set of fields, you could leave out the attributes section and specify your fields section. You could also call the type \nDISTINCT\n instead of\n\nGROUP\n to make that explicit. \nDISTINCT\n is just an alias for \nGROUP\n. See \nthe DISTINCT example\n.", 
            "title": "Examples"
        }, 
        {
            "location": "/ws/examples/#examples", 
            "text": "Rather than sourcing the examples from the  Quick Start , these examples are real-world ones sourced from Bullet running on raw, user events generated by instrumentation on Yahoo sites.   Real data!?  The actual data shown here has been edited and is not how actual Yahoo user events look.    SQL translation  For each query, we will also rewrite it to a SQL like syntax for readability. Eventually, we plan the API to support this format as input so you could just write this in place of the JSON syntax.", 
            "title": "Examples"
        }, 
        {
            "location": "/ws/examples/#simplest-query", 
            "text": "The simplest query you can write would be:  Bullet Query  {}  While not a very useful query - this will get any one event record (no filters =  any record would be matched, no projection =  gets the entire record, default aggregation =   LIMIT or  RAW , with size 1, default duration =  30000 ms), this can be used to quickly test your connection to Bullet.  SQL  SELECT * FROM WINDOW(30000) LIMIT 1;  Since there is only one data stream in Bullet, the  FROM  clause is replaced with a  WINDOW  function that sets the time window for the Bullet query.", 
            "title": "Simplest Query"
        }, 
        {
            "location": "/ws/examples/#simple-filtering", 
            "text": "Bullet Query  {\n    filters :[\n       {\n            field : id ,\n            operation : == ,\n            values :[\n                btsg8l9b234ha \n           ]\n       }\n    ]\n}  SQL  SELECT * FROM WINDOW(30s) WHERE id =  btsg8l9b234ha  LIMIT 1;  Because of the default constraints, this query would find at most 1 record with the id matching the value provided. The record would have all its fields.  A sample response could be (it has been edited to remove PII and other Yahoo data). The response contains a single matching record, and the associated meta information.  {\n    records :[\n       {\n            server_name : EDITED ,\n            page_uri : / ,\n            is_page_view :true,\n            device : tablet ,\n            debug_codes :{\n                http_status_code : 200 \n           },\n            referrer_domain : www.yahoo.com ,\n            is_logged_in :true,\n            timestamp :1446842189000,\n            event_family : view ,\n            id : btsg8l9b234ha ,\n            os_name : mac os ,\n            demographics :{\n                age  :  25 ,\n                gender  :  m ,\n            }\n       }\n    ],\n     meta :{\n         rule_id :1167304238598842449,\n         rule_body : {} ,\n         rule_finish_time :1480723799550,\n         rule_receive_time :1480723799540\n    }\n}", 
            "title": "Simple Filtering"
        }, 
        {
            "location": "/ws/examples/#relational-filters-and-projections", 
            "text": "Bullet Query  {\n     filters :[\n        {\n             field : id ,\n             operation : == ,\n             values :[\n                 btsg8l9b234ha \n            ]\n        },\n        {\n             field : page_id ,\n             operation : != ,\n             values :[\n                 null \n            ]\n        }\n    ],\n     projection :{\n         fields :{\n             timestamp : ts ,\n             device_timestamp : device_ts ,\n             event : event ,\n             page_domain : domain ,\n             id : id \n        }\n    },\n     aggregation :{\n         type : RAW ,\n         size :10\n    },\n     duration :20000\n}  SQL  SELECT timestamp AS ts, device_timestamp AS device_ts, event AS event, page_domain AS domain\nFROM WINDOW(20s)\nWHERE id =  btsg8l9b234ha  AND page_id IS NOT NULL\nLIMIT 10;  The above query finds all events with id set to 'btsg8l9b234ha' and page_id is not null, projects out the fields listed above with their new names (timestamp becomes ts etc) and limits the results to at most 10 such records.  RAW  indicates that the complete raw record fields will be returned, and more complicated aggregations such as  COUNT  or  SUM  will not be performed. The duration would set the query to wait at most 20 seconds for records to show up.  The resulting response could look like (only 3 events were generated that matched the criteria):  {\n     records : [\n        {\n             domain :  http://some.url.com ,\n             device_ts : 1481152233788,\n             id : 2273844742998,\n             event :  page ,\n             ts : null\n        },\n        {\n             domain :  www.yahoo.com ,\n             device_ts : 1481152233788,\n             id : 227384472956,\n             event :  click ,\n             ts : 1481152233888\n        },\n        {\n             domain :  https://news.yahoo.com ,\n             device_ts : null,\n             id : 2273844742556,\n             event :  page ,\n             ts : null\n        }\n    ],\n     meta : {\n         rule_id : -3239746252817510000,\n         rule_body :  entire rule body is re-emitted here ,\n         rule_finish_time : 1481152233799,\n         rule_receive_time : 1481152233796\n    }\n}", 
            "title": "Relational Filters and Projections"
        }, 
        {
            "location": "/ws/examples/#logical-filters-and-projections", 
            "text": "Bullet Query  {\n  filters : [\n                {\n                     operation :  OR ,\n                     clauses : [\n                        {\n                             operation :  AND ,\n                             clauses : [\n                                {\n                                     field :  id ,\n                                     operation :  == ,\n                                     values : [ c14plm1begla7 ]\n                                },\n                                {\n                                     operation :  OR ,\n                                     clauses : [\n                                        {\n                                             operation :  AND ,\n                                             clauses : [\n                                                {\n                                                     field :  experience ,\n                                                     operation :  == ,\n                                                     values : [ web ]\n                                                },\n                                                {\n                                                     field :  page_id ,\n                                                     operation :  == ,\n                                                     values : [ 18025 ,  47729 ]\n                                                }\n                                            ]\n                                        },\n                                        {\n                                             field :  link_id ,\n                                             operation :  RLIKE ,\n                                             values : [ 2.* ]\n                                        }\n                                    ]\n                                }\n                            ]\n                        },\n                        {\n                             operation :  AND ,\n                             clauses : [\n                                {\n                                     field :  tags.player ,\n                                     operation :  == ,\n                                     values : [ true ]\n                                },\n                                {\n                                     field :  demographics.age ,\n                                     operation :  ,\n                                     values : [ 65 ]\n                                }\n                            ]\n                        }\n                    ]\n                }\n            ],\n  projection  : {\n     fields : {\n         id :  id ,\n         experience :  experience ,\n         page_id :  pid ,\n         link_id :  lid ,\n         tags :  tags ,\n         demographics.age :  age \n    }\n },\n  aggregation : { type  :  RAW ,  size  : 1},\n  duration : 60000\n}  SQL  SELECT id AS id, experience AS experience, page_id AS pid,\n       link_id AS lid, tags AS tags, demographics[ age ] AS age\nFROM WINDOW(1min)\nWHERE (id =  c14plm1begla7  AND ((experience =  web  AND page_id IN [ 18025 ,  47729 ])\n                                 OR link_id MATCHES  2.* ))\n      OR\n      (tags[ player ] AND demographics[ age ]    65 )\nLIMIT 1;   Typing  If demographics[\"age\"] was of type Long, then Bullet will convert 85 to be an Long, but in this example, we are pretending that it is String.  So, no conversion is made. Similarly for link_id, id, experience and page_id. tags is a Map of String to Boolean so Bullet converts  \"true\"  to the Boolean  true .   This query is looking for a single event with a specific id and either the page_id is in two specific pages on the \"web\" experience or with a link_id that starts with 2, or a player event where the age is greater than \"65\". In other words, it is looking for senior citizens who generate video player events or a particular person's (based on id) events on two specific pages or a group of pages that have link that have ids that start with 2. It then projects out only these fields with different names.  A sample result could look like (it matched because of tags.player was true and demographics.age was   65):  {\n     records : [\n        {\n             pid : 158 ,\n             id : 0qcgofdbfqs9s ,\n             experience : web ,\n             lid : 978500434 ,\n             age : 66 ,\n             tags :{ player :true}\n        }\n    ],\n     meta : {\n         rule_id : 3239746252812284004,\n         rule_body :  RULE_BODY_EDITED_OUT ,\n         rule_finish_time : 1481152233805,\n         rule_receive_time : 1481152233881\n    }\n}", 
            "title": "Logical Filters and Projections"
        }, 
        {
            "location": "/ws/examples/#group-all-count-aggregation", 
            "text": "An example of a query performing a COUNT all records aggregation would look like:  Bullet Query  {\n    filters :[\n      {\n          field :  demographics.age ,\n          operation :  ,\n          values : [ 65 ]\n      }\n   ],\n    aggregation :{\n       type :  GROUP ,\n       attributes : {\n          operations : [\n            {\n                type :  COUNT ,\n                newName :  numSeniors \n            }\n         ]\n      }\n   },\n    duration : 20000\n}  SQL  SELECT COUNT(*) AS numSeniors\nFROM WINDOW(20s)\nWHERE demographics[ age ]    65 ;  This query will count the number events for which demographics.age   65. The aggregation type GROUP indicates that it is a group aggregation. To group by a key, the  fields  key needs to be set in the  aggregation  part of the query. If  fields  is empty or is omitted (as it is in the query above) and the  type  is  GROUP , it is as if all the records are collapsed into a single group - a  GROUP ALL . Adding a  COUNT  in the  operations  part of the  attributes  indicates that the number of records in this group will be counted, and the \"newName\" key denotes the name the resulting column \"numSeniors\" in the result. Setting the duration to 20000 counts matching records for\nthis duration.  A sample result would look like:  {\n     records : [\n        {\n             numSeniors : 363201\n        }\n    ],\n     meta : {}\n}  This result indicates that 363,201 records were counted with demographics.age   65 during the 20 s the query was running.", 
            "title": "GROUP ALL COUNT Aggregation"
        }, 
        {
            "location": "/ws/examples/#group-all-multiple-aggregations", 
            "text": "COUNT is the only GROUP operation for which you can omit a \"field\".  Bullet Query  {\n    filters :[\n      {\n          field :  demographics.state ,\n          operation :  == ,\n          values : [ california ]\n      }\n   ],\n    aggregation :{\n       type :  GROUP ,\n       attributes : {\n          operations : [\n            {\n                type :  COUNT ,\n                newName :  numCalifornians \n            },\n            {\n                type :  AVG ,\n                field :  demographics.age ,\n                newName :  avgAge \n            },\n            {\n                type :  MIN ,\n                field :  demographics.age ,\n                newName :  minAge \n            },\n            {\n                type :  MAX ,\n                field :  demographics.age ,\n                newName :  maxAge \n            }\n         ]\n      }\n   },\n    duration : 20000\n}  SQL  SELECT COUNT(*) AS numCalifornians, AVG(demographics[ age ]) AS avgAge,\n       MIN(demographics[ age ]) AS minAge, MAX(demographics[ age ]) AS maxAge,\nFROM WINDOW(20s)\nWHERE demographics[ state ] =  california ;  A sample result would look like:  {\n     records : [\n        {\n             maxAge : 94.0,\n             numCalifornians : 188451,\n             minAge : 6.0,\n             avgAge : 33.71828\n        }\n    ],\n     meta : {\n         rule_id : 8051040987827161000,\n         rule_body :  RULE BODY HERE ,\n         rule_finish_time : 1482371927435,\n         rule_receive_time : 1482371916625\n    }\n}  This result indicates that, among the records observed during the 20 s this query ran, there were 188,451 users with demographics.state equal to \"california\". Among these users the average age was 33.71828, the max age observed was 94, and the minimum age observed was 6.", 
            "title": "GROUP ALL Multiple Aggregations"
        }, 
        {
            "location": "/ws/examples/#exact-count-distinct-aggregation", 
            "text": "Bullet Query  {\n   aggregation : {\n       type :  COUNT DISTINCT ,\n       fields : {\n           browser_name :  ,\n           browser_version :  \n      }\n    }\n}  SQL  SELECT COUNT(*) AS  COUNT DISTINCT \nFROM (SELECT browser_name, browser_version\n      FROM WINDOW(30s)\n      GROUP BY browser_name, browser_version) tmp;  This gets the count of the unique browser names and versions in the next 30 s (default duration). Note that we do not specify values for the keys in fields. This is because they are not relevant  {\n     records : [\n        {\n             COUNT DISTINCT : 158.0\n        }\n    ],\n     meta : {\n         rule_id : 4451146261377394443,\n         aggregation : {\n             standardDeviations : {\n                 1 : {\n                     upperBound : 158.0,\n                     lowerBound : 158.0\n                },\n                 2 : {\n                     upperBound : 158.0,\n                     lowerBound : 158.0\n                },\n                 3 : {\n                     upperBound : 158.0,\n                     lowerBound : 158.0\n                }\n            },\n             wasEstimated : false,\n             sketchFamily :  COMPACT ,\n             sketchTheta : 1.0,\n             sketchSize : 1280\n        },\n         rule_body :  RULE BODY HERE ,\n         rule_finish_time : 1484084869073,\n         rule_receive_time : 1484084832684\n    }\n}  There were 158 unique combinations on browser names and versions in our dataset for those 30 s. Note the new  aggregation  object in the meta. It has various metadata about the result and Sketches. In particular, the  wasEstimated  key denotes where the result\nwas estimated or not. The  standardDeviations  key denotes the confidence at various sigmas: 1 (1 sigma = ~68% confidence, 2 sigma = ~95% confidence, 3 sigma = ~99% confidence). Since this result was not estimated, the result is the same as the upper and lower bounds for the result.", 
            "title": "Exact COUNT DISTINCT Aggregation"
        }, 
        {
            "location": "/ws/examples/#approximate-count-distinct-aggregation", 
            "text": "Bullet Query  {\n   aggregation : {\n       type :  COUNT DISTINCT ,\n       fields : {\n           ip_address :  \n      },\n       attributes : {\n           newName :  uniqueIPs \n      }\n    },\n     duration : 10000\n}  SQL  SELECT COUNT(*) AS uniqueIPs\nFROM (SELECT ip_address\n      FROM WINDOW(10s)\n      GROUP BY ip_address) tmp;  This query gets us the unique IP addresses in the next 10 s. It renames the result column from \"COUNT DISTINCT\" to \"uniqueIPs\".  {\n     records : [\n        {\n             uniqueIPs : 130551.07952805843\n        }\n    ],\n     meta : {\n         rule_id : 5377782455857451480,\n         aggregation : {\n             standardDeviations : {\n                 1 : {\n                     upperBound : 131512.85413760383,\n                     lowerBound : 129596.30223107953\n                },\n                 2 : {\n                     upperBound : 132477.15103015225,\n                     lowerBound : 128652.93906100772\n                },\n                 3 : {\n                     upperBound : 133448.49248615955,\n                     lowerBound : 127716.46773622213\n                }\n            },\n             wasEstimated : true,\n             sketchFamily :  COMPACT ,\n             sketchTheta : 0.12549877074343688,\n             sketchSize : 131096\n        },\n         rule_body :  RULE BODY HERE ,\n         rule_finish_time : 1484090240812,\n         rule_receive_time : 1484090223351\n    }\n}  The number of unique IPs in our dataset was 130551 in those 10 s (approximately) with the true value between (129596, 131512) at 68% confidence, (128652, 132477) at 95% confidence and (127716, 133448) at 99% confidence. In the  worst  case at 3 sigma (99% confidence),\nour error is 2.17%. The final result was computed with 131096 bytes or ~128 KiB as denoted by  sketchSize . This happens to be maximum size the the COUNT DISTINCT sketch will take up at the default nominal entries, so even if we had billions of unique IPs, the size will be the same and the error may be higher (depends on the distribution). For example, the error when the same query was run for 30 s was 2.28% at 99% confidence (actual unique IPs: 559428, upper bound: 572514). In fact, the worst the error can get at this\nSketch size is 2.34% as defined  here ,  regardless of the number of unique entries added to the Sketch! .", 
            "title": "Approximate COUNT DISTINCT Aggregation"
        }, 
        {
            "location": "/ws/examples/#distinct-aggregation", 
            "text": "Bullet Query  {\n    aggregation :{\n       type :  DISTINCT ,\n       size : 10,\n       fields : {\n          browser_name :  browser \n      }\n   }\n}  SQL  SELECT browser_name AS browser\nFROM WINDOW(30s)\nGROUP BY browser_name\nLIMIT 10;  This query gets the distinct values for the browser_name field and limit the results to 10. It runs for 30 s.  {\n   records :[\n    {\n       browser : opera \n    },\n    {\n       browser : flock \n    },\n    {\n       browser : links \n    },\n    {\n       browser : mozilla firefox \n    },\n    {\n       browser : dolfin \n    },\n    {\n       browser : lynx \n    },\n    {\n       browser : chrome \n    },\n    {\n       browser : microsoft internet explorer \n    },\n    {\n       browser : aol browser \n    },\n    {\n       browser : edge \n    }\n  ],\n   meta :{\n     rule_id :-4872093887360741287,\n     aggregation :{\n       standardDeviations :{\n         1 :{\n           upperBound :28.0,\n           lowerBound :28.0\n        },\n         2 :{\n           upperBound :28.0,\n           lowerBound :28.0\n        },\n         3 :{\n           upperBound :28.0,\n           lowerBound :28.0\n        }\n      },\n       wasEstimated :false,\n       uniquesEstimate :28.0,\n       sketchTheta :1.0\n    },\n     rule_body : RULE_BODY_HERE ,\n     rule_finish_time :1485469087971,\n     rule_receive_time :1485469054070\n  }\n}  There were 28 unique results but we asked for 10, so the query returned a uniform sample across the 28 distinct values.  DISTINCT is just an alias for GROUP. A GROUP by with no operations is exactly a DISTINCT.", 
            "title": "DISTINCT Aggregation"
        }, 
        {
            "location": "/ws/examples/#group-by-aggregation", 
            "text": "Bullet Query  {\n    filters :[\n      {\n          field :  demographics ,\n          operation :  != ,\n          values : [ null ]\n      }\n   ],\n    aggregation :{\n       type :  GROUP ,\n       size : 50,\n       fields : {\n           demographics.country :  country ,\n           device :  \n      },\n       attributes : {\n          operations : [\n            {\n                type :  COUNT ,\n                newName :  count \n            },\n            {\n                type :  AVG ,\n                field :  demographics.age ,\n                newName :  averageAge \n            },\n            {\n                type :  AVG ,\n                field :  timespent ,\n                newName :  averageTimespent \n            }\n         ]\n      }\n   },\n    duration : 20000\n}  SQL  SELECT demographics[ country ] AS country, device AS device,\n       COUNT(*) AS count, AVG(demographics[ age ]) AS averageAge,\n       AVG(timespent) AS averageTimespent\nFROM WINDOW(20s)\nWHERE demographics IS NOT NULL\nGROUP BY demographics[ country ], device\nLIMIT 50;  This query groups by the country and the device and for each unique group gets the count, average age and time spent by the users for the next 20 seconds. It renames demographics.country to country and does not rename device. It limits the groups to 50. If there were more than\n50 groups, the results would be a uniform sampling of the groups (but each group in the result would have the correct result). These parameters can all be tweaked  in the configuration .  {\n   records :[\n    {\n       country : uk ,\n       device : desktop ,\n       count :203034,\n       averageAge :32.42523,\n       averageTimespent :1.342\n    },\n    {\n       country : us ,\n       device : desktop ,\n       count :1934030,\n       averageAge :29.42523,\n       averageTimespent :3.234520\n    },\n     ...and 41 other such records here \n  ],\n   meta :{\n     rule_id :1705911449584057747,\n     aggregation :{\n       standardDeviations :{\n         1 :{\n           upperBound :43.0,\n           lowerBound :43.0\n        },\n         2 :{\n           upperBound :43.0,\n           lowerBound :43.0\n        },\n         3 :{\n           upperBound :43.0,\n           lowerBound :43.0\n        }\n      },\n       wasEstimated :false,\n       uniquesEstimate :43.0,\n       sketchTheta :1.0\n    },\n     rule_body : RULE_BODY_HERE ,\n     rule_finish_time :1485217172780,\n     rule_receive_time :1485217148840\n  }\n}  We received 43 rows for this result. The maximum groups that was allowed for the instance of Bullet was 512. If there were more groups than the maximum specified by your configuration,  a uniform sample  across them would be chosen\nfor the result. However, for each group, the values computed (average, count) would be exact. The standard deviations, whether the result was estimated and the number of approximate uniques in the metadata would reflect the change.  If you asked for 50 rows in the aggregation (as the query did above) but there were more than 50 in the result (but   512), the metadata would reflect the fact that the result was not estimated. You would still get a uniform sample\nbut by increasing your aggregation size higher, you could get the rest.  For readability, if you were just trying to get the unique values for a field or a set of fields, you could leave out the attributes section and specify your fields section. You could also call the type  DISTINCT  instead of GROUP  to make that explicit.  DISTINCT  is just an alias for  GROUP . See  the DISTINCT example .", 
            "title": "GROUP by Aggregation"
        }, 
        {
            "location": "/ui/setup/", 
            "text": "The UI Layer\n\n\nThe Bullet UI lets you easily create and work with Bullet queries and results for your custom data. It stores all created queries, results and other metadata in the local browser storage or \nLocalStorage\n.\n\n\nWhile LocalStorage is sufficient for simple usage, the UI users can run out of space when a lot of queries and results are being stored. We are looking into more robust solutions like \nLocalForage\n. See \n#9\n. This should be landing soon\n.\n\n\n\n\nReally!? LocalStorage only!?\n\n\nWe're serious about the no persistence thing with Bullet! And while we're at it, we are also not interested in supporting old browsers. Joking aside though, we wanted to keep Bullet as light and simple as possible to start with. We can look into extending support from the server-side by adding a database or the like if needed. In practice, we have found that this isn't as important as it initially seems.\n\n\n\n\nPrerequisites\n\n\nIn order for your UI to work with Bullet, you should have:\n\n\n\n\nAn instance of the \nbackend\n set up\n\n\nAn instance of the \nWeb Service\n set up\n\n\nYou should also have a Web Service serving your schema (either by using the \nfile based serving\n from the Web Service or your own somewhere else)\n\n\n\n\nInstallation\n\n\nWe are considering various packaging options at the moment like Docker etc. In the meantime, the following two options are available:\n\n\nGitHub Releases\n\n\n\n\nHead to the \nReleases page\n and grab the latest release\n\n\nDownload the bullet-ui-vX.X.X.tar.gz archive\n\n\nUnarchive it into your web server where you wish to run the UI.\n\n\nInstall \nNode\n (recommend using \nnvm\n to manage Node versions) on the web server\n\n\n\n\nBuild from source\n\n\n\n\nInstall \nNode\n (recommend using \nnvm\n to manage Node versions).\n\n\nInstall \nBower\n. Use NPM to install it with \nsudo npm install -g bower\n\n\nInstall \nEmber\n. \nsudo npm install -g ember-cli\n (sudo required only if not using nvm)\n\n\ngit clone git@github.com:yahoo/bullet-ui.git\n\n\ncd bullet-ui\n\n\nnpm install\n\n\nbower install\n\n\nember build --environment production\n\n\n\n\nThe entire application with all its assets and dependencies are compiled and placed into dist/. You could point a web server directly at this folder but you will \nonly\n be able to use the default configuration (see \nbelow\n).\n\n\nRunning\n\n\nThere is a Node.js server endpoint defined at \nserver/index.js\n to serve the UI. This dynamically injects the settings (see configuration \nbelow\n) into the served UI based on the environment variable NODE_ENV. You should not need to worry about if you only have one environment.\n\n\nThe entrypoint for the UI is the \nExpress\n endpoint defined as the main in package.json that simply adds the server/index.js as a middleware.\n\n\nRegardless of which \ninstallation\n option you chose, you need the following folder structure in order to run the UI:\n\n\ndist/*\nconfig/env-settings.json\nserver/index.js\nexpress-server.js\n\n\n\n\nYou can use node to launch the UI from the top-level of the folder structure above.\n\n\nTo launch the UI with the default settings (without specifying proper API endpoints you will not be able to create or run a query):\n\n\nPORT=8800 node express-server.js\n\n\n\n\nTo launch with custom settings:\n\n\nNODE_ENV=\nyour_property_name_from_env-settings.json\n PORT=8800 node express-server.js\n\n\n\n\nVisit localhost:8800 to see your UI that should be configured with the right settings.\n\n\nConfiguration\n\n\nAll of the configuration for the UI is \nenvironment-specific\n. This lets you have different instances of Bullet for different environments (e.g. CI, Staging, Production).\nThese settings can be found in \nenv-settings.json\n.\n\n\nEach property in the env-settings.json file will contain the settings that will be used when running a custom instance of the UI (see \nabove\n).\n\n\nThe \ndefault\n property shows the default settings for the UI that can be selectively overridden based on which host you are running on. The file does not specify the \ndefaultFilter\n setting shown below.\n\n\ndefault\n: {\n  \ndrpcHost\n: \nhttp://foo.bar.com:4080\n,\n  \ndrpcNamespace\n: \nbullet/api\n,\n  \ndrpcPath\n: \ndrpc\n,\n  \nschemaHost\n: \nhttp://foo.bar.com:4080\n,\n  \nschemaNamespace\n: \nbullet/api\n,\n  \nhelpLinks\n: [\n    {\n      \nname\n: \nExample Docs Page\n,\n      \nlink\n: \n\n    }\n  ],\n  \ndefaultFilter\n: {\n      \nclauses\n: [\n          {\n              \nfield\n: \nprimary_key\n,\n              \nvalues\n:[\n123123123321321\n],\n              \noperation\n:\n==\n\n          }\n      ],\n      \noperation\n:\nAND\n\n  },\n  \naggregateDataDefaultSize\n: 512,\n  \nmodelVersion\n: 1\n}\n\n\n\n\nYou can add more configuration at the top level for each host you have the UI running on.\n\n\ndrpcHost\n is the end point (port included) of your Web Service machine that is proxying to the Bullet topology.\n\n\ndrpcNamespace\n is the fragment of the path to your Web Service on the \ndrpcHost\n.\n\n\nschemaHost\n is the end point (port included) of your Web Service machine that is serving your schema in the JSON API format (see the \nWeb Service setup\n for details.)\n\n\nschemaNamespace\n is the fragment of the path to your schema Web Service on the \nschemaHost\n. There is no \nschemaPath\n because it \nmust\n be \"columns\" in order for the UI to be able fetch the column resource (columns in your schema).\n\n\nmodelVersion\n is a way for you to control your UI users' Ember models saved in LocalStorage. If there is a need for you to purge all your user's created queries, results and other data stored in their LocalStorage, then you should increment this number. The UI, on startup, will compare this number with what it has seen before (your old version) and purge the LocalStorage.\n\n\nhelpLinks\n is a list of objects, where each object is a help link. These links drive the dropdown list when you click the \"Help\" button on the UI's top navbar. You can use this to point to your particular help links. For example, you could use this to point your users toward a page that\nhelps them understand your data (that this UI is operating on).\n\n\ndefaultFilter\n can either be an \nAPI Filter\n or a URL from which one could be fetched dynamically. The UI adds this filter to every newly created Query. You could use this as a way to have user specific (for example, cookie based) filters created for your users when they create a new query in the UI.\n\n\nbugLink\n is a url that by default points to the issues page for the UI GitHub repository (this). You can change it to point to your own custom JIRA queue or the like if you want to.\n\n\naggregateDataDefaultSize\n is the aggregation size for all queries that are not pulling raw data. In order to keep the\naggregation size from being ambiguous for UI users when doing a Count Distinct or a Distinct or a Group By query, this is\nthe size that is used. You should set this to your max size that you have configured for your non-raw aggregations in\nyour topology configuration.\n\n\nNote that all your Web Service endpoints must support CORS (return the right headers) in order for the UI to be able to communicate with it.\n The Bullet Web Service already does this for the DRPC and columns endpoints.\n\n\nTo cement all this, if you wanted an instance of the UI in your CI environment, you could add this to the env-settings.json file.\n\n\n{\n  \ndefault\n: {\n      \ndrpcHost\n: \n,\n      \ndrpcNamespace\n: \nbullet/api\n,\n      \ndrpcPath\n: \ndrpc\n,\n      \nschemaHost\n: \n,\n      \nschemaNamespace\n: \nbullet/api\n,\n      \nhelpLinks\n: [\n        {\n          \nname\n: \nData Documentation\n,\n          \nlink\n: \nhttp://data.docs.domain.com\n\n        }\n      ],\n      \nbugLink\n: \nhttp://your.issues.page.com\n,\n      \naggregateDataDefaultSize\n: 500,\n      \nmodelVersion\n: 1\n  },\n   \nci\n: {\n        \ndrpcHost\n: \nhttp://bullet-ws.development.domain.com:4080\n,\n        \nschemaHost\n: \nhttp://bullet-ws.development.domain.com:4080\n,\n        \ndefaultFilter\n: \nhttp://bullet-ws.development.domain.com:4080/custom-endpoint/api/defaultQuery\n\n      }\n}\n\n\n\n\nYour UI on CI host will POST to http://bullet-ws.development.domain.com:4080/bullet/api/drpc for UI created Bullet queries, GET the schema from http://bullet-ws.development.domain.com:4080/bullet/api/columns, populate an additional link on the Help dropdown pointing to http://data.docs.domain.com and will GET and cache a defaultFilter from http://bullet-ws.development.domain.com:4080/custom-endpoint/api/defaultQuery.", 
            "title": "Setup"
        }, 
        {
            "location": "/ui/setup/#the-ui-layer", 
            "text": "The Bullet UI lets you easily create and work with Bullet queries and results for your custom data. It stores all created queries, results and other metadata in the local browser storage or  LocalStorage .  While LocalStorage is sufficient for simple usage, the UI users can run out of space when a lot of queries and results are being stored. We are looking into more robust solutions like  LocalForage . See  #9 . This should be landing soon .   Really!? LocalStorage only!?  We're serious about the no persistence thing with Bullet! And while we're at it, we are also not interested in supporting old browsers. Joking aside though, we wanted to keep Bullet as light and simple as possible to start with. We can look into extending support from the server-side by adding a database or the like if needed. In practice, we have found that this isn't as important as it initially seems.", 
            "title": "The UI Layer"
        }, 
        {
            "location": "/ui/setup/#prerequisites", 
            "text": "In order for your UI to work with Bullet, you should have:   An instance of the  backend  set up  An instance of the  Web Service  set up  You should also have a Web Service serving your schema (either by using the  file based serving  from the Web Service or your own somewhere else)", 
            "title": "Prerequisites"
        }, 
        {
            "location": "/ui/setup/#installation", 
            "text": "We are considering various packaging options at the moment like Docker etc. In the meantime, the following two options are available:", 
            "title": "Installation"
        }, 
        {
            "location": "/ui/setup/#github-releases", 
            "text": "Head to the  Releases page  and grab the latest release  Download the bullet-ui-vX.X.X.tar.gz archive  Unarchive it into your web server where you wish to run the UI.  Install  Node  (recommend using  nvm  to manage Node versions) on the web server", 
            "title": "GitHub Releases"
        }, 
        {
            "location": "/ui/setup/#build-from-source", 
            "text": "Install  Node  (recommend using  nvm  to manage Node versions).  Install  Bower . Use NPM to install it with  sudo npm install -g bower  Install  Ember .  sudo npm install -g ember-cli  (sudo required only if not using nvm)  git clone git@github.com:yahoo/bullet-ui.git  cd bullet-ui  npm install  bower install  ember build --environment production   The entire application with all its assets and dependencies are compiled and placed into dist/. You could point a web server directly at this folder but you will  only  be able to use the default configuration (see  below ).", 
            "title": "Build from source"
        }, 
        {
            "location": "/ui/setup/#running", 
            "text": "There is a Node.js server endpoint defined at  server/index.js  to serve the UI. This dynamically injects the settings (see configuration  below ) into the served UI based on the environment variable NODE_ENV. You should not need to worry about if you only have one environment.  The entrypoint for the UI is the  Express  endpoint defined as the main in package.json that simply adds the server/index.js as a middleware.  Regardless of which  installation  option you chose, you need the following folder structure in order to run the UI:  dist/*\nconfig/env-settings.json\nserver/index.js\nexpress-server.js  You can use node to launch the UI from the top-level of the folder structure above.  To launch the UI with the default settings (without specifying proper API endpoints you will not be able to create or run a query):  PORT=8800 node express-server.js  To launch with custom settings:  NODE_ENV= your_property_name_from_env-settings.json  PORT=8800 node express-server.js  Visit localhost:8800 to see your UI that should be configured with the right settings.", 
            "title": "Running"
        }, 
        {
            "location": "/ui/setup/#configuration", 
            "text": "All of the configuration for the UI is  environment-specific . This lets you have different instances of Bullet for different environments (e.g. CI, Staging, Production).\nThese settings can be found in  env-settings.json .  Each property in the env-settings.json file will contain the settings that will be used when running a custom instance of the UI (see  above ).  The  default  property shows the default settings for the UI that can be selectively overridden based on which host you are running on. The file does not specify the  defaultFilter  setting shown below.  default : {\n   drpcHost :  http://foo.bar.com:4080 ,\n   drpcNamespace :  bullet/api ,\n   drpcPath :  drpc ,\n   schemaHost :  http://foo.bar.com:4080 ,\n   schemaNamespace :  bullet/api ,\n   helpLinks : [\n    {\n       name :  Example Docs Page ,\n       link :  \n    }\n  ],\n   defaultFilter : {\n       clauses : [\n          {\n               field :  primary_key ,\n               values :[ 123123123321321 ],\n               operation : == \n          }\n      ],\n       operation : AND \n  },\n   aggregateDataDefaultSize : 512,\n   modelVersion : 1\n}  You can add more configuration at the top level for each host you have the UI running on.  drpcHost  is the end point (port included) of your Web Service machine that is proxying to the Bullet topology.  drpcNamespace  is the fragment of the path to your Web Service on the  drpcHost .  schemaHost  is the end point (port included) of your Web Service machine that is serving your schema in the JSON API format (see the  Web Service setup  for details.)  schemaNamespace  is the fragment of the path to your schema Web Service on the  schemaHost . There is no  schemaPath  because it  must  be \"columns\" in order for the UI to be able fetch the column resource (columns in your schema).  modelVersion  is a way for you to control your UI users' Ember models saved in LocalStorage. If there is a need for you to purge all your user's created queries, results and other data stored in their LocalStorage, then you should increment this number. The UI, on startup, will compare this number with what it has seen before (your old version) and purge the LocalStorage.  helpLinks  is a list of objects, where each object is a help link. These links drive the dropdown list when you click the \"Help\" button on the UI's top navbar. You can use this to point to your particular help links. For example, you could use this to point your users toward a page that\nhelps them understand your data (that this UI is operating on).  defaultFilter  can either be an  API Filter  or a URL from which one could be fetched dynamically. The UI adds this filter to every newly created Query. You could use this as a way to have user specific (for example, cookie based) filters created for your users when they create a new query in the UI.  bugLink  is a url that by default points to the issues page for the UI GitHub repository (this). You can change it to point to your own custom JIRA queue or the like if you want to.  aggregateDataDefaultSize  is the aggregation size for all queries that are not pulling raw data. In order to keep the\naggregation size from being ambiguous for UI users when doing a Count Distinct or a Distinct or a Group By query, this is\nthe size that is used. You should set this to your max size that you have configured for your non-raw aggregations in\nyour topology configuration.  Note that all your Web Service endpoints must support CORS (return the right headers) in order for the UI to be able to communicate with it.  The Bullet Web Service already does this for the DRPC and columns endpoints.  To cement all this, if you wanted an instance of the UI in your CI environment, you could add this to the env-settings.json file.  {\n   default : {\n       drpcHost :  ,\n       drpcNamespace :  bullet/api ,\n       drpcPath :  drpc ,\n       schemaHost :  ,\n       schemaNamespace :  bullet/api ,\n       helpLinks : [\n        {\n           name :  Data Documentation ,\n           link :  http://data.docs.domain.com \n        }\n      ],\n       bugLink :  http://your.issues.page.com ,\n       aggregateDataDefaultSize : 500,\n       modelVersion : 1\n  },\n    ci : {\n         drpcHost :  http://bullet-ws.development.domain.com:4080 ,\n         schemaHost :  http://bullet-ws.development.domain.com:4080 ,\n         defaultFilter :  http://bullet-ws.development.domain.com:4080/custom-endpoint/api/defaultQuery \n      }\n}  Your UI on CI host will POST to http://bullet-ws.development.domain.com:4080/bullet/api/drpc for UI created Bullet queries, GET the schema from http://bullet-ws.development.domain.com:4080/bullet/api/columns, populate an additional link on the Help dropdown pointing to http://data.docs.domain.com and will GET and cache a defaultFilter from http://bullet-ws.development.domain.com:4080/custom-endpoint/api/defaultQuery.", 
            "title": "Configuration"
        }, 
        {
            "location": "/ui/usage/", 
            "text": "Navigating the UI\n\n\nThe UI should (hopefully) be self-explanatory. Any particular section that requires additional information has the \n icon next to it. Clicking this will display information relevant to that section.\n\n\nComing soon!", 
            "title": "Usage"
        }, 
        {
            "location": "/ui/usage/#navigating-the-ui", 
            "text": "The UI should (hopefully) be self-explanatory. Any particular section that requires additional information has the   icon next to it. Clicking this will display information relevant to that section.", 
            "title": "Navigating the UI"
        }, 
        {
            "location": "/ui/usage/#coming-soon", 
            "text": "", 
            "title": "Coming soon!"
        }, 
        {
            "location": "/about/releases/", 
            "text": "Releases\n\n\nThis sections gathers all the relevant releases of the three components of Bullet in one place. It may not include the very few initial releases of these components if they were largely irrelevant. Full release notes can be found by clicking on the actual releases.\n\n\nBullet is still in active development. We welcome all contributions. Feel free to raise any issues/questions/bugs and whatever else on the relevant issues section for each component. Please include as many details as you can.\n\n\nBullet Storm\n\n\nThe implementation of Bullet on Storm. Due to major API changes between Storm \n= 0.10 and Storm 1.0, Bullet Storm \nbuilds two artifacts\n. The \nartifactId\n changes from \nbullet-storm\n (for 1.0+) to \nbullet-storm-0.10\n.\nAll releases include migration and testing of the code on \nboth\n versions. Both versions are built simultaneously. Feature parity depends on what was new in Storm 1.0. For example, the Resource Aware Scheduler or RAS, is only present in Storm 1.0+. So, bullet-storm-0.10 removes\ncertain CPU and memory related settings specific to RAS in its configuration. There are also minor changes to the Metrics API in Storm. In terms of Bullet itself, there should be no differences.\n\n\n\n\nFuture support\n\n\nWe will support Storm 0.10 for a bit longer till Storm 2.0 is up and stable. Storm 1.0+ have a lot of performance fixes and features that you should be running with.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nStorm-1.0+ Repository\n\n\nhttps://github.com/yahoo/bullet-storm\n\n\n\n\n\n\nStorm-0.10- Repository\n\n\nhttps://github.com/yahoo/bullet-storm/tree/storm-0.10\n\n\n\n\n\n\nIssues\n\n\nhttps://github.com/yahoo/bullet-storm/issues\n\n\n\n\n\n\nLast Tag\n\n\n\n\n\n\n\n\nLatest Artifact\n\n\n\n\n\n\n\n\n\n\nReleases\n\n\n\n\n\n\n\n\nDate\n\n\nStorm 1.0\n\n\nStorm 0.10\n\n\nHighlights\n\n\n\n\n\n\n\n\n\n\n2017-02-27\n\n\n0.3.0\n\n\n0.3.0\n\n\nMetrics interface, config namespace, NPE bug fix\n\n\n\n\n\n\n2017-02-15\n\n\n0.2.1\n\n\n0.2.1\n\n\nAcking support, Max size and other bug fixes\n\n\n\n\n\n\n2017-01-26\n\n\n0.2.0\n\n\n0.2.0\n\n\nGROUP (DISTINCT, SUM, COUNT, MIN, MAX, AVG)\n\n\n\n\n\n\n2017-01-09\n\n\n0.1.0\n\n\n0.1.0\n\n\nCOUNT DISTINCT and micro-batching\n\n\n\n\n\n\n\n\nBullet Web Service\n\n\nThe Web Service implementation that can serve a static schema from a file and talk to the Storm backend.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRepository\n\n\nhttps://github.com/yahoo/bullet-service\n\n\n\n\n\n\nIssues\n\n\nhttps://github.com/yahoo/bullet-service/issues\n\n\n\n\n\n\nLast Tag\n\n\n\n\n\n\n\n\nLatest Artifact\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDate\n\n\nRelease\n\n\nHighlights\n\n\n\n\n\n\n\n\n\n\n2016-12-16\n\n\n0.0.1\n\n\nThe first release with support for DRPC and the file-based schema\n\n\n\n\n\n\n\n\nBullet UI\n\n\nThe Bullet UI that lets you build, run, save and visualize results from Bullet.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRepository\n\n\nhttps://github.com/yahoo/bullet-ui\n\n\n\n\n\n\nIssues\n\n\nhttps://github.com/yahoo/bullet-ui/issues\n\n\n\n\n\n\nLast Tag\n\n\n\n\n\n\n\n\nLatest Artifact\n\n\n\n\n\n\n\n\n\n\nReleases\n\n\n\n\n\n\n\n\nDate\n\n\nRelease\n\n\nHighlights\n\n\n\n\n\n\n\n\n\n\n2016-02-21\n\n\n0.1.0\n\n\nThe first release with support for all features included in Bullet Storm 0.2.1+\n\n\n\n\n\n\n\n\nBullet Record\n\n\nThe AVRO container that you need to convert your data into to be consumed by Bullet.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRepository\n\n\nhttps://github.com/yahoo/bullet-record\n\n\n\n\n\n\nIssues\n\n\nhttps://github.com/yahoo/bullet-record/issues\n\n\n\n\n\n\nLast Tag\n\n\n\n\n\n\n\n\nLatest Artifact\n\n\n\n\n\n\n\n\n\n\nReleases\n\n\n\n\n\n\n\n\nDate\n\n\nRelease\n\n\nHighlights\n\n\n\n\n\n\n\n\n\n\n2017-02-09\n\n\n0.1.0\n\n\nMap constructor", 
            "title": "Releases"
        }, 
        {
            "location": "/about/releases/#releases", 
            "text": "This sections gathers all the relevant releases of the three components of Bullet in one place. It may not include the very few initial releases of these components if they were largely irrelevant. Full release notes can be found by clicking on the actual releases.  Bullet is still in active development. We welcome all contributions. Feel free to raise any issues/questions/bugs and whatever else on the relevant issues section for each component. Please include as many details as you can.", 
            "title": "Releases"
        }, 
        {
            "location": "/about/releases/#bullet-storm", 
            "text": "The implementation of Bullet on Storm. Due to major API changes between Storm  = 0.10 and Storm 1.0, Bullet Storm  builds two artifacts . The  artifactId  changes from  bullet-storm  (for 1.0+) to  bullet-storm-0.10 .\nAll releases include migration and testing of the code on  both  versions. Both versions are built simultaneously. Feature parity depends on what was new in Storm 1.0. For example, the Resource Aware Scheduler or RAS, is only present in Storm 1.0+. So, bullet-storm-0.10 removes\ncertain CPU and memory related settings specific to RAS in its configuration. There are also minor changes to the Metrics API in Storm. In terms of Bullet itself, there should be no differences.   Future support  We will support Storm 0.10 for a bit longer till Storm 2.0 is up and stable. Storm 1.0+ have a lot of performance fixes and features that you should be running with.            Storm-1.0+ Repository  https://github.com/yahoo/bullet-storm    Storm-0.10- Repository  https://github.com/yahoo/bullet-storm/tree/storm-0.10    Issues  https://github.com/yahoo/bullet-storm/issues    Last Tag     Latest Artifact", 
            "title": "Bullet Storm"
        }, 
        {
            "location": "/about/releases/#releases_1", 
            "text": "Date  Storm 1.0  Storm 0.10  Highlights      2017-02-27  0.3.0  0.3.0  Metrics interface, config namespace, NPE bug fix    2017-02-15  0.2.1  0.2.1  Acking support, Max size and other bug fixes    2017-01-26  0.2.0  0.2.0  GROUP (DISTINCT, SUM, COUNT, MIN, MAX, AVG)    2017-01-09  0.1.0  0.1.0  COUNT DISTINCT and micro-batching", 
            "title": "Releases"
        }, 
        {
            "location": "/about/releases/#bullet-web-service", 
            "text": "The Web Service implementation that can serve a static schema from a file and talk to the Storm backend.           Repository  https://github.com/yahoo/bullet-service    Issues  https://github.com/yahoo/bullet-service/issues    Last Tag     Latest Artifact         Date  Release  Highlights      2016-12-16  0.0.1  The first release with support for DRPC and the file-based schema", 
            "title": "Bullet Web Service"
        }, 
        {
            "location": "/about/releases/#bullet-ui", 
            "text": "The Bullet UI that lets you build, run, save and visualize results from Bullet.           Repository  https://github.com/yahoo/bullet-ui    Issues  https://github.com/yahoo/bullet-ui/issues    Last Tag     Latest Artifact", 
            "title": "Bullet UI"
        }, 
        {
            "location": "/about/releases/#releases_2", 
            "text": "Date  Release  Highlights      2016-02-21  0.1.0  The first release with support for all features included in Bullet Storm 0.2.1+", 
            "title": "Releases"
        }, 
        {
            "location": "/about/releases/#bullet-record", 
            "text": "The AVRO container that you need to convert your data into to be consumed by Bullet.           Repository  https://github.com/yahoo/bullet-record    Issues  https://github.com/yahoo/bullet-record/issues    Last Tag     Latest Artifact", 
            "title": "Bullet Record"
        }, 
        {
            "location": "/about/releases/#releases_3", 
            "text": "Date  Release  Highlights      2017-02-09  0.1.0  Map constructor", 
            "title": "Releases"
        }
    ]
}