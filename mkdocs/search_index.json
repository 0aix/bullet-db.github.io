{
    "docs": [
        {
            "location": "/", 
            "text": "Overview\n\n\nBullet ...\n\n\n\n\n\n\nIs a real-time query engine that lets you run queries on very large data streams\n\n\n\n\n\n\nDoes not use a \na persistence layer\n. This makes it \nlight-weight, cheap and fast\n\n\n\n\n\n\nIs a \nlook-forward\n query system. Queries are submitted first and they operate on data that arrive after the query is submitted\n\n\n\n\n\n\nIs \nmulti-tenant\n and can scale independently for more queries and for more data in the first order\n\n\n\n\n\n\nProvides a \nUI and Web Service\n that are also pluggable for a full end-to-end solution to your querying needs\n\n\n\n\n\n\nCan be implemented on different Stream processing frameworks. Bullet on \nStorm\n is currently available\n\n\n\n\n\n\nIs \npluggable\n. Any data source that can be read from Storm can be converted into a standard data container letting you query that data. Data is \ntyped\n\n\n\n\n\n\nIs used at scale and in production at Yahoo with running 500+ queries simultaneously on 200,000 rps (records per second) and tested up to 2,000,000 rps\n\n\n\n\n\n\nHow is this useful\n\n\nHow Bullet is used is largely determined by the data source it consumes. Depending on what kind of data you put Bullet on, the types of queries you run on it and your use-cases will change. As a look-forward query system with no persistence, you will not be able to repeat your queries on the same data. The next time you run your query, it will operate on the different data that arrives after that submission. If this usage pattern is what you need and you are looking for a light-weight system that can tap into your streaming data, then Bullet is for you!\n\n\nExample: How Bullet is used at Yahoo\n\n\nBullet is used in production internally at Yahoo by having it sit on a subset of raw user engagement events from Yahoo sites and apps. This lets Yahoo developers automatically validate their instrumentation code \nend-to-end\n in their Continuous Delivery pipelines. Validating instrumentation is critical since it powers pretty much all decisions and products including machine learning, corporate KPIs, analytics, personalization, targeting.\n\n\nThis instance of Bullet also powers other use-cases such as letting analysts validate assumptions about data, product managers verify launches instantly, debug issues and outages, or simply explore and play around with the data.\n\n\n\n\nQuick Start\n\n\nSee \nQuick Start\n to set up Bullet on a local Storm topology. We will generate some fake streaming data that you can then query with Bullet.\n\n\nSetting up Bullet on your streaming data\n\n\nTo set up Bullet on a real data stream, you need:\n\n\n\n\nThe backend set up on a Stream processor:\n\n\nPlug in your source of data. See \nGetting your data into Bullet\n for details\n\n\nConsume your data stream. Currently, we support \nBullet on Storm\n\n\n\n\n\n\nThe \nWeb Service\n set up to convey queries and return results back from the backend\n\n\nThe optional \nUI\n set up to talk to your Web Service. You can skip the UI if all your access is programmatic\n\n\n\n\n\n\nSchema in the UI\n\n\nThe UI also needs an endpoint that provides your data schema to help with query building. The Web Service you set up provides a simple file based schema endpoint that you can point the UI to if that is sufficient for your needs.\n\n\n\n\n\n\nQuerying in Bullet\n\n\nBullet queries allow you to filter, project and aggregate data. It lets you fetch raw (the individual data records) as well as aggregated data.\n\n\nSee the \nUI Usage section\n for using the UI to build Bullet queries. This is the same UI you will build in the \nQuick Start\n\n\nSee the \nAPI section\n for building Bullet API queries.\n\n\nFor examples using the API, see \nExamples\n. These are actual albeit cleansed queries sourced from the instance at Yahoo.\n\n\nTermination conditions\n\n\nA Bullet query terminates and returns whatever has been collected so far when:\n\n\n\n\nA maximum duration is reached. In other words, a query runs for a defined time window\n\n\nA maximum number of records is reached (only applicable for queries that are fetching raw data records and not aggregating).\n\n\n\n\nFilters\n\n\nBullet supports two kinds of filters:\n\n\n\n\n\n\n\n\nFilter Type\n\n\nMeaning\n\n\n\n\n\n\n\n\n\n\nLogical filter\n\n\nAllow you to combine filter clauses (Logical or Relational) with logical operations like AND, OR and NOTs\n\n\n\n\n\n\nRelational filters\n\n\nAllow you to use comparison operations like equals, not equals, greater than, less than, regex like etc, on fields\n\n\n\n\n\n\n\n\nProjections\n\n\nProjections allow you to pull out only the fields needed and rename them when you are querying for raw data records.\n\n\nAggregations\n\n\nAggregations allow you to perform some operation on the collected records.\n\n\nThe current aggregation types that are supported are:\n\n\n\n\n\n\n\n\nAggregation\n\n\nMeaning\n\n\n\n\n\n\n\n\n\n\nGROUP\n\n\nThe resulting output would be a record containing the result of an operation for each unique value combination in your specified fields\n\n\n\n\n\n\nCOUNT DISTINCT\n\n\nComputes the number of distinct elements in the fields. (May be approximate)\n\n\n\n\n\n\nLIMIT or RAW\n\n\nThe resulting output would be at most the number specified in size.\n\n\n\n\n\n\n\n\nCurrently we support \nGROUP\n aggregations on the following operations:\n\n\n\n\n\n\n\n\nOperation\n\n\nMeaning\n\n\n\n\n\n\n\n\n\n\nCOUNT\n\n\nComputes the number of the elements in the group\n\n\n\n\n\n\nSUM\n\n\nComputes the sum of the elements in the group\n\n\n\n\n\n\nMIN\n\n\nReturns the minimum of the elements in the group\n\n\n\n\n\n\nMAX\n\n\nReturns the maximum of the elements in the group\n\n\n\n\n\n\nAVG\n\n\nComputes the average of the elements in the group\n\n\n\n\n\n\n\n\nResults\n\n\nThe Bullet Web Service returns your query result as well as associated metadata information in a structured JSON format. The UI can display the results in different formats.\n\n\n\n\nApproximate computation\n\n\nIt is often intractable to perform aggregations on an unbounded stream of data and still support arbitrary queries. However, it is possible if an exact answer is not required and the approximate answer's error is exactly quantifiable. There are stochastic algorithms and data structures that let us do this. We use \nData Sketches\n to perform aggregations such as counting uniques, and will be using Sketches to implement some future aggregations.\n\n\nSketches let us be exact in our computation up to configured thresholds and approximate after. The error is very controllable and quantifiable. All Bullet queries that use Sketches return the error bounds with Standard Deviations as part of the results so you can quantify the error exactly. Using Sketches lets us address otherwise hard to solve problems in sub-linear space.\n\n\nWe also use Sketches as a way to control high cardinality grouping (group by a natural key column or related) and rely on the Sketching data structure to drop excess groups. It is up to you setting up Bullet to determine to set Sketch sizes large or small enough for to satisfy the queries that will be performed on that instance of Bullet.\n\n\nNew query types coming soon\n\n\nUsing Sketches, we have implemented \nCOUNT DISTINCT\n and \nGROUP\n and are working on other aggregations including but not limited to:\n\n\n\n\n\n\n\n\nAggregation\n\n\nMeaning\n\n\n\n\n\n\n\n\n\n\nTOP K\n\n\nReturns the top K most frequently appearing values in the column\n\n\n\n\n\n\nDISTRIBUTION\n\n\nComputes distributions of the elements in the column. E.g. Find the median value or the 95th percentile of a field or graph the entire distribution as a histogram\n\n\n\n\n\n\n\n\nArchitecture\n\n\nBackend\n\n\n\n\nThe Bullet backend can be split into three main sub-systems:\n\n\n\n\nRequest Processor - receives queries, adds metadata and sends it to the rest of the system\n\n\nData Processor - converts the data from an stream and matches it against queries\n\n\nCombiner - combines results for different queries, performs final aggregations and returns results\n\n\n\n\nWeb Service and UI\n\n\nThe rest of the pieces are just the standard other two pieces in a full-stack application:\n\n\n\n\nA Web Service that talks to this backend\n\n\nA UI that talks to this Web Service\n\n\n\n\nThe \nBullet Web Service\n is built using \nJersey\n and the \nUI\n is built in \nEmber\n.\n\n\nThe Web Service can be deployed with your favorite servlet container like \nJetty\n. The UI is a client-side application that can be served using \nNode.js\n\n\nIn the case of Bullet on Storm, the Web Service and UI talk to the backend using \nStorm DRPC\n.\n\n\nEnd-to-End Architecture\n\n\n\n\n\n\nWant to know more?\n\n\nIn practice, the backend is implemented using the basic components that the Stream processing framework provides. See \nStorm Architecture\n for details.\n\n\n\n\nPast Releases and Source\n\n\nSee the \nReleases\n section where the various Bullet releases and repository links are collected in one place.", 
            "title": "Home"
        }, 
        {
            "location": "/#overview", 
            "text": "", 
            "title": "Overview"
        }, 
        {
            "location": "/#bullet", 
            "text": "Is a real-time query engine that lets you run queries on very large data streams    Does not use a  a persistence layer . This makes it  light-weight, cheap and fast    Is a  look-forward  query system. Queries are submitted first and they operate on data that arrive after the query is submitted    Is  multi-tenant  and can scale independently for more queries and for more data in the first order    Provides a  UI and Web Service  that are also pluggable for a full end-to-end solution to your querying needs    Can be implemented on different Stream processing frameworks. Bullet on  Storm  is currently available    Is  pluggable . Any data source that can be read from Storm can be converted into a standard data container letting you query that data. Data is  typed    Is used at scale and in production at Yahoo with running 500+ queries simultaneously on 200,000 rps (records per second) and tested up to 2,000,000 rps", 
            "title": "Bullet ..."
        }, 
        {
            "location": "/#how-is-this-useful", 
            "text": "How Bullet is used is largely determined by the data source it consumes. Depending on what kind of data you put Bullet on, the types of queries you run on it and your use-cases will change. As a look-forward query system with no persistence, you will not be able to repeat your queries on the same data. The next time you run your query, it will operate on the different data that arrives after that submission. If this usage pattern is what you need and you are looking for a light-weight system that can tap into your streaming data, then Bullet is for you!", 
            "title": "How is this useful"
        }, 
        {
            "location": "/#example-how-bullet-is-used-at-yahoo", 
            "text": "Bullet is used in production internally at Yahoo by having it sit on a subset of raw user engagement events from Yahoo sites and apps. This lets Yahoo developers automatically validate their instrumentation code  end-to-end  in their Continuous Delivery pipelines. Validating instrumentation is critical since it powers pretty much all decisions and products including machine learning, corporate KPIs, analytics, personalization, targeting.  This instance of Bullet also powers other use-cases such as letting analysts validate assumptions about data, product managers verify launches instantly, debug issues and outages, or simply explore and play around with the data.", 
            "title": "Example: How Bullet is used at Yahoo"
        }, 
        {
            "location": "/#quick-start", 
            "text": "See  Quick Start  to set up Bullet on a local Storm topology. We will generate some fake streaming data that you can then query with Bullet.", 
            "title": "Quick Start"
        }, 
        {
            "location": "/#setting-up-bullet-on-your-streaming-data", 
            "text": "To set up Bullet on a real data stream, you need:   The backend set up on a Stream processor:  Plug in your source of data. See  Getting your data into Bullet  for details  Consume your data stream. Currently, we support  Bullet on Storm    The  Web Service  set up to convey queries and return results back from the backend  The optional  UI  set up to talk to your Web Service. You can skip the UI if all your access is programmatic    Schema in the UI  The UI also needs an endpoint that provides your data schema to help with query building. The Web Service you set up provides a simple file based schema endpoint that you can point the UI to if that is sufficient for your needs.", 
            "title": "Setting up Bullet on your streaming data"
        }, 
        {
            "location": "/#querying-in-bullet", 
            "text": "Bullet queries allow you to filter, project and aggregate data. It lets you fetch raw (the individual data records) as well as aggregated data.  See the  UI Usage section  for using the UI to build Bullet queries. This is the same UI you will build in the  Quick Start  See the  API section  for building Bullet API queries.  For examples using the API, see  Examples . These are actual albeit cleansed queries sourced from the instance at Yahoo.", 
            "title": "Querying in Bullet"
        }, 
        {
            "location": "/#termination-conditions", 
            "text": "A Bullet query terminates and returns whatever has been collected so far when:   A maximum duration is reached. In other words, a query runs for a defined time window  A maximum number of records is reached (only applicable for queries that are fetching raw data records and not aggregating).", 
            "title": "Termination conditions"
        }, 
        {
            "location": "/#filters", 
            "text": "Bullet supports two kinds of filters:     Filter Type  Meaning      Logical filter  Allow you to combine filter clauses (Logical or Relational) with logical operations like AND, OR and NOTs    Relational filters  Allow you to use comparison operations like equals, not equals, greater than, less than, regex like etc, on fields", 
            "title": "Filters"
        }, 
        {
            "location": "/#projections", 
            "text": "Projections allow you to pull out only the fields needed and rename them when you are querying for raw data records.", 
            "title": "Projections"
        }, 
        {
            "location": "/#aggregations", 
            "text": "Aggregations allow you to perform some operation on the collected records.  The current aggregation types that are supported are:     Aggregation  Meaning      GROUP  The resulting output would be a record containing the result of an operation for each unique value combination in your specified fields    COUNT DISTINCT  Computes the number of distinct elements in the fields. (May be approximate)    LIMIT or RAW  The resulting output would be at most the number specified in size.     Currently we support  GROUP  aggregations on the following operations:     Operation  Meaning      COUNT  Computes the number of the elements in the group    SUM  Computes the sum of the elements in the group    MIN  Returns the minimum of the elements in the group    MAX  Returns the maximum of the elements in the group    AVG  Computes the average of the elements in the group", 
            "title": "Aggregations"
        }, 
        {
            "location": "/#results", 
            "text": "The Bullet Web Service returns your query result as well as associated metadata information in a structured JSON format. The UI can display the results in different formats.", 
            "title": "Results"
        }, 
        {
            "location": "/#approximate-computation", 
            "text": "It is often intractable to perform aggregations on an unbounded stream of data and still support arbitrary queries. However, it is possible if an exact answer is not required and the approximate answer's error is exactly quantifiable. There are stochastic algorithms and data structures that let us do this. We use  Data Sketches  to perform aggregations such as counting uniques, and will be using Sketches to implement some future aggregations.  Sketches let us be exact in our computation up to configured thresholds and approximate after. The error is very controllable and quantifiable. All Bullet queries that use Sketches return the error bounds with Standard Deviations as part of the results so you can quantify the error exactly. Using Sketches lets us address otherwise hard to solve problems in sub-linear space.  We also use Sketches as a way to control high cardinality grouping (group by a natural key column or related) and rely on the Sketching data structure to drop excess groups. It is up to you setting up Bullet to determine to set Sketch sizes large or small enough for to satisfy the queries that will be performed on that instance of Bullet.", 
            "title": "Approximate computation"
        }, 
        {
            "location": "/#new-query-types-coming-soon", 
            "text": "Using Sketches, we have implemented  COUNT DISTINCT  and  GROUP  and are working on other aggregations including but not limited to:     Aggregation  Meaning      TOP K  Returns the top K most frequently appearing values in the column    DISTRIBUTION  Computes distributions of the elements in the column. E.g. Find the median value or the 95th percentile of a field or graph the entire distribution as a histogram", 
            "title": "New query types coming soon"
        }, 
        {
            "location": "/#architecture", 
            "text": "", 
            "title": "Architecture"
        }, 
        {
            "location": "/#backend", 
            "text": "The Bullet backend can be split into three main sub-systems:   Request Processor - receives queries, adds metadata and sends it to the rest of the system  Data Processor - converts the data from an stream and matches it against queries  Combiner - combines results for different queries, performs final aggregations and returns results", 
            "title": "Backend"
        }, 
        {
            "location": "/#web-service-and-ui", 
            "text": "The rest of the pieces are just the standard other two pieces in a full-stack application:   A Web Service that talks to this backend  A UI that talks to this Web Service   The  Bullet Web Service  is built using  Jersey  and the  UI  is built in  Ember .  The Web Service can be deployed with your favorite servlet container like  Jetty . The UI is a client-side application that can be served using  Node.js  In the case of Bullet on Storm, the Web Service and UI talk to the backend using  Storm DRPC .", 
            "title": "Web Service and UI"
        }, 
        {
            "location": "/#end-to-end-architecture", 
            "text": "Want to know more?  In practice, the backend is implemented using the basic components that the Stream processing framework provides. See  Storm Architecture  for details.", 
            "title": "End-to-End Architecture"
        }, 
        {
            "location": "/#past-releases-and-source", 
            "text": "See the  Releases  section where the various Bullet releases and repository links are collected in one place.", 
            "title": "Past Releases and Source"
        }, 
        {
            "location": "/quick-start/", 
            "text": "Quick Start\n\n\nThis section gets you running a mock instance of Bullet to play around with. The instance will run using \nBullet on Storm\n. Since we do not have an actual data source, we will produce some fake data and convert it into \nBullet Records\n in a \ncustom Storm spout\n. If you want to use Bullet for your data, you will need to do read and convert your data to Bullet Records in a similar manner.\n\n\nBy the following the steps in this section, you will:\n\n\n\n\nSetup the Bullet topology using a custom spout on \nbullet-storm-0.3.1\n\n\nSetup the \nWeb Service\n talking to the topology and serving a schema for your UI using \nbullet-service-0.0.1\n\n\nSetup the \nUI\n talking to the Web Service using \nbullet-ui-0.1.0\n\n\n\n\nPrerequisites\n\n\n\n\nYou will need to be on an Unix-based system (Mac OS X, Ubuntu ...)\n\n\nYou will need \nJDK 8\n installed.\n\n\nYou will need enough CPU and RAM on your machine to run about 8-10 JVMs. You will be setting up a Storm cluster with multiple components, a couple of Jetty instances and a Node server\n\n\n\n\nQuicker Start\n\n\nDon't want to follow all these Steps? Make sure you have your prerequisites installed and you can just run:\n\n\nexport BULLET_HOME=$(pwd)/bullet-quickstart\ncurl -sLo- https://raw.githubusercontent.com/yahoo/bullet-docs/v0.1.2/examples/install-all.sh | bash\n\n\n\n\nThis will run all the Steps for you. Once everything has launched, you should be able to go to the Bullet UI running locally at \nhttp://localhost:8800\n. You can then continue this guide from \nhere\n.\n\n\nIf you want to manually run all the commands  or if something failed above (might want to perform the \nteardown\n first), you can continue below.\n\n\n\n\nSetting up Storm\n\n\nTo set up a clean working environment, let's start with creating some directories.\n\n\nStep 1: Setup directories and examples\n\n\nexport BULLET_HOME=$(pwd)/bullet-quickstart\nmkdir -p $BULLET_HOME/backend/storm\nmkdir -p $BULLET_HOME/service\nmkdir -p $BULLET_HOME/ui\ncd $BULLET_HOME\ncurl -LO https://github.com/yahoo/bullet-docs/releases/download/v0.1.2/examples_artifacts.tar.gz\ntar -xzf examples_artifacts.tar.gz\nexport BULLET_EXAMPLES=$BULLET_HOME/bullet-examples\n\n\n\n\nStep 2: Install Storm 1.0\n\n\ncd $BULLET_HOME/backend\ncurl -O http://apache.org/dist/storm/apache-storm-1.0.3/apache-storm-1.0.3.zip\nunzip apache-storm-1.0.3.zip\nexport PATH=$(pwd)/apache-storm-1.0.3/bin/:$PATH\n\n\n\n\nAdd a DRPC server setting to the Storm config:\n\n\necho 'drpc.servers: [\n127.0.0.1\n]' \n apache-storm-1.0.3/conf/storm.yaml\n\n\n\n\nStep 3: Launch Storm components\n\n\nLaunch each of the following components, in order and wait for the commands to go through. You may have to do these one at a time. You will see a JVM being launched for each one and connection messages as the components communicate through Zookeeper.\n\n\nstorm dev-zookeeper \n\nstorm nimbus \n\nstorm drpc \n\nstorm ui \n\nstorm logviewer \n\nstorm supervisor \n\n\n\n\n\nOnce everything is up without errors, visit \nhttp://localhost:8080\n and see if the Storm UI loads.\n\n\nStep 4: Test Storm (Optional)\n\n\nBefore Bullet, test to see if Storm and DRPC are up and running by launching a example topology that comes with your Storm installation:\n\n\nstorm jar apache-storm-1.0.3/examples/storm-starter/storm-starter-topologies-1.0.3.jar org.apache.storm.starter.BasicDRPCTopology topology\n\n\n\n\nVisit your UI with a browser and see if a topology with name \"topology\" is running. If everything is good, you should be able to ping DRPC with:\n\n\ncurl localhost:3774/drpc/exclamation/foo\n\n\n\n\nand get back a \nfoo!\n. Any string you pass as part of the URL is returned to you with a \"!\" at the end.\n\n\nKill this topology after with:\n\n\nstorm kill topology\n\n\n\n\n\n\nLocal mode cleanup\n\n\nIf you notice any problems while setting up storm or while relaunching a topology, it may be because some state is corrupted. When running Storm in this fashion, states and serializations are stored in \nstorm-local\n and \n/tmp/\n. You may want to \nrm -rf storm-local/* /tmp/dev-storm-zookeeper\n to clean up this state before relaunching Storm components. See the \ntear down section\n on how to kill any running instances.\n\n\n\n\nSetting up the example Bullet topology\n\n\nNow that Storm is up and running, we can put Bullet on it. We will use an example Spout that runs on Bullet 0.3.1 on our Storm cluster. The source is available \nhere\n. This was part of the artifact that you installed in Step 1.\n\n\nStep 5: Setup the Storm example\n\n\ncp $BULLET_EXAMPLES/storm/* $BULLET_HOME/backend/storm\n\n\n\n\n\n\nSettings\n\n\nTake a look at bullet_settings.yaml for the settings that are being overridden for this example. You can add or change settings as you like by referring to \nbullet_defaults.yaml\n. In particular, we have \ncustomized these settings\n that affect the Bullet queries you can run:\n\n\nbullet.rule.max.duration: 570000\n Longest query time can be 570s. The Storm cluster default DRPC timeout is 600s.\n\n\nbullet.rule.aggregation.raw.max.size: 500\n The max \nRAW\n records you can fetch is 500.\n\n\nbullet.rule.aggregation.max.size: 1024\n The max records you can fetch for any query is 1024.\n\n\nbullet.rule.aggregation.count.distinct.sketch.entries: 16384\n We can count 16384 unique values exactly. Approximates after.\n\n\nbullet.rule.aggregation.group.sketch.entries: 1024\n The max unique groups can be 1024. Uniform sample after.\n\n\n\n\n\n\nWant to tweak the example topology code?\n\n\nYou will need to clone the \nexamples repository\n and customize it. To build the examples, you'll need to install \nMaven 3\n.\n\n\ncd $BULLET_HOME \n git clone git@github.com:yahoo/bullet-docs.git\n\n\ncd bullet-docs/examples/storm \n mvn package\n\n\nYou will find the \nbullet-storm-example-1.0-SNAPSHOT-jar-with-dependencies.jar\n in \n$BULLET_HOME/bullet-docs/examples/storm/target/\n\n\n\n\nStep 6: Launch the topology\n\n\ncd $BULLET_HOME/backend/storm \n ./launch.sh\n\n\n\n\nThis script also kills any existing Bullet instances running (you may see an ignorable exception if there is nothing running). There can only be one topology in the cluster with a particular name. Visit the UI and see if the topology is up. You should see the \nDataSource\n spout begin emitting records.\n\n\nTest the Bullet topology by:\n\n\ncurl -s -X POST -d '{}' http://localhost:3774/drpc/bullet\n\n\n\n\nYou should get a random record from Bullet.\n\n\n\n\nWhat is this data?\n\n\nThis data is randomly generated by the \ncustom Storm spout\n that is in the example topology you just launched. In practice, your spout would read from an actual data source such as Kafka instead. See \nbelow\n for more details about this random data spout.\n\n\n\n\nSetting up the Bullet Web Service\n\n\nStep 7: Install Jetty\n\n\ncd $BULLET_HOME/service\ncurl -O http://central.maven.org/maven2/org/eclipse/jetty/jetty-distribution/9.3.16.v20170120/jetty-distribution-9.3.16.v20170120.zip\nunzip jetty-distribution-9.3.16.v20170120.zip\n\n\n\n\nStep 8: Install the Bullet Web Service\n\n\ncd jetty-distribution-9.3.16.v20170120\ncurl -Lo webapps/bullet-service.war http://jcenter.bintray.com/com/yahoo/bullet/bullet-service/0.0.1/bullet-service-0.0.1.war\ncp $BULLET_EXAMPLES/web-service/example_* $BULLET_HOME/service/jetty-distribution-9.3.16.v20170120\n\n\n\n\nStep 9: Launch the Web Service\n\n\ncd $BULLET_HOME/service/jetty-distribution-9.3.16.v20170120\njava -jar -Dbullet.service.configuration.file=\nexample_context.properties\n -Djetty.http.port=9999 start.jar \n logs/out 2\n1 \n\n\n\n\n\nYou can verify that it is up by running the Bullet query and getting the example columns through the API:\n\n\ncurl -s -X POST -d '{}' http://localhost:9999/bullet-service/api/drpc\ncurl -s http://localhost:9999/bullet-service/api/columns\n\n\n\n\nSetting up the Bullet UI\n\n\nStep 10: Install Node\n\n\ncurl -s https://raw.githubusercontent.com/creationix/nvm/v0.33.1/install.sh | bash\nsource ~/.bashrc\nnvm install v6.9.4\nnvm use v6.9.4\n\n\n\n\nStep 11: Install the Bullet UI\n\n\ncd $BULLET_HOME/ui\ncurl -LO https://github.com/yahoo/bullet-ui/releases/download/v0.1.0/bullet-ui-v0.1.0.tar.gz\ntar -xzf bullet-ui-v0.1.0.tar.gz\ncp $BULLET_EXAMPLES/ui/env-settings.json config/\n\n\n\n\nStep 12: Launch the UI\n\n\nPORT=8800 node express-server.js \n\n\n\n\n\nVisit \nhttp://localhost:8800\n to query your topology with the UI. See \nUI usage\n for some example queries and interactions using this UI. You see what the Schema means by visiting the Schema section.\n\n\n\n\nRunning it remotely?\n\n\nIf you access the UI from another machine than where your UI is actually running, you will need to edit \nconfig/env-settings.json\n. Since the UI is a client-side app, the machine that your browser is running on will fetch the UI and attempt to use these settings to talk to the Web Service. Since they point to localhost by default, your browser will attempt to connect there and fail. An easy fix is to change \nlocalhost\n in your env-settings.json to point to the host name where you will hosting the UI. This will be the same as the UI host you use in the browser.\n\n\n\n\nTeardown\n\n\nTo cleanup all the components we bought up:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUI\n\n\nps aux | grep [e]xpress-server.js | awk '{print $2}' | xargs kill\n\n\n\n\n\n\nWeb Service\n\n\nps aux | grep [e]xample_context.properties | awk '{print $2}' | xargs kill\n\n\n\n\n\n\nStorm\n\n\nps aux | grep [a]pache-storm-1.0.3 | awk '{print $2}' | xargs kill\n\n\n\n\n\n\nFile System\n\n\nrm -rf $BULLET_HOME /tmp/dev-storm-zookeeper /tmp/jetty-*\n\n\n\n\n\n\n\n\nYou can also do:\n\n\ncurl -sLo- https://raw.githubusercontent.com/yahoo/bullet-docs/v0.1.2/examples/install-all.sh | bash -s cleanup\n\n\n\n\nWhat did we do?\n\n\nThis section will go over the various custom pieces this example plugged into Bullet, so you can better understand what we did.\n\n\nStorm topology\n\n\nThe topology was the Bullet topology plugged in with a custom spout. This spout is implemented in this \nexample project\n and was already built for you when you \ndownloaded the examples\n. It does not read from any data source and just produces random, structured data. It also produces only up to a maximum number of records in a given period. Both this maximum and the length of a period are configurable. If you examine $BULLET_HOME/backend/storm/launch.sh, you'll see the following:\n\n\nstorm jar bullet-storm-example-1.0-SNAPSHOT-jar-with-dependencies.jar \\\n          com.yahoo.bullet.Topology \\\n          --bullet-conf bullet_settings.yaml \\\n          --bullet-spout com.yahoo.bullet.storm.examples.RandomSpout \\\n          --bullet-spout-parallelism 1 \\\n          ...\n          --bullet-spout-arg 20 \\\n          --bullet-spout-arg 101 \\\n          ...\n\n\n\n\nThis command launches the jar (an uber or \"fat\" jar) containing the custom spout code and all dependencies you copied in Step 5. We pass the name of your spout class with \n--bullet-spout com.yahoo.bullet.storm.examples.RandomSpout\n to the Bullet main class \ncom.yahoo.bullet.Topology\n with two arguments \n--bullet-spout-arg 20\n and \n--bullet-spout-arg 101\n. The first argument tells the Spout to generate at most 20 tuples (records) in a period and the second argument says a period is 101 ms long.\n\n\nThe settings defined by \n--bullet-conf bullet_settings.yaml\n and the arguments here run all components in the topology with a parallelism of 1. So there will be one spout that is producing ~200 rps.\n\n\n\n\nI thought you said hundreds of thousands of records...\n\n\n200 records is not Big Data by any stretch of the imagination but this Quick Start is running everything on one machine and is meant to introduce you to what Bullet does. In practice, you would scale and run your components with CPU and memory configurations to accommodate for your data volume and querying needs.\n\n\n\n\nLet's look at the \ncustom spout code\n that generates the data.\n\n\n    @Override\n    public void nextTuple() {\n        long timeNow = System.nanoTime();\n        // Only emit if we are still in the interval and haven't gone over our per period max\n        if (timeNow \n= nextIntervalStart \n generatedThisPeriod \n maxPerPeriod) {\n            outputCollector.emit(new Values(generateRecord()), DUMMY_ID);\n            generatedThisPeriod++;\n        }\n        if (timeNow \n nextIntervalStart) {\n            log.info(\nGenerated {} tuples out of {}\n, generatedThisPeriod, maxPerPeriod);\n            nextIntervalStart = timeNow + period;\n            generatedThisPeriod = 0;\n            periodCount++;\n        }\n        // It is courteous to sleep for a short time if you're not emitting anything...\n        try {\n            Thread.sleep(1);\n        } catch (InterruptedException e) {\n            log.error(\nError: \n, e);\n        }\n    }\n\n\n\n\nThis method above emits the tuples. The Storm framework calls this method. This function only emits at most the given maximum tuples per period.\n\n\n\n\nWhy a DUMMY_ID?\n\n\nWhen the spout emits the randomly generated tuple, it attaches a \nDUMMY_ID\n to it. In Storm terms, this is a message ID. By adding a message ID, this tuple can be made to flow reliably. The Bullet component that receives this tuple (Filter bolt) acknowledges or \"acks\" this tuple. If the tuple did not make it to Filter bolt within a configured timeout window, Storm will call a \nfail(Object messageId)\n method on the spout. This particular spout does not define one and hence the usage of a \nDUMMY_ID\n. If your source of data can identify records uniquely and you can re-emit them on a fail, you should attach that actual ID in place of the \nDUMMY_ID\n.\n\n\n\n\n    private BulletRecord generateRecord() {\n        BulletRecord record = new BulletRecord();\n        String uuid = UUID.randomUUID().toString();\n\n        record.setString(STRING, uuid);\n        record.setLong(LONG, (long) generatedThisPeriod);\n        record.setDouble(DOUBLE, random.nextDouble());\n\n        Map\nString, Boolean\n booleanMap = new HashMap\n(4);\n        booleanMap.put(uuid.substring(0, 8), random.nextBoolean());\n        booleanMap.put(uuid.substring(9, 13), random.nextBoolean());\n        booleanMap.put(uuid.substring(14, 18), random.nextBoolean());\n        booleanMap.put(uuid.substring(19, 23), random.nextBoolean());\n        record.setBooleanMap(BOOLEAN_MAP, booleanMap);\n\n        Map\nString, Long\n statsMap = new HashMap\n(4);\n        statsMap.put(PERIOD_COUNT, periodCount);\n        statsMap.put(RECORD_NUMBER, periodCount * maxPerPeriod + generatedThisPeriod);\n        statsMap.put(NANO_TIME, System.nanoTime());\n        statsMap.put(TIMESTAMP, System.currentTimeMillis());\n        record.setLongMap(STATS_MAP, statsMap);\n\n        Map\nString, String\n randomMapA = new HashMap\n(2);\n        Map\nString, String\n randomMapB = new HashMap\n(2);\n        randomMapA.put(RANDOM_MAP_KEY_A, STRING_POOL[random.nextInt(STRING_POOL.length)]);\n        randomMapA.put(RANDOM_MAP_KEY_B, STRING_POOL[random.nextInt(STRING_POOL.length)]);\n        randomMapB.put(RANDOM_MAP_KEY_A, STRING_POOL[random.nextInt(STRING_POOL.length)]);\n        randomMapB.put(RANDOM_MAP_KEY_B, STRING_POOL[random.nextInt(STRING_POOL.length)]);\n        record.setListOfStringMap(LIST, asList(randomMapA, randomMapB));\n\n        return record;\n    }\n\n\n\n\nThis method generates some fields randomly and inserts them into a BulletRecord. Note that the BulletRecord is typed and all data must be inserted with the proper types.\n\n\nIf you put Bullet on your data, you will need to write a Spout (or a topology if your reading is complex), that reads from your data source and emits BulletRecords with the fields you wish to be query-able placed into a BulletRecord similar to this example.\n\n\nWeb Service\n\n\nWe launched the Web Service using two custom files - a properties file and JSON schema file.\n\n\nThe JSON columns file contains the schema for our data specified in JSON. Since our schema is not going to change, we use the Web Service to serve it from a file. If your schema changes dynamically, you will need to provide your own endpoint to the UI.\n\n\nThe following is a snippet from the \nJSON file\n. Notice how the types of the fields are specified. Also, if you have generated BulletRecord with Map fields whose keys are known, you can specify them here using \nenumerations\n.\n\n\n[\n    {\n        \nname\n: \nprobability\n,\n        \ntype\n: \nDOUBLE\n,\n        \ndescription\n: \nGenerated from Random#nextDouble\n\n    },\n    ...\n    {\n        \nname\n: \nstats_map\n,\n        \ntype\n: \nMAP\n,\n        \nsubtype\n: \nLONG\n,\n        \ndescription\n: \nThis map contains some numeric information such as the current number of periods etc.\n,\n        \nenumerations\n: [\n            ...\n            {\nname\n: \nnano_time\n, \ndescription\n: \nThe ns time when this record was generated\n}\n        ]\n    },\n    {\n        \nname\n: \nclassifiers\n,\n        \ntype\n: \nLIST\n,\n        \nsubtype\n: \nMAP\n,\n        \ndescription\n: \nThis contains two maps, each with: field_A and field_B whose values are randomly chosen from: foo, bar, baz, qux, quux, norf\n\n    }\n]\n\n\n\n\nThe \nexample properties file\n points to the DRPC host, port, and path as well points to the custom columns file.\n\n\ndrpc.servers=localhost\ndrpc.port=3774\ndrpc.path=drpc/bullet\ndrpc.retry.limit=3\ndrpc.connect.timeout=1000\ncolumns.file=example_columns.json\ncolumns.schema.version=1.0\n\n\n\n\ndrpc.servers\n is a CSV entry that contains the various DRPC servers in your Storm cluster. If you \nvisit the Storm UI\n and search in the \nNimbus Configuration\n section, you can find the list of DRPC servers for your cluster. Similarly, \ndrpc.port\n in the properties file is \ndrpc.http.port\n in \nNimbus Configuration\n. The \ndrpc.path\n is the constant string \ndrpc/\n followed by the value of the \ntopology.function\n setting in bullet_settings.yaml.\n\n\nUI\n\n\nFinally, we configured the UI with the custom environment specific settings file. We did not add any environments since we only had the one.\n\n\n{\n  \ndefault\n: {\n    \ndrpcHost\n: \nhttp://localhost:9999\n,\n    \ndrpcNamespace\n: \nbullet-service/api\n,\n    \ndrpcPath\n: \ndrpc\n,\n    \nschemaHost\n: \nhttp://localhost:9999\n,\n    \nschemaNamespace\n: \nbullet-service/api\n,\n    \nhelpLinks\n: [\n      {\n        \nname\n: \nExample Docs Page\n,\n        \nlink\n: \n\n      }\n    ],\n    \nbugLink\n: \nhttps://github.com/yahoo/bullet-ui/issues\n,\n    \naggregateDataDefaultSize\n: 1024,\n    \nmodelVersion\n: 1\n  }\n}\n\n\n\n\nSince we served our schema through the same Web Service as our queries, both these point to our Web Service. Note that there is no \nschemaPath\n because it must be the constant string \ncolumns\n. If you define a custom endpoint for your schema, you must ensure that it can be obtained by making a GET request to \nschemaHost/schemaNamespace/columns\n.\n\n\nThe \nUI Usage\n page shows you some queries you can run using one such instance of the UI.", 
            "title": "Quick Start"
        }, 
        {
            "location": "/quick-start/#quick-start", 
            "text": "This section gets you running a mock instance of Bullet to play around with. The instance will run using  Bullet on Storm . Since we do not have an actual data source, we will produce some fake data and convert it into  Bullet Records  in a  custom Storm spout . If you want to use Bullet for your data, you will need to do read and convert your data to Bullet Records in a similar manner.  By the following the steps in this section, you will:   Setup the Bullet topology using a custom spout on  bullet-storm-0.3.1  Setup the  Web Service  talking to the topology and serving a schema for your UI using  bullet-service-0.0.1  Setup the  UI  talking to the Web Service using  bullet-ui-0.1.0   Prerequisites   You will need to be on an Unix-based system (Mac OS X, Ubuntu ...)  You will need  JDK 8  installed.  You will need enough CPU and RAM on your machine to run about 8-10 JVMs. You will be setting up a Storm cluster with multiple components, a couple of Jetty instances and a Node server", 
            "title": "Quick Start"
        }, 
        {
            "location": "/quick-start/#quicker-start", 
            "text": "Don't want to follow all these Steps? Make sure you have your prerequisites installed and you can just run:  export BULLET_HOME=$(pwd)/bullet-quickstart\ncurl -sLo- https://raw.githubusercontent.com/yahoo/bullet-docs/v0.1.2/examples/install-all.sh | bash  This will run all the Steps for you. Once everything has launched, you should be able to go to the Bullet UI running locally at  http://localhost:8800 . You can then continue this guide from  here .  If you want to manually run all the commands  or if something failed above (might want to perform the  teardown  first), you can continue below.", 
            "title": "Quicker Start"
        }, 
        {
            "location": "/quick-start/#setting-up-storm", 
            "text": "To set up a clean working environment, let's start with creating some directories.", 
            "title": "Setting up Storm"
        }, 
        {
            "location": "/quick-start/#step-1-setup-directories-and-examples", 
            "text": "export BULLET_HOME=$(pwd)/bullet-quickstart\nmkdir -p $BULLET_HOME/backend/storm\nmkdir -p $BULLET_HOME/service\nmkdir -p $BULLET_HOME/ui\ncd $BULLET_HOME\ncurl -LO https://github.com/yahoo/bullet-docs/releases/download/v0.1.2/examples_artifacts.tar.gz\ntar -xzf examples_artifacts.tar.gz\nexport BULLET_EXAMPLES=$BULLET_HOME/bullet-examples", 
            "title": "Step 1: Setup directories and examples"
        }, 
        {
            "location": "/quick-start/#step-2-install-storm-10", 
            "text": "cd $BULLET_HOME/backend\ncurl -O http://apache.org/dist/storm/apache-storm-1.0.3/apache-storm-1.0.3.zip\nunzip apache-storm-1.0.3.zip\nexport PATH=$(pwd)/apache-storm-1.0.3/bin/:$PATH  Add a DRPC server setting to the Storm config:  echo 'drpc.servers: [ 127.0.0.1 ]'   apache-storm-1.0.3/conf/storm.yaml", 
            "title": "Step 2: Install Storm 1.0"
        }, 
        {
            "location": "/quick-start/#step-3-launch-storm-components", 
            "text": "Launch each of the following components, in order and wait for the commands to go through. You may have to do these one at a time. You will see a JVM being launched for each one and connection messages as the components communicate through Zookeeper.  storm dev-zookeeper  \nstorm nimbus  \nstorm drpc  \nstorm ui  \nstorm logviewer  \nstorm supervisor    Once everything is up without errors, visit  http://localhost:8080  and see if the Storm UI loads.", 
            "title": "Step 3: Launch Storm components"
        }, 
        {
            "location": "/quick-start/#step-4-test-storm-optional", 
            "text": "Before Bullet, test to see if Storm and DRPC are up and running by launching a example topology that comes with your Storm installation:  storm jar apache-storm-1.0.3/examples/storm-starter/storm-starter-topologies-1.0.3.jar org.apache.storm.starter.BasicDRPCTopology topology  Visit your UI with a browser and see if a topology with name \"topology\" is running. If everything is good, you should be able to ping DRPC with:  curl localhost:3774/drpc/exclamation/foo  and get back a  foo! . Any string you pass as part of the URL is returned to you with a \"!\" at the end.  Kill this topology after with:  storm kill topology   Local mode cleanup  If you notice any problems while setting up storm or while relaunching a topology, it may be because some state is corrupted. When running Storm in this fashion, states and serializations are stored in  storm-local  and  /tmp/ . You may want to  rm -rf storm-local/* /tmp/dev-storm-zookeeper  to clean up this state before relaunching Storm components. See the  tear down section  on how to kill any running instances.", 
            "title": "Step 4: Test Storm (Optional)"
        }, 
        {
            "location": "/quick-start/#setting-up-the-example-bullet-topology", 
            "text": "Now that Storm is up and running, we can put Bullet on it. We will use an example Spout that runs on Bullet 0.3.1 on our Storm cluster. The source is available  here . This was part of the artifact that you installed in Step 1.", 
            "title": "Setting up the example Bullet topology"
        }, 
        {
            "location": "/quick-start/#step-5-setup-the-storm-example", 
            "text": "cp $BULLET_EXAMPLES/storm/* $BULLET_HOME/backend/storm   Settings  Take a look at bullet_settings.yaml for the settings that are being overridden for this example. You can add or change settings as you like by referring to  bullet_defaults.yaml . In particular, we have  customized these settings  that affect the Bullet queries you can run:  bullet.rule.max.duration: 570000  Longest query time can be 570s. The Storm cluster default DRPC timeout is 600s.  bullet.rule.aggregation.raw.max.size: 500  The max  RAW  records you can fetch is 500.  bullet.rule.aggregation.max.size: 1024  The max records you can fetch for any query is 1024.  bullet.rule.aggregation.count.distinct.sketch.entries: 16384  We can count 16384 unique values exactly. Approximates after.  bullet.rule.aggregation.group.sketch.entries: 1024  The max unique groups can be 1024. Uniform sample after.    Want to tweak the example topology code?  You will need to clone the  examples repository  and customize it. To build the examples, you'll need to install  Maven 3 .  cd $BULLET_HOME   git clone git@github.com:yahoo/bullet-docs.git  cd bullet-docs/examples/storm   mvn package  You will find the  bullet-storm-example-1.0-SNAPSHOT-jar-with-dependencies.jar  in  $BULLET_HOME/bullet-docs/examples/storm/target/", 
            "title": "Step 5: Setup the Storm example"
        }, 
        {
            "location": "/quick-start/#step-6-launch-the-topology", 
            "text": "cd $BULLET_HOME/backend/storm   ./launch.sh  This script also kills any existing Bullet instances running (you may see an ignorable exception if there is nothing running). There can only be one topology in the cluster with a particular name. Visit the UI and see if the topology is up. You should see the  DataSource  spout begin emitting records.  Test the Bullet topology by:  curl -s -X POST -d '{}' http://localhost:3774/drpc/bullet  You should get a random record from Bullet.   What is this data?  This data is randomly generated by the  custom Storm spout  that is in the example topology you just launched. In practice, your spout would read from an actual data source such as Kafka instead. See  below  for more details about this random data spout.", 
            "title": "Step 6: Launch the topology"
        }, 
        {
            "location": "/quick-start/#setting-up-the-bullet-web-service", 
            "text": "", 
            "title": "Setting up the Bullet Web Service"
        }, 
        {
            "location": "/quick-start/#step-7-install-jetty", 
            "text": "cd $BULLET_HOME/service\ncurl -O http://central.maven.org/maven2/org/eclipse/jetty/jetty-distribution/9.3.16.v20170120/jetty-distribution-9.3.16.v20170120.zip\nunzip jetty-distribution-9.3.16.v20170120.zip", 
            "title": "Step 7: Install Jetty"
        }, 
        {
            "location": "/quick-start/#step-8-install-the-bullet-web-service", 
            "text": "cd jetty-distribution-9.3.16.v20170120\ncurl -Lo webapps/bullet-service.war http://jcenter.bintray.com/com/yahoo/bullet/bullet-service/0.0.1/bullet-service-0.0.1.war\ncp $BULLET_EXAMPLES/web-service/example_* $BULLET_HOME/service/jetty-distribution-9.3.16.v20170120", 
            "title": "Step 8: Install the Bullet Web Service"
        }, 
        {
            "location": "/quick-start/#step-9-launch-the-web-service", 
            "text": "cd $BULLET_HOME/service/jetty-distribution-9.3.16.v20170120\njava -jar -Dbullet.service.configuration.file= example_context.properties  -Djetty.http.port=9999 start.jar   logs/out 2 1    You can verify that it is up by running the Bullet query and getting the example columns through the API:  curl -s -X POST -d '{}' http://localhost:9999/bullet-service/api/drpc\ncurl -s http://localhost:9999/bullet-service/api/columns", 
            "title": "Step 9: Launch the Web Service"
        }, 
        {
            "location": "/quick-start/#setting-up-the-bullet-ui", 
            "text": "", 
            "title": "Setting up the Bullet UI"
        }, 
        {
            "location": "/quick-start/#step-10-install-node", 
            "text": "curl -s https://raw.githubusercontent.com/creationix/nvm/v0.33.1/install.sh | bash\nsource ~/.bashrc\nnvm install v6.9.4\nnvm use v6.9.4", 
            "title": "Step 10: Install Node"
        }, 
        {
            "location": "/quick-start/#step-11-install-the-bullet-ui", 
            "text": "cd $BULLET_HOME/ui\ncurl -LO https://github.com/yahoo/bullet-ui/releases/download/v0.1.0/bullet-ui-v0.1.0.tar.gz\ntar -xzf bullet-ui-v0.1.0.tar.gz\ncp $BULLET_EXAMPLES/ui/env-settings.json config/", 
            "title": "Step 11: Install the Bullet UI"
        }, 
        {
            "location": "/quick-start/#step-12-launch-the-ui", 
            "text": "PORT=8800 node express-server.js    Visit  http://localhost:8800  to query your topology with the UI. See  UI usage  for some example queries and interactions using this UI. You see what the Schema means by visiting the Schema section.   Running it remotely?  If you access the UI from another machine than where your UI is actually running, you will need to edit  config/env-settings.json . Since the UI is a client-side app, the machine that your browser is running on will fetch the UI and attempt to use these settings to talk to the Web Service. Since they point to localhost by default, your browser will attempt to connect there and fail. An easy fix is to change  localhost  in your env-settings.json to point to the host name where you will hosting the UI. This will be the same as the UI host you use in the browser.", 
            "title": "Step 12: Launch the UI"
        }, 
        {
            "location": "/quick-start/#teardown", 
            "text": "To cleanup all the components we bought up:           UI  ps aux | grep [e]xpress-server.js | awk '{print $2}' | xargs kill    Web Service  ps aux | grep [e]xample_context.properties | awk '{print $2}' | xargs kill    Storm  ps aux | grep [a]pache-storm-1.0.3 | awk '{print $2}' | xargs kill    File System  rm -rf $BULLET_HOME /tmp/dev-storm-zookeeper /tmp/jetty-*     You can also do:  curl -sLo- https://raw.githubusercontent.com/yahoo/bullet-docs/v0.1.2/examples/install-all.sh | bash -s cleanup", 
            "title": "Teardown"
        }, 
        {
            "location": "/quick-start/#what-did-we-do", 
            "text": "This section will go over the various custom pieces this example plugged into Bullet, so you can better understand what we did.", 
            "title": "What did we do?"
        }, 
        {
            "location": "/quick-start/#storm-topology", 
            "text": "The topology was the Bullet topology plugged in with a custom spout. This spout is implemented in this  example project  and was already built for you when you  downloaded the examples . It does not read from any data source and just produces random, structured data. It also produces only up to a maximum number of records in a given period. Both this maximum and the length of a period are configurable. If you examine $BULLET_HOME/backend/storm/launch.sh, you'll see the following:  storm jar bullet-storm-example-1.0-SNAPSHOT-jar-with-dependencies.jar \\\n          com.yahoo.bullet.Topology \\\n          --bullet-conf bullet_settings.yaml \\\n          --bullet-spout com.yahoo.bullet.storm.examples.RandomSpout \\\n          --bullet-spout-parallelism 1 \\\n          ...\n          --bullet-spout-arg 20 \\\n          --bullet-spout-arg 101 \\\n          ...  This command launches the jar (an uber or \"fat\" jar) containing the custom spout code and all dependencies you copied in Step 5. We pass the name of your spout class with  --bullet-spout com.yahoo.bullet.storm.examples.RandomSpout  to the Bullet main class  com.yahoo.bullet.Topology  with two arguments  --bullet-spout-arg 20  and  --bullet-spout-arg 101 . The first argument tells the Spout to generate at most 20 tuples (records) in a period and the second argument says a period is 101 ms long.  The settings defined by  --bullet-conf bullet_settings.yaml  and the arguments here run all components in the topology with a parallelism of 1. So there will be one spout that is producing ~200 rps.   I thought you said hundreds of thousands of records...  200 records is not Big Data by any stretch of the imagination but this Quick Start is running everything on one machine and is meant to introduce you to what Bullet does. In practice, you would scale and run your components with CPU and memory configurations to accommodate for your data volume and querying needs.   Let's look at the  custom spout code  that generates the data.      @Override\n    public void nextTuple() {\n        long timeNow = System.nanoTime();\n        // Only emit if we are still in the interval and haven't gone over our per period max\n        if (timeNow  = nextIntervalStart   generatedThisPeriod   maxPerPeriod) {\n            outputCollector.emit(new Values(generateRecord()), DUMMY_ID);\n            generatedThisPeriod++;\n        }\n        if (timeNow   nextIntervalStart) {\n            log.info( Generated {} tuples out of {} , generatedThisPeriod, maxPerPeriod);\n            nextIntervalStart = timeNow + period;\n            generatedThisPeriod = 0;\n            periodCount++;\n        }\n        // It is courteous to sleep for a short time if you're not emitting anything...\n        try {\n            Thread.sleep(1);\n        } catch (InterruptedException e) {\n            log.error( Error:  , e);\n        }\n    }  This method above emits the tuples. The Storm framework calls this method. This function only emits at most the given maximum tuples per period.   Why a DUMMY_ID?  When the spout emits the randomly generated tuple, it attaches a  DUMMY_ID  to it. In Storm terms, this is a message ID. By adding a message ID, this tuple can be made to flow reliably. The Bullet component that receives this tuple (Filter bolt) acknowledges or \"acks\" this tuple. If the tuple did not make it to Filter bolt within a configured timeout window, Storm will call a  fail(Object messageId)  method on the spout. This particular spout does not define one and hence the usage of a  DUMMY_ID . If your source of data can identify records uniquely and you can re-emit them on a fail, you should attach that actual ID in place of the  DUMMY_ID .       private BulletRecord generateRecord() {\n        BulletRecord record = new BulletRecord();\n        String uuid = UUID.randomUUID().toString();\n\n        record.setString(STRING, uuid);\n        record.setLong(LONG, (long) generatedThisPeriod);\n        record.setDouble(DOUBLE, random.nextDouble());\n\n        Map String, Boolean  booleanMap = new HashMap (4);\n        booleanMap.put(uuid.substring(0, 8), random.nextBoolean());\n        booleanMap.put(uuid.substring(9, 13), random.nextBoolean());\n        booleanMap.put(uuid.substring(14, 18), random.nextBoolean());\n        booleanMap.put(uuid.substring(19, 23), random.nextBoolean());\n        record.setBooleanMap(BOOLEAN_MAP, booleanMap);\n\n        Map String, Long  statsMap = new HashMap (4);\n        statsMap.put(PERIOD_COUNT, periodCount);\n        statsMap.put(RECORD_NUMBER, periodCount * maxPerPeriod + generatedThisPeriod);\n        statsMap.put(NANO_TIME, System.nanoTime());\n        statsMap.put(TIMESTAMP, System.currentTimeMillis());\n        record.setLongMap(STATS_MAP, statsMap);\n\n        Map String, String  randomMapA = new HashMap (2);\n        Map String, String  randomMapB = new HashMap (2);\n        randomMapA.put(RANDOM_MAP_KEY_A, STRING_POOL[random.nextInt(STRING_POOL.length)]);\n        randomMapA.put(RANDOM_MAP_KEY_B, STRING_POOL[random.nextInt(STRING_POOL.length)]);\n        randomMapB.put(RANDOM_MAP_KEY_A, STRING_POOL[random.nextInt(STRING_POOL.length)]);\n        randomMapB.put(RANDOM_MAP_KEY_B, STRING_POOL[random.nextInt(STRING_POOL.length)]);\n        record.setListOfStringMap(LIST, asList(randomMapA, randomMapB));\n\n        return record;\n    }  This method generates some fields randomly and inserts them into a BulletRecord. Note that the BulletRecord is typed and all data must be inserted with the proper types.  If you put Bullet on your data, you will need to write a Spout (or a topology if your reading is complex), that reads from your data source and emits BulletRecords with the fields you wish to be query-able placed into a BulletRecord similar to this example.", 
            "title": "Storm topology"
        }, 
        {
            "location": "/quick-start/#web-service", 
            "text": "We launched the Web Service using two custom files - a properties file and JSON schema file.  The JSON columns file contains the schema for our data specified in JSON. Since our schema is not going to change, we use the Web Service to serve it from a file. If your schema changes dynamically, you will need to provide your own endpoint to the UI.  The following is a snippet from the  JSON file . Notice how the types of the fields are specified. Also, if you have generated BulletRecord with Map fields whose keys are known, you can specify them here using  enumerations .  [\n    {\n         name :  probability ,\n         type :  DOUBLE ,\n         description :  Generated from Random#nextDouble \n    },\n    ...\n    {\n         name :  stats_map ,\n         type :  MAP ,\n         subtype :  LONG ,\n         description :  This map contains some numeric information such as the current number of periods etc. ,\n         enumerations : [\n            ...\n            { name :  nano_time ,  description :  The ns time when this record was generated }\n        ]\n    },\n    {\n         name :  classifiers ,\n         type :  LIST ,\n         subtype :  MAP ,\n         description :  This contains two maps, each with: field_A and field_B whose values are randomly chosen from: foo, bar, baz, qux, quux, norf \n    }\n]  The  example properties file  points to the DRPC host, port, and path as well points to the custom columns file.  drpc.servers=localhost\ndrpc.port=3774\ndrpc.path=drpc/bullet\ndrpc.retry.limit=3\ndrpc.connect.timeout=1000\ncolumns.file=example_columns.json\ncolumns.schema.version=1.0  drpc.servers  is a CSV entry that contains the various DRPC servers in your Storm cluster. If you  visit the Storm UI  and search in the  Nimbus Configuration  section, you can find the list of DRPC servers for your cluster. Similarly,  drpc.port  in the properties file is  drpc.http.port  in  Nimbus Configuration . The  drpc.path  is the constant string  drpc/  followed by the value of the  topology.function  setting in bullet_settings.yaml.", 
            "title": "Web Service"
        }, 
        {
            "location": "/quick-start/#ui", 
            "text": "Finally, we configured the UI with the custom environment specific settings file. We did not add any environments since we only had the one.  {\n   default : {\n     drpcHost :  http://localhost:9999 ,\n     drpcNamespace :  bullet-service/api ,\n     drpcPath :  drpc ,\n     schemaHost :  http://localhost:9999 ,\n     schemaNamespace :  bullet-service/api ,\n     helpLinks : [\n      {\n         name :  Example Docs Page ,\n         link :  \n      }\n    ],\n     bugLink :  https://github.com/yahoo/bullet-ui/issues ,\n     aggregateDataDefaultSize : 1024,\n     modelVersion : 1\n  }\n}  Since we served our schema through the same Web Service as our queries, both these point to our Web Service. Note that there is no  schemaPath  because it must be the constant string  columns . If you define a custom endpoint for your schema, you must ensure that it can be obtained by making a GET request to  schemaHost/schemaNamespace/columns .  The  UI Usage  page shows you some queries you can run using one such instance of the UI.", 
            "title": "UI"
        }, 
        {
            "location": "/backend/storm-architecture/", 
            "text": "Storm architecture\n\n\nThis section describes how the \nBackend architecture\n is implemented in Storm.\n\n\nStorm DRPC\n\n\nBullet on \nStorm\n is built using \nStorm DRPC\n. DRPC or Distributed Remote Procedure Call, is built into Storm and consists of a set of servers that are part of the Storm cluster. When a Storm topology that uses DRPC is launched, it registers a spout with a unique name (the procedure in the Distributed Remote Procedure Call) with the DRPC infrastructure. The DRPC Servers expose a REST endpoint where data can be POSTed to or a GET request can be made with this unique name. The DRPC infrastructure then sends the request (a query in Bullet) through the spout(s) to the topology that registered that name (Bullet). The result from topology is sent back to the client. We picked Storm to implement Bullet on first not only because it was the most popular Streaming framework at Yahoo but also since DRPC provides us a nice and simple way to handle getting queries into Bullet and sending responses back.\n\n\n\n\nThrift and DRPC servers\n\n\nDRPC also exposes a \nThrift\n endpoint but the Bullet Web Service uses REST for simplicity. When you launch your Bullet Storm topology, you can POST Bullet queries to a DRPC server directly with the function name that you specified in the Bullet configuration. This is a quick way to check if your topology is up and running!\n\n\n\n\nTopology\n\n\nFor Bullet on Storm, the Storm topology implements the backend piece from the full \nArchitecture\n. The topology is implemented with the standard Storm spout and bolt components:\n\n\n\n\nThe components in \nArchitecture\n have direct counterparts here. The DRPC servers, the DRPC spouts, the Prepare Request bolts comprise the Request Processor. The Filter bolts and your plugin for your source of Data make up the Data Processor. The Join bolt and the Return Results bolt make up the Combiner.\n\n\nThe red colored lines are the path for the queries that come in through Storm DRPC and the blue is for the data from your data source. The pattern on the lines denote how the data (Storm tuples) is moved to the next component. Dashed indicates a broadcast (sent to all instances of the component), dotted indicates a key grouping (sent to a particular instance based on hashing on a particular field), and solid indicates a shuffle (randomly sent to an instance).\n\n\n\n\nWhat's a Ticker?\n\n\nThe Ticker component is attached to the Filter and Join Bolts produce Storm tuples at predefined intervals. This is a Storm feature (and is configurable when you launch the Bullet topology). These tuples, called tick tuples, behave like a CPU clock cycles for Bullet. Bullet performs all its system related activities on a tick. This includes purging stale queries, emitting left over data for queries, etc. We could have gone the route of having asynchronous threads that do the same thing but this was a far more simpler solution. The downside is that Bullet is as fast or as slow as its tick period, which can only go as low at 1 s in Storm. In practice, this means that your window is longer by a tick and you can accommodate that in your query if you wish.\n\n\nAs a practical example of how Bullet uses ticks: when the final data is emitted from the Filter bolts when the query has expired, the Join bolt receiving it waits for 3 (this is configurable) ticks after \nits query\n expires to collect all the last intermediate results from the Filter bolts. If the tick period is set as high as 5 s, this means that a query will take 3 * 15 or 15 s to get back after its expiry! Setting it to 1 s, makes it 1 * 3 s. By changing the number of ticks that the Join bolt waits for and the tick period, you can get to any integral delay \n= 1 s.\n\n\n\n\nData processing\n\n\nBullet can accept arbitrary sources of data as long as they can be read from Storm. You can either:\n\n\n\n\nWrite a Storm spout that reads your data from where ever it is (Kafka, etc) and \nconverts it to Bullet Records\n. See \nQuick Start\n for an example.\n\n\nHook up an existing topology that is doing something else directly to Bullet. You will still write and hook up a component that converts your data into Bullet Records in your existing topology.\n\n\n\n\nOption 2 is nice if you do not want to introduce a persistence layer between your existing Streaming pipeline and Bullet. For example, if you just want periodically look at some data within your topology, you could filter them, convert them into Bullet Records and send it into Bullet. You could also sample data. The downside of Option 2 is that you will directly couple your topology with Bullet leaving your topology to be affected by Bullet through Storm features like back-pressure (if you are on Storm 1.0) etc. You could also go with Option 2 if you need something more complex than just a spout from Option 1. For example, you may want to process your data in some fashion before emitting to Bullet.\n\n\nYour data is then emitted to the Filter bolt which promptly drops all Bullet Records and does absolutely nothing if you have no queries in your system. If there are queries in the Filter bolt, the record is checked against the \nfilters\n in each query and if it matches, it is processed by the query. Each query can choose to emit matched records in micro-batches. For example, queries that collect raw records (a LIMIT operation) do not micro-batch at all. Every matched record (up to the maximum for the query) is emitted. Queries that aggregate, on the other hand, keep the query around till its duration is up and emit the local result.\n\n\n\n\nTo micro-batch or not to micro-batch?\n\n\nRAW\n queries micro-batch by size 1, which makes Bullet really snappy when running those queries. As soon as your maximum record limit is reached, the query immediately returns. On the other hand, the other queries do not micro-batch at all. \nGROUP\n and other aggregate queries \ncannot\n return till they see all the data in your time window because some late arriving data may update an existing aggregate. So, these other queries have to wait for the entire query duration anyway. Once the queries have timed out, we have to rely on the ticks to get all the intermediate results over to the combiner to merge. Micro-batches are still useful here because we can still emit intermediate aggregations (and they are \nadditive\n) and relieve memory pressure by periodically purging intermediate results. In practice though, Bullet queries are generally short-lived, so this isn't as needed as it may seem on first glance. Depends on whether others (you) find it necessary, we may decide to implement micro-batching for other queries besides \nRAW\n types.\n\n\n\n\nRequest processing\n\n\nStorm DRPC handles receiving REST requests for the whole topology. The DRPC spouts fetch these requests (DRPC knows the request is for the Bullet topology using the unique function name set when launching the topology) and shuffle them to the Prepare Request bolts. The request also contains information about how to return the response back to the DRPC servers. The Prepare Request bolts generate unique identifiers for each request (a Bullet query) and broadcasts them to every Filter bolt. Since every Filter bolt has a copy of every query, the shuffled data from the source of data can be compared against the query no matter which particular Filter bolt it ends up at. Each Filter bolt has access to the unique query id and is able to key group by the id to the Join bolt with the intermediate results for the query.\n\n\nThe Prepare Request bolt also key groups the query and the return information to the Join bolts. This means that only \none\n Join bolt ever gets one query.\n\n\nCombining\n\n\nSince the data from the Prepare Request bolt (a query and a piece of return information for the query) and the data from all Filter bolts (intermediate results) is key grouped by the unique query id, only one particular Join bolt receives both the query and all the intermediate results for a particular query. The Join bolt can then combine all the intermediate results and produce a final result. This final result is joined (hence the name) with the return information for the query and is shuffled to the Return Results bolt. This bolt then uses the return information to send the results back to a DRPC server, who then returns it back to the requester.\n\n\n\n\nCombining and operations\n\n\nIn order to be able to combine intermediate results and process data in any order, all aggregations that Bullet does need to be associative and have an identity. In other words, they need to be \nMonoids\n. Luckily for us, the \nDataSketches\n that we use are monoids (actually are commutative monoids). Sketches be unioned and thus all the aggregations we support - SUM, COUNT, MIN, MAX, AVG, COUNT DISTINCTS, DISTINCT - are monoidal. (AVG is monoidal if you store a SUM and a COUNT instead).\n\n\n\n\nScalability\n\n\nThe topology set up this way scales horizontally and has some nice properties:\n\n\n\n\nIf you want to scale for processing more data but the same amount of queries, you only need to scale the components that read your data (the spout reading the data or your custom topology) and the Filter bolts.\n\n\nIf you want to scale for more queries but the same amount of data, you only need to scale up the DRPC spouts, Prepare Request bolts, Join bolts and Return Results bolts (first order). These components generally have low parallelism compared to your data since the data is generally much higher.\n\n\n\n\n\n\nFirst order?\n\n\nIf you send more queries to the Filter bolt, it will be limited by at most how many queries a Filter bolt can store and still process data efficiently. Factors like CPU, memory allocations etc for the Filter bolts come in to the picture in addition to the parallelism. Generally, if you have allocated enough Filter bolts to process your data with enough head room, this should let you run hundreds of queries simultaneously before you run into these issues.", 
            "title": "Storm Architecture"
        }, 
        {
            "location": "/backend/storm-architecture/#storm-architecture", 
            "text": "This section describes how the  Backend architecture  is implemented in Storm.", 
            "title": "Storm architecture"
        }, 
        {
            "location": "/backend/storm-architecture/#storm-drpc", 
            "text": "Bullet on  Storm  is built using  Storm DRPC . DRPC or Distributed Remote Procedure Call, is built into Storm and consists of a set of servers that are part of the Storm cluster. When a Storm topology that uses DRPC is launched, it registers a spout with a unique name (the procedure in the Distributed Remote Procedure Call) with the DRPC infrastructure. The DRPC Servers expose a REST endpoint where data can be POSTed to or a GET request can be made with this unique name. The DRPC infrastructure then sends the request (a query in Bullet) through the spout(s) to the topology that registered that name (Bullet). The result from topology is sent back to the client. We picked Storm to implement Bullet on first not only because it was the most popular Streaming framework at Yahoo but also since DRPC provides us a nice and simple way to handle getting queries into Bullet and sending responses back.   Thrift and DRPC servers  DRPC also exposes a  Thrift  endpoint but the Bullet Web Service uses REST for simplicity. When you launch your Bullet Storm topology, you can POST Bullet queries to a DRPC server directly with the function name that you specified in the Bullet configuration. This is a quick way to check if your topology is up and running!", 
            "title": "Storm DRPC"
        }, 
        {
            "location": "/backend/storm-architecture/#topology", 
            "text": "For Bullet on Storm, the Storm topology implements the backend piece from the full  Architecture . The topology is implemented with the standard Storm spout and bolt components:   The components in  Architecture  have direct counterparts here. The DRPC servers, the DRPC spouts, the Prepare Request bolts comprise the Request Processor. The Filter bolts and your plugin for your source of Data make up the Data Processor. The Join bolt and the Return Results bolt make up the Combiner.  The red colored lines are the path for the queries that come in through Storm DRPC and the blue is for the data from your data source. The pattern on the lines denote how the data (Storm tuples) is moved to the next component. Dashed indicates a broadcast (sent to all instances of the component), dotted indicates a key grouping (sent to a particular instance based on hashing on a particular field), and solid indicates a shuffle (randomly sent to an instance).   What's a Ticker?  The Ticker component is attached to the Filter and Join Bolts produce Storm tuples at predefined intervals. This is a Storm feature (and is configurable when you launch the Bullet topology). These tuples, called tick tuples, behave like a CPU clock cycles for Bullet. Bullet performs all its system related activities on a tick. This includes purging stale queries, emitting left over data for queries, etc. We could have gone the route of having asynchronous threads that do the same thing but this was a far more simpler solution. The downside is that Bullet is as fast or as slow as its tick period, which can only go as low at 1 s in Storm. In practice, this means that your window is longer by a tick and you can accommodate that in your query if you wish.  As a practical example of how Bullet uses ticks: when the final data is emitted from the Filter bolts when the query has expired, the Join bolt receiving it waits for 3 (this is configurable) ticks after  its query  expires to collect all the last intermediate results from the Filter bolts. If the tick period is set as high as 5 s, this means that a query will take 3 * 15 or 15 s to get back after its expiry! Setting it to 1 s, makes it 1 * 3 s. By changing the number of ticks that the Join bolt waits for and the tick period, you can get to any integral delay  = 1 s.", 
            "title": "Topology"
        }, 
        {
            "location": "/backend/storm-architecture/#data-processing", 
            "text": "Bullet can accept arbitrary sources of data as long as they can be read from Storm. You can either:   Write a Storm spout that reads your data from where ever it is (Kafka, etc) and  converts it to Bullet Records . See  Quick Start  for an example.  Hook up an existing topology that is doing something else directly to Bullet. You will still write and hook up a component that converts your data into Bullet Records in your existing topology.   Option 2 is nice if you do not want to introduce a persistence layer between your existing Streaming pipeline and Bullet. For example, if you just want periodically look at some data within your topology, you could filter them, convert them into Bullet Records and send it into Bullet. You could also sample data. The downside of Option 2 is that you will directly couple your topology with Bullet leaving your topology to be affected by Bullet through Storm features like back-pressure (if you are on Storm 1.0) etc. You could also go with Option 2 if you need something more complex than just a spout from Option 1. For example, you may want to process your data in some fashion before emitting to Bullet.  Your data is then emitted to the Filter bolt which promptly drops all Bullet Records and does absolutely nothing if you have no queries in your system. If there are queries in the Filter bolt, the record is checked against the  filters  in each query and if it matches, it is processed by the query. Each query can choose to emit matched records in micro-batches. For example, queries that collect raw records (a LIMIT operation) do not micro-batch at all. Every matched record (up to the maximum for the query) is emitted. Queries that aggregate, on the other hand, keep the query around till its duration is up and emit the local result.   To micro-batch or not to micro-batch?  RAW  queries micro-batch by size 1, which makes Bullet really snappy when running those queries. As soon as your maximum record limit is reached, the query immediately returns. On the other hand, the other queries do not micro-batch at all.  GROUP  and other aggregate queries  cannot  return till they see all the data in your time window because some late arriving data may update an existing aggregate. So, these other queries have to wait for the entire query duration anyway. Once the queries have timed out, we have to rely on the ticks to get all the intermediate results over to the combiner to merge. Micro-batches are still useful here because we can still emit intermediate aggregations (and they are  additive ) and relieve memory pressure by periodically purging intermediate results. In practice though, Bullet queries are generally short-lived, so this isn't as needed as it may seem on first glance. Depends on whether others (you) find it necessary, we may decide to implement micro-batching for other queries besides  RAW  types.", 
            "title": "Data processing"
        }, 
        {
            "location": "/backend/storm-architecture/#request-processing", 
            "text": "Storm DRPC handles receiving REST requests for the whole topology. The DRPC spouts fetch these requests (DRPC knows the request is for the Bullet topology using the unique function name set when launching the topology) and shuffle them to the Prepare Request bolts. The request also contains information about how to return the response back to the DRPC servers. The Prepare Request bolts generate unique identifiers for each request (a Bullet query) and broadcasts them to every Filter bolt. Since every Filter bolt has a copy of every query, the shuffled data from the source of data can be compared against the query no matter which particular Filter bolt it ends up at. Each Filter bolt has access to the unique query id and is able to key group by the id to the Join bolt with the intermediate results for the query.  The Prepare Request bolt also key groups the query and the return information to the Join bolts. This means that only  one  Join bolt ever gets one query.", 
            "title": "Request processing"
        }, 
        {
            "location": "/backend/storm-architecture/#combining", 
            "text": "Since the data from the Prepare Request bolt (a query and a piece of return information for the query) and the data from all Filter bolts (intermediate results) is key grouped by the unique query id, only one particular Join bolt receives both the query and all the intermediate results for a particular query. The Join bolt can then combine all the intermediate results and produce a final result. This final result is joined (hence the name) with the return information for the query and is shuffled to the Return Results bolt. This bolt then uses the return information to send the results back to a DRPC server, who then returns it back to the requester.   Combining and operations  In order to be able to combine intermediate results and process data in any order, all aggregations that Bullet does need to be associative and have an identity. In other words, they need to be  Monoids . Luckily for us, the  DataSketches  that we use are monoids (actually are commutative monoids). Sketches be unioned and thus all the aggregations we support - SUM, COUNT, MIN, MAX, AVG, COUNT DISTINCTS, DISTINCT - are monoidal. (AVG is monoidal if you store a SUM and a COUNT instead).", 
            "title": "Combining"
        }, 
        {
            "location": "/backend/storm-architecture/#scalability", 
            "text": "The topology set up this way scales horizontally and has some nice properties:   If you want to scale for processing more data but the same amount of queries, you only need to scale the components that read your data (the spout reading the data or your custom topology) and the Filter bolts.  If you want to scale for more queries but the same amount of data, you only need to scale up the DRPC spouts, Prepare Request bolts, Join bolts and Return Results bolts (first order). These components generally have low parallelism compared to your data since the data is generally much higher.    First order?  If you send more queries to the Filter bolt, it will be limited by at most how many queries a Filter bolt can store and still process data efficiently. Factors like CPU, memory allocations etc for the Filter bolts come in to the picture in addition to the parallelism. Generally, if you have allocated enough Filter bolts to process your data with enough head room, this should let you run hundreds of queries simultaneously before you run into these issues.", 
            "title": "Scalability"
        }, 
        {
            "location": "/backend/ingestion/", 
            "text": "Data Ingestion\n\n\nBullet operates on a generic data container that it understands. In order to get Bullet to operate on your data, you need to convert your data records into this format. This conversion is usually done when you plug in your data source into Bullet. Bullet does not make any assumptions on where you get this data from. It could be \nKafka\n, \nRabbitMQ\n, or something else.\n\n\n\n\nIf you are trying to set up Bullet...\n\n\nThe rest of this page gives more information about the Record container and how to depend on it in code directly. If you are setting up Bullet, the Record is already included by default with the Bullet artifact. You can head on over to \nsetting up the Storm topology\n to build the piece that gets your data into the Record container.\n\n\n\n\nBullet Record\n\n\nThe Bullet Record is a serializable data container based on \nAvro\n. It is typed and has a generic schema. You can refer to the \nAvro Schema\n file for details if you wish to see the internals of the data model. The Bullet Record is also lazy and only deserializes itself when you try to read something from it. So, you can pass it around before sending to Bullet with minimal cost. Partial deserialization is being considered if performance is key. This will let you deserialize a much narrower chunk of the Record if you are just looking for a couple of fields.\n\n\nTypes\n\n\nData placed into a Bullet Record is strongly typed. We support these types currently:\n\n\nPrimitives\n\n\n\n\nBoolean\n\n\nLong\n\n\nDouble\n\n\nString\n\n\n\n\nComplex\n\n\n\n\nMap of Strings to any of the \nPrimitives\n\n\nMap of Strings to any Map in 1\n\n\nList of any Map in 1\n\n\n\n\nWith these types, it is unlikely you would have data that cannot be represented as Bullet Record but if you do, please let us know and we are more than willing to accommodate.\n\n\nInstalling the Record directly\n\n\nGenerally, you depend on the Bullet artifact for your Stream Processor when you plug in the piece that gets your data into the Stream processor. The Bullet artifact already brings in the Bullet Record container as well. See the usage for the \nStorm\n.\n\n\nHowever, if you need it, the artifacts are available through JCenter to depend on them in code directly. You will need to add the repository. Below is a Maven example:\n\n\nrepositories\n\n    \nrepository\n\n        \nsnapshots\n\n            \nenabled\nfalse\n/enabled\n\n        \n/snapshots\n\n        \nid\ncentral\n/id\n\n        \nname\nbintray\n/name\n\n        \nurl\nhttp://jcenter.bintray.com\n/url\n\n    \n/repository\n\n\n/repositories\n\n\n\n\n\ndependency\n\n  \ngroupId\ncom.yahoo.bullet\n/groupId\n\n  \nartifactId\nbullet-record\n/artifactId\n\n  \nversion\n${bullet.version}\n/version\n\n\n/dependency\n\n\n\n\n\nIf you just need the jar artifact, you can download it directly from \nJCenter\n.\n\n\nYou can also add \nclassifier\nsources\n/classifier\n  or \nclassifier\njavadoc\n/classifier\n if you want the sources or the javadoc.", 
            "title": "Getting your data into Bullet"
        }, 
        {
            "location": "/backend/ingestion/#data-ingestion", 
            "text": "Bullet operates on a generic data container that it understands. In order to get Bullet to operate on your data, you need to convert your data records into this format. This conversion is usually done when you plug in your data source into Bullet. Bullet does not make any assumptions on where you get this data from. It could be  Kafka ,  RabbitMQ , or something else.   If you are trying to set up Bullet...  The rest of this page gives more information about the Record container and how to depend on it in code directly. If you are setting up Bullet, the Record is already included by default with the Bullet artifact. You can head on over to  setting up the Storm topology  to build the piece that gets your data into the Record container.", 
            "title": "Data Ingestion"
        }, 
        {
            "location": "/backend/ingestion/#bullet-record", 
            "text": "The Bullet Record is a serializable data container based on  Avro . It is typed and has a generic schema. You can refer to the  Avro Schema  file for details if you wish to see the internals of the data model. The Bullet Record is also lazy and only deserializes itself when you try to read something from it. So, you can pass it around before sending to Bullet with minimal cost. Partial deserialization is being considered if performance is key. This will let you deserialize a much narrower chunk of the Record if you are just looking for a couple of fields.", 
            "title": "Bullet Record"
        }, 
        {
            "location": "/backend/ingestion/#types", 
            "text": "Data placed into a Bullet Record is strongly typed. We support these types currently:", 
            "title": "Types"
        }, 
        {
            "location": "/backend/ingestion/#primitives", 
            "text": "Boolean  Long  Double  String", 
            "title": "Primitives"
        }, 
        {
            "location": "/backend/ingestion/#complex", 
            "text": "Map of Strings to any of the  Primitives  Map of Strings to any Map in 1  List of any Map in 1   With these types, it is unlikely you would have data that cannot be represented as Bullet Record but if you do, please let us know and we are more than willing to accommodate.", 
            "title": "Complex"
        }, 
        {
            "location": "/backend/ingestion/#installing-the-record-directly", 
            "text": "Generally, you depend on the Bullet artifact for your Stream Processor when you plug in the piece that gets your data into the Stream processor. The Bullet artifact already brings in the Bullet Record container as well. See the usage for the  Storm .  However, if you need it, the artifacts are available through JCenter to depend on them in code directly. You will need to add the repository. Below is a Maven example:  repositories \n     repository \n         snapshots \n             enabled false /enabled \n         /snapshots \n         id central /id \n         name bintray /name \n         url http://jcenter.bintray.com /url \n     /repository  /repositories   dependency \n   groupId com.yahoo.bullet /groupId \n   artifactId bullet-record /artifactId \n   version ${bullet.version} /version  /dependency   If you just need the jar artifact, you can download it directly from  JCenter .  You can also add  classifier sources /classifier   or  classifier javadoc /classifier  if you want the sources or the javadoc.", 
            "title": "Installing the Record directly"
        }, 
        {
            "location": "/backend/setup-storm/", 
            "text": "Bullet on Storm\n\n\nStorm DRPC\n\n\nBullet on \nStorm\n is built using \nStorm DRPC\n. DRPC comes with Storm installations and generally consist of a set of DRPC servers. When a Storm topology is launched and it uses DRPC, it registers a spout with a unique name with the DRPC infrastructure. The DRPC Servers expose a REST endpoint where data can be POSTed to or a GET request can be made with this unique name. The DRPC infrastructure then sends the request (a query in Bullet) through the spout(s) to the topology (Bullet). Bullet uses the query to filter and joins all records emitted from your (configurable) data source - either a Spout or a topology component according to the query specification. The resulting matched records are aggregated and sent back to the client. We chose to implement Bullet on Storm first since DRPC provides us a nice and simple way to handle getting queries into Bullet and sending responses back.\n\n\nQuery duration in Storm DRPC\n\n\nThe maximum time a query can run for depends on the maximum time Storm DRPC request can last in your Storm topology. Generally the default is set to 10 minutes. This means that the \nlongest query duration possible will be 10 minutes\n. This is up to your cluster maintainers.\n\n\nConfiguration\n\n\nBullet is configured at run-time using settings defined in a file. Settings not overridden will default to the values in \nbullet_defaults.yaml\n. There are too many to list here. You can find out what these settings do in the comments listed in the defaults.\n\n\nInstallation\n\n\nTo use Bullet, you need to implement a way to read from your data source and convert your data into Bullet Records (bullet-record is a transitive dependency for Bullet and can be found \nin JCenter\n. You have two options in how to get your data into Bullet:\n\n\n\n\nYou can implement a Spout that reads from your data source and emits Bullet Record. This spout must have a constructor that takes a List of Strings.\n\n\nYou can pipe your existing Storm topology directly into Bullet. In other words, you convert the data you wish to be query-able through Bullet into Bullet Records from a bolt in your topology.\n\n\n\n\nOption 2 \ndirectly\n couples your topology to Bullet and as such, you would need to watch out for things like back-pressure etc.\n\n\nYou need a JVM based project that implements one of the two options above. You include the Bullet artifact and Storm dependencies in your pom.xml or other dependency management system. The artifacts are available through JCenter, so you will need to add the repository.\n\n\nrepositories\n\n    \nrepository\n\n        \nsnapshots\n\n            \nenabled\nfalse\n/enabled\n\n        \n/snapshots\n\n        \nid\ncentral\n/id\n\n        \nname\nbintray\n/name\n\n        \nurl\nhttp://jcenter.bintray.com\n/url\n\n    \n/repository\n\n\n/repositories\n\n\n\n\n\ndependency\n\n  \ngroupId\norg.apache.storm\n/groupId\n\n  \nartifactId\nstorm-core\n/artifactId\n\n  \nversion\n${storm.version}\n/version\n\n  \nscope\nprovided\n/scope\n\n\n/dependency\n\n\n\ndependency\n\n  \ngroupId\ncom.yahoo.bullet\n/groupId\n\n  \nartifactId\nbullet-storm\n/artifactId\n\n  \nversion\n${bullet.version}\n/version\n\n\n/dependency\n\n\n\n\n\nIf you just need the jar artifact directly, you can download it from \nJCenter\n.\n\n\nYou can also add \nclassifier\nsources\n/classifier\n  or \nclassifier\njavadoc\n/classifier\n if you want the sources or javadoc. We also package up our test code where we have some helper classes to deal with \nStorm components\n. If you wish to use these to help with testing your topology, you can add another dependency on bullet-storm with \ntype\ntest-jar\n/type\n.\n\n\nIf you are going to use the second option (directly pipe data into Bullet from your Storm topology), then you will need a main class that directly calls the submit method with your wired up topology and the name of the component that is going to emit Bullet Records in that wired up topology. The submit method can be found in \nTopology.java\n. The submit method submits the topology so it should be the last thing you do in your main.\n\n\nIf you are just implementing a Spout, see the \nLaunch\n section below on how to use the main class in Bullet to create and submit your topology.\n\n\nStorm topologies are generally launched with \"fat\" jars (jar-with-dependencies), excluding storm itself:\n\n\nplugin\n\n    \ngroupId\norg.apache.maven.plugins\n/groupId\n\n    \nartifactId\nmaven-assembly-plugin\n/artifactId\n\n    \nversion\n2.4\n/version\n\n    \nexecutions\n\n        \nexecution\n\n            \nid\nassemble-all\n/id\n\n            \nphase\npackage\n/phase\n\n            \ngoals\n\n                \ngoal\nsingle\n/goal\n\n            \n/goals\n\n        \n/execution\n\n    \n/executions\n\n    \nconfiguration\n\n        \ndescriptorRefs\n\n            \ndescriptorRef\njar-with-dependencies\n/descriptorRef\n\n        \n/descriptorRefs\n\n    \n/configuration\n\n\n/plugin\n\n\n\n\n\nOlder Storm Versions\n\n\nSince package prefixes changed from \nbacktype.storm\n to \norg.apache.storm\n in Storm 1.0 and above, you will need to get the storm-0.10 version of Bullet if\nyour Storm cluster is still not at 1.0 or higher. You change your dependency to:\n\n\ndependency\n\n    \ngroupId\ncom.yahoo.bullet\n/groupId\n\n    \nartifactId\nbullet-storm-0.10\n/artifactId\n\n    \nversion\n${bullet.version}\n/version\n\n\n/dependency\n\n\n\n\n\nThe jar artifact can be downloaded directly from \nJCenter\n.\n\n\nYou can also add \nclassifier\nsources\n/classifier\n  or \nclassifier\njavadoc\n/classifier\n if you want the source or javadoc and \ntype\ntest-jar\n/type\n for the test classes as with bullet-storm.\n\n\nAlso, since storm-metrics and the Resource Aware Scheduler are not in Storm versions less than 1.0, there are changes in the Bullet settings. The settings that set the CPU and memory loads do not exist (so the config file does not specify them). The setting to enable the topology scheduler are no longer present (you can still override these settings if you run a custom version of Storm by passing it to the storm jar command. \nSee below\n.) You can take a look the settings file on the storm-0.10 branch in the Git repo.\n\n\nIf for some reason, you are running a version of Storm less than 1.0 that has the RAS back-ported to it and you wish to set the CPU and other settings, you will your own main class that mirrors the master branch of the main class but with backtype.storm packages instead.\n\n\nLaunch\n\n\nIf you have implemented your own main class (option 2 above), you just pass your main class to the storm executable as usual. If you are implementing a spout, here's an example of how you could launch the topology:\n\n\nstorm jar your-fat-jar-with-dependencies.jar \\\n          com.yahoo.bullet.Topology \\\n          --bullet-conf path/to/the/bullet_settings.yaml \\\n          --bullet-spout full.package.prefix.to.your.spout.implementation \\\n          --bullet-spout-parallelism 64 \\\n          --bullet-spout-cpu-load 200.0 \\\n          --bullet-spout-on-heap-memory-load 512.0 \\\n          --bullet-spout-off-heap-memory-load 256.0 \\\n          --bullet-spout-arg arg-to-your-spout-class-for-ex-a-path-to-a-config-file \\\n          --bullet-spout-arg another-arg-to-your-spout-class \\\n          -c topology.acker.executors=64 \\\n          -c topology.max.spout.pending=10000\n\n\n\n\nYou can pass other arguments to Storm using the -c argument. The example above uses 64 ackers, which is the parallelism of the Filter Bolt. Storm DRPC follows the principle of leaving retries to the DRPC user (in our case, the Bullet web service). As a result, most of the DRPC components do not follow any at least once guarantees. However, you can enable at least once for the hop from your topology (or spout) to the Filter Bolt. This is why this example uses the parallelism of the Filter Bolt as the number of ackers since that is exactly the number of acker tasks we would need (not accounting for the DRPCSpout to the PrepareRequest Bolt acking). Ackers are lightweight so you need not have the same number of tasks as Filter Bolts but you can tweak it accordingly. The example above also sets max spout pending to control how fast the spout emits. You could use the back-pressure mechanisms in Storm in addition or in lieu of as you choose. We have found that max spout pending gives a much more predictable way of throttling our spouts during catch up or data spikes.\n\n\n\n\nMain Class Arguments\n\n\nIf you run the main class without arguments or pass in the \n--help\n argument, you can see what these arguments mean and what others are supported.\n\n\n\n\nTest\n\n\nOnce the topology is up and your data consumption has stabilized, you could post a query to a DRPC server in your cluster. Try a simple query from the \nexamples\n by running a curl from a command line:\n\n\ncurl -s -X POST -d '{}' http://\nDRPC_HOST\n:\nDRPC_PORT\n/drpc/\nYOUR_TOPOLOGY_FUNCTION_FROM_YOUR_BULLET_CONF\n\n\n\n\n\nYou should receive a random record flowing through Bullet instantly (if you left the Raw aggregation micro-batch size at the default of 1).", 
            "title": "Setup on Storm"
        }, 
        {
            "location": "/backend/setup-storm/#bullet-on-storm", 
            "text": "", 
            "title": "Bullet on Storm"
        }, 
        {
            "location": "/backend/setup-storm/#storm-drpc", 
            "text": "Bullet on  Storm  is built using  Storm DRPC . DRPC comes with Storm installations and generally consist of a set of DRPC servers. When a Storm topology is launched and it uses DRPC, it registers a spout with a unique name with the DRPC infrastructure. The DRPC Servers expose a REST endpoint where data can be POSTed to or a GET request can be made with this unique name. The DRPC infrastructure then sends the request (a query in Bullet) through the spout(s) to the topology (Bullet). Bullet uses the query to filter and joins all records emitted from your (configurable) data source - either a Spout or a topology component according to the query specification. The resulting matched records are aggregated and sent back to the client. We chose to implement Bullet on Storm first since DRPC provides us a nice and simple way to handle getting queries into Bullet and sending responses back.", 
            "title": "Storm DRPC"
        }, 
        {
            "location": "/backend/setup-storm/#query-duration-in-storm-drpc", 
            "text": "The maximum time a query can run for depends on the maximum time Storm DRPC request can last in your Storm topology. Generally the default is set to 10 minutes. This means that the  longest query duration possible will be 10 minutes . This is up to your cluster maintainers.", 
            "title": "Query duration in Storm DRPC"
        }, 
        {
            "location": "/backend/setup-storm/#configuration", 
            "text": "Bullet is configured at run-time using settings defined in a file. Settings not overridden will default to the values in  bullet_defaults.yaml . There are too many to list here. You can find out what these settings do in the comments listed in the defaults.", 
            "title": "Configuration"
        }, 
        {
            "location": "/backend/setup-storm/#installation", 
            "text": "To use Bullet, you need to implement a way to read from your data source and convert your data into Bullet Records (bullet-record is a transitive dependency for Bullet and can be found  in JCenter . You have two options in how to get your data into Bullet:   You can implement a Spout that reads from your data source and emits Bullet Record. This spout must have a constructor that takes a List of Strings.  You can pipe your existing Storm topology directly into Bullet. In other words, you convert the data you wish to be query-able through Bullet into Bullet Records from a bolt in your topology.   Option 2  directly  couples your topology to Bullet and as such, you would need to watch out for things like back-pressure etc.  You need a JVM based project that implements one of the two options above. You include the Bullet artifact and Storm dependencies in your pom.xml or other dependency management system. The artifacts are available through JCenter, so you will need to add the repository.  repositories \n     repository \n         snapshots \n             enabled false /enabled \n         /snapshots \n         id central /id \n         name bintray /name \n         url http://jcenter.bintray.com /url \n     /repository  /repositories   dependency \n   groupId org.apache.storm /groupId \n   artifactId storm-core /artifactId \n   version ${storm.version} /version \n   scope provided /scope  /dependency  dependency \n   groupId com.yahoo.bullet /groupId \n   artifactId bullet-storm /artifactId \n   version ${bullet.version} /version  /dependency   If you just need the jar artifact directly, you can download it from  JCenter .  You can also add  classifier sources /classifier   or  classifier javadoc /classifier  if you want the sources or javadoc. We also package up our test code where we have some helper classes to deal with  Storm components . If you wish to use these to help with testing your topology, you can add another dependency on bullet-storm with  type test-jar /type .  If you are going to use the second option (directly pipe data into Bullet from your Storm topology), then you will need a main class that directly calls the submit method with your wired up topology and the name of the component that is going to emit Bullet Records in that wired up topology. The submit method can be found in  Topology.java . The submit method submits the topology so it should be the last thing you do in your main.  If you are just implementing a Spout, see the  Launch  section below on how to use the main class in Bullet to create and submit your topology.  Storm topologies are generally launched with \"fat\" jars (jar-with-dependencies), excluding storm itself:  plugin \n     groupId org.apache.maven.plugins /groupId \n     artifactId maven-assembly-plugin /artifactId \n     version 2.4 /version \n     executions \n         execution \n             id assemble-all /id \n             phase package /phase \n             goals \n                 goal single /goal \n             /goals \n         /execution \n     /executions \n     configuration \n         descriptorRefs \n             descriptorRef jar-with-dependencies /descriptorRef \n         /descriptorRefs \n     /configuration  /plugin", 
            "title": "Installation"
        }, 
        {
            "location": "/backend/setup-storm/#older-storm-versions", 
            "text": "Since package prefixes changed from  backtype.storm  to  org.apache.storm  in Storm 1.0 and above, you will need to get the storm-0.10 version of Bullet if\nyour Storm cluster is still not at 1.0 or higher. You change your dependency to:  dependency \n     groupId com.yahoo.bullet /groupId \n     artifactId bullet-storm-0.10 /artifactId \n     version ${bullet.version} /version  /dependency   The jar artifact can be downloaded directly from  JCenter .  You can also add  classifier sources /classifier   or  classifier javadoc /classifier  if you want the source or javadoc and  type test-jar /type  for the test classes as with bullet-storm.  Also, since storm-metrics and the Resource Aware Scheduler are not in Storm versions less than 1.0, there are changes in the Bullet settings. The settings that set the CPU and memory loads do not exist (so the config file does not specify them). The setting to enable the topology scheduler are no longer present (you can still override these settings if you run a custom version of Storm by passing it to the storm jar command.  See below .) You can take a look the settings file on the storm-0.10 branch in the Git repo.  If for some reason, you are running a version of Storm less than 1.0 that has the RAS back-ported to it and you wish to set the CPU and other settings, you will your own main class that mirrors the master branch of the main class but with backtype.storm packages instead.", 
            "title": "Older Storm Versions"
        }, 
        {
            "location": "/backend/setup-storm/#launch", 
            "text": "If you have implemented your own main class (option 2 above), you just pass your main class to the storm executable as usual. If you are implementing a spout, here's an example of how you could launch the topology:  storm jar your-fat-jar-with-dependencies.jar \\\n          com.yahoo.bullet.Topology \\\n          --bullet-conf path/to/the/bullet_settings.yaml \\\n          --bullet-spout full.package.prefix.to.your.spout.implementation \\\n          --bullet-spout-parallelism 64 \\\n          --bullet-spout-cpu-load 200.0 \\\n          --bullet-spout-on-heap-memory-load 512.0 \\\n          --bullet-spout-off-heap-memory-load 256.0 \\\n          --bullet-spout-arg arg-to-your-spout-class-for-ex-a-path-to-a-config-file \\\n          --bullet-spout-arg another-arg-to-your-spout-class \\\n          -c topology.acker.executors=64 \\\n          -c topology.max.spout.pending=10000  You can pass other arguments to Storm using the -c argument. The example above uses 64 ackers, which is the parallelism of the Filter Bolt. Storm DRPC follows the principle of leaving retries to the DRPC user (in our case, the Bullet web service). As a result, most of the DRPC components do not follow any at least once guarantees. However, you can enable at least once for the hop from your topology (or spout) to the Filter Bolt. This is why this example uses the parallelism of the Filter Bolt as the number of ackers since that is exactly the number of acker tasks we would need (not accounting for the DRPCSpout to the PrepareRequest Bolt acking). Ackers are lightweight so you need not have the same number of tasks as Filter Bolts but you can tweak it accordingly. The example above also sets max spout pending to control how fast the spout emits. You could use the back-pressure mechanisms in Storm in addition or in lieu of as you choose. We have found that max spout pending gives a much more predictable way of throttling our spouts during catch up or data spikes.   Main Class Arguments  If you run the main class without arguments or pass in the  --help  argument, you can see what these arguments mean and what others are supported.", 
            "title": "Launch"
        }, 
        {
            "location": "/backend/setup-storm/#test", 
            "text": "Once the topology is up and your data consumption has stabilized, you could post a query to a DRPC server in your cluster. Try a simple query from the  examples  by running a curl from a command line:  curl -s -X POST -d '{}' http:// DRPC_HOST : DRPC_PORT /drpc/ YOUR_TOPOLOGY_FUNCTION_FROM_YOUR_BULLET_CONF   You should receive a random record flowing through Bullet instantly (if you left the Raw aggregation micro-batch size at the default of 1).", 
            "title": "Test"
        }, 
        {
            "location": "/backend/performance/", 
            "text": "Coming soon!", 
            "title": "Performance"
        }, 
        {
            "location": "/backend/performance/#coming-soon", 
            "text": "", 
            "title": "Coming soon!"
        }, 
        {
            "location": "/ws/setup/", 
            "text": "The Web Service\n\n\nThe Web Service is a Java WAR file that you can deploy on a machine to communicate with the Bullet Backend. For Storm, it talks to the Storm DRPC servers. To set up the Bullet backend topology, see \nStorm setup\n.\n\n\nThere are two main purposes for this layer at this time:\n\n\n1) It provides an endpoint that can serve a \nJSON API schema\n for the Bullet UI. Currently, static schemas from a file are supported.\n\n\n2) It proxies a JSON Bullet query to Bullet and wraps errors if the backend is unreachable.\n\n\n\n\nThat's it?\n\n\nThe Web Service essentially just wraps Storm DRPC and provides some helpful endpoints. But the Web Service is there to be a point of abstraction for implementing things like security, monitoring, access-control, rate-limiting, different query formats (e.g. SQL Bullet queries) etc, which are planned in the near future.\n\n\n\n\nPrerequisites\n\n\nIn order for your Web Service to work with Bullet, you should have an instance of the \nbackend\n already set up.\n\n\nInstallation\n\n\nYou can download the WAR file directly from \nJCenter\n.\n\n\nIf you need to depend on the source code directly for any reason, you need to add the JCenter repository and get the artifact through your dependency management system. Maven is shown below.\n\n\nrepositories\n\n    \nrepository\n\n        \nsnapshots\n\n            \nenabled\nfalse\n/enabled\n\n        \n/snapshots\n\n        \nid\ncentral\n/id\n\n        \nname\nbintray\n/name\n\n        \nurl\nhttp://jcenter.bintray.com\n/url\n\n    \n/repository\n\n\n/repositories\n\n\n\n\n\ndependency\n\n  \ngroupId\ncom.yahoo.bullet\n/groupId\n\n  \nartifactId\nbullet-service\n/artifactId\n\n  \nversion\n${bullet.version}\n/version\n\n\n/dependency\n\n\n\n\n\nYou can also add \nclassifier\nsources\n/classifier\n  or \nclassifier\njavadoc\n/classifier\n if you want the source or javadoc.\n\n\nConfiguration\n\n\nYou specify how to talk to your Bullet instance and where to find your schema file (optional for if you want to power the \nUI\n schema with a static file) through a configuration file. See sample at \nbullet_defaults.yaml\n.\n\n\nThe values in the defaults file are used for any missing properties. You can specify a path to your custom configuration using the property:\n\n\nbullet.service.configuration.file=\npath to your configuration file\n\n\n\n\n\nFor example, if you are using Jetty as your servlet container,\n\n\njava -jar -Dbullet.service.configuration.file=/var/bullet-service/context.properties start.jar\n\n\n\n\n\n\nSpring and context\n\n\nThe Web Service uses your passed in configuration properties file to configure its dependency injections using \nSpring\n. See \nApplicationContext.xml\n for how this is loaded.\n\n\n\n\nFile based schema\n\n\nThe Web Service can also provide a endpoint that serves your data schema to your UI. You do not necessarily have to use this to serve your schema. The UI can use any JSON API schema specification. But if your schema is fixed or does not change often, it might be simpler for you to use this endpoint to provide the schema for the UI, instead of creating a new one. The Web Service also takes care to provide the right \nCORS\n headers so that your UI can communicate with it.\n\n\nYou can use \nsample_columns.json\n as a guideline for what your actual schema file should look like or if you want to create your own Web Service that dynamically serves your schema to the UI if it changes frequently.\n\n\nLaunch\n\n\nYou need to deploy the WAR file to a servlet container. We recommend \nJetty\n.\n\n\nQuick start with Jetty\n\n\n\n\nDownload a \nJetty installation\n\n\nUnarchive the installation into a folder\n\n\nDownload the Bullet Service WAR from \nJCenter\n.\n\n\nPlace the WAR into your Jetty installation folder's \nwebapps\n directory.\n\n\nCreate a properties file containing actual values for the settings in the \ndefaults\n. For example, for Bullet on Storm, you will point to your DRPC servers in the properties. If you want to use the file based schema endpoint, you would point to your schema file.\n\n\nLaunch Jetty using \njava -jar -Dbullet.service.configuration.file=/path/to/your/properties/file start.jar\n, where start.jar is in your Jetty installation folder.\n\n\n\n\nYou should tweak and properly install Jetty into a global location with proper logging when you productionize your Web Service.\n\n\nUsage\n\n\nOnce the Web Service is up (defaults to port 8080, you can change it with a \n-Djetty.http.port=\nPORT\n setting), you should be able to test to see if it's able to talk to the Bullet backend:\n\n\nYou can HTTP POST a Bullet query to the API with:\n\n\ncurl -s -H \nContent-Type: application/json\n -X POST -d '{}' http://localhost:8080/bullet-service/api/drpc\n\n\n\n\nYou should receive a random record flowing through Bullet instantly (if you left the Raw aggregation micro-batch size at the default of 1 when launching the Bullet backend).\n\n\n\n\nContext Path\n\n\nThe context path, or \"bullet-service\" in the URL above is the name of the WAR file in Jetty. If you rename it, you will need to change this.\n\n\n\n\nIf you provided a path to a schema file in your configuration file when you \nlaunch\n the Web Service, you can also HTTP GET your schema at \nhttp://localhost:8080/bullet-service/api/columns\n\n\nIf you did not, the schema in \nsample_columns.json\n is the response. The Web Service converts it to a JSON API response and provides the right headers for CORS.", 
            "title": "Setup"
        }, 
        {
            "location": "/ws/setup/#the-web-service", 
            "text": "The Web Service is a Java WAR file that you can deploy on a machine to communicate with the Bullet Backend. For Storm, it talks to the Storm DRPC servers. To set up the Bullet backend topology, see  Storm setup .  There are two main purposes for this layer at this time:  1) It provides an endpoint that can serve a  JSON API schema  for the Bullet UI. Currently, static schemas from a file are supported.  2) It proxies a JSON Bullet query to Bullet and wraps errors if the backend is unreachable.   That's it?  The Web Service essentially just wraps Storm DRPC and provides some helpful endpoints. But the Web Service is there to be a point of abstraction for implementing things like security, monitoring, access-control, rate-limiting, different query formats (e.g. SQL Bullet queries) etc, which are planned in the near future.", 
            "title": "The Web Service"
        }, 
        {
            "location": "/ws/setup/#prerequisites", 
            "text": "In order for your Web Service to work with Bullet, you should have an instance of the  backend  already set up.", 
            "title": "Prerequisites"
        }, 
        {
            "location": "/ws/setup/#installation", 
            "text": "You can download the WAR file directly from  JCenter .  If you need to depend on the source code directly for any reason, you need to add the JCenter repository and get the artifact through your dependency management system. Maven is shown below.  repositories \n     repository \n         snapshots \n             enabled false /enabled \n         /snapshots \n         id central /id \n         name bintray /name \n         url http://jcenter.bintray.com /url \n     /repository  /repositories   dependency \n   groupId com.yahoo.bullet /groupId \n   artifactId bullet-service /artifactId \n   version ${bullet.version} /version  /dependency   You can also add  classifier sources /classifier   or  classifier javadoc /classifier  if you want the source or javadoc.", 
            "title": "Installation"
        }, 
        {
            "location": "/ws/setup/#configuration", 
            "text": "You specify how to talk to your Bullet instance and where to find your schema file (optional for if you want to power the  UI  schema with a static file) through a configuration file. See sample at  bullet_defaults.yaml .  The values in the defaults file are used for any missing properties. You can specify a path to your custom configuration using the property:  bullet.service.configuration.file= path to your configuration file   For example, if you are using Jetty as your servlet container,  java -jar -Dbullet.service.configuration.file=/var/bullet-service/context.properties start.jar   Spring and context  The Web Service uses your passed in configuration properties file to configure its dependency injections using  Spring . See  ApplicationContext.xml  for how this is loaded.", 
            "title": "Configuration"
        }, 
        {
            "location": "/ws/setup/#file-based-schema", 
            "text": "The Web Service can also provide a endpoint that serves your data schema to your UI. You do not necessarily have to use this to serve your schema. The UI can use any JSON API schema specification. But if your schema is fixed or does not change often, it might be simpler for you to use this endpoint to provide the schema for the UI, instead of creating a new one. The Web Service also takes care to provide the right  CORS  headers so that your UI can communicate with it.  You can use  sample_columns.json  as a guideline for what your actual schema file should look like or if you want to create your own Web Service that dynamically serves your schema to the UI if it changes frequently.", 
            "title": "File based schema"
        }, 
        {
            "location": "/ws/setup/#launch", 
            "text": "You need to deploy the WAR file to a servlet container. We recommend  Jetty .", 
            "title": "Launch"
        }, 
        {
            "location": "/ws/setup/#quick-start-with-jetty", 
            "text": "Download a  Jetty installation  Unarchive the installation into a folder  Download the Bullet Service WAR from  JCenter .  Place the WAR into your Jetty installation folder's  webapps  directory.  Create a properties file containing actual values for the settings in the  defaults . For example, for Bullet on Storm, you will point to your DRPC servers in the properties. If you want to use the file based schema endpoint, you would point to your schema file.  Launch Jetty using  java -jar -Dbullet.service.configuration.file=/path/to/your/properties/file start.jar , where start.jar is in your Jetty installation folder.   You should tweak and properly install Jetty into a global location with proper logging when you productionize your Web Service.", 
            "title": "Quick start with Jetty"
        }, 
        {
            "location": "/ws/setup/#usage", 
            "text": "Once the Web Service is up (defaults to port 8080, you can change it with a  -Djetty.http.port= PORT  setting), you should be able to test to see if it's able to talk to the Bullet backend:  You can HTTP POST a Bullet query to the API with:  curl -s -H  Content-Type: application/json  -X POST -d '{}' http://localhost:8080/bullet-service/api/drpc  You should receive a random record flowing through Bullet instantly (if you left the Raw aggregation micro-batch size at the default of 1 when launching the Bullet backend).   Context Path  The context path, or \"bullet-service\" in the URL above is the name of the WAR file in Jetty. If you rename it, you will need to change this.   If you provided a path to a schema file in your configuration file when you  launch  the Web Service, you can also HTTP GET your schema at  http://localhost:8080/bullet-service/api/columns  If you did not, the schema in  sample_columns.json  is the response. The Web Service converts it to a JSON API response and provides the right headers for CORS.", 
            "title": "Usage"
        }, 
        {
            "location": "/ws/api/", 
            "text": "API\n\n\nSee the \nUI Usage section\n for using the UI to build Bullet queries. This section deals with examples of the JSON query format that the API currently exposes (and the UI uses underneath).\n\n\nQuerying\n\n\nBullet queries allow you to filter, project and aggregate data. It lets you fetch raw and aggregated data. Fields inside maps can be accessed using the '.' notation in queries. For example, myMap.key will access the key field inside the myMap map. There is no support for accessing fields inside Lists or inside nested Maps as of yet. Only the entire object can be operated on for now.\n\n\nThe three main sections of a Bullet query are:\n\n\n{\n    \nfilters\n: {},\n    \nprojection\n: {},\n    \naggregation\n: {}.\n    \nduration\n: 20000\n}\n\n\n\n\nThe duration represents how long the query runs for (a window from when you submit it to that many milliseconds into the future). See the \nFilters\n, \nProjections\n and \nAggregation\n sections for their respective specifications. Each of those sections are objects.\n\n\nFilters\n\n\nBullet supports two kinds of filters:\n\n\n\n\nLogical filters\n\n\nRelational filters\n\n\n\n\nLogical Filters\n\n\nLogical filters allow you to combine other filter clauses with logical operations like AND, OR and NOT.\n\n\nThe current logical operators allowed in filters are:\n\n\n\n\n\n\n\n\nLogical Operator\n\n\nMeaning\n\n\n\n\n\n\n\n\n\n\nAND\n\n\nAll filters must be true. The first false filter evaluated left to right will short-circuit the computation.\n\n\n\n\n\n\nOR\n\n\nAny filter must be true. The first true filter evaluated left to right will short-circuit the computation.\n\n\n\n\n\n\nNOT\n\n\nNegates the value of the first filter clause. The filter is satisfied iff the value is true.\n\n\n\n\n\n\n\n\nThe format for a Logical filter is:\n\n\n{\n   \noperation\n: \nAND | OR | NOT\n\n   \nclauses\n: [\n      {\noperation\n: \n...\n, clauses: [{}, ...]},\n      {\nfield\n: \n...\n, \noperation\n: \n, values: [\n...\n]},\n      {\noperation\n: \n...\n, clauses: [{}, ...]}\n      ...\n   ]\n}\n\n\n\n\nAny other type of filter may be provided as a clause in clauses.\n\n\nRelational Filters\n\n\nRelational filters allow you to specify conditions on a field, using a comparison operator and a list of values.\n\n\nThe current comparisons allowed in filters are:\n\n\n\n\n\n\n\n\nComparison\n\n\nMeaning\n\n\n\n\n\n\n\n\n\n\n==\n\n\nEqual to any value in values\n\n\n\n\n\n\n!=\n\n\nNot equal to any value in values\n\n\n\n\n\n\n=\n\n\nLess than or equal to any value in values\n\n\n\n\n\n\n=\n\n\nGreater than or equal to any value in values\n\n\n\n\n\n\n\n\nLess than any value in values\n\n\n\n\n\n\n\n\nGreater than any value in values\n\n\n\n\n\n\nRLIKE\n\n\nMatches using \nJava Regex notation\n, any Regex value in values\n\n\n\n\n\n\n\n\nThese operators are all typed based on the type of the left hand side from the Bullet record. If the elements on the right hand side cannot be\ncasted to the types on the LHS, those items will be ignored for the comparison.\n\n\nThe format for a Relational filter is:\n\n\n{\n    \noperation\n: \n== | != | \n= | \n= | \n | \n | RLIKE\n\n    \nfield\n: \nrecord_field_name | map_field.subfield\n,\n    \nvalues\n: [\n        \nstring values\n,\n        \nthat go here\n,\n        \nwill be casted\n,\n        \nto the\n,\n        \ntype of field\n\n    ]\n}\n\n\n\n\nMultiple top level relational filters behave as if they are ANDed together.\n\n\nProjections\n\n\nProjections allow you to pull out only the fields needed and rename them (renaming is being supported in order to give\nbetter names to fields pulled out from maps). If projections are not specified, the entire record is returned. If you are querying\nfor raw records, you can use projections to help reduce the load on the system and network.\n\n\n{\n    \nprojection\n: {\n        \nfieldA\n: \nnewNameA\n,\n        \nfieldB\n: \nnewNameB\n\n    }\n}\n\n\n\n\nAggregations\n\n\nAggregations allow you to perform some operation on the collected records. They take an optional size to restrict\nthe size of the aggregation (this applies for aggregations high cardinality aggregations and raw records).\n\n\nThe current aggregation types that are supported are:\n\n\n\n\n\n\n\n\nAggregation\n\n\nMeaning\n\n\n\n\n\n\n\n\n\n\nGROUP\n\n\nThe resulting output would be a record containing the result of an operation for each unique group in the specified fields\n\n\n\n\n\n\nCOUNT DISTINCT\n\n\nComputes the number of distinct elements in the fields. (May be approximate)\n\n\n\n\n\n\nLIMIT\n\n\nThe resulting output would be at most the number specified in size.\n\n\n\n\n\n\n\n\nThe current format for an aggregation is (\nnote see above for what is supported at the moment\n):\n\n\n{\n    \ntype\n: \nGROUP | COUNT DISTINCT | TOP | PERCENTILE | RAW\n,\n    \nsize\n: \na limit on the number of resulting records\n,\n    \nfields\n: {\n        \nfields\n: \nnewNameA\n,\n        \nthat go here\n: \nnewNameB\n,\n        \nare what the\n: \nnewNameC\n,\n        \naggregation type applies to\n: \nnewNameD\n\n    },\n    \nattributes\n: {\n        \nthese\n: \nchange\n,\n        \nper\n: [\n           \naggregation type\n\n        ]\n    }\n}\n\n\n\n\nYou can also use LIMIT as an alias for RAW. DISTINCT is also an alias for GROUP. These exist to make some queries read a bit better.\n\n\nCurrently we support GROUP aggregations on the following operations:\n\n\n\n\n\n\n\n\nOperation\n\n\nMeaning\n\n\n\n\n\n\n\n\n\n\nCOUNT\n\n\nComputes the number of the elements in the group\n\n\n\n\n\n\nSUM\n\n\nComputes the sum of the elements in the group\n\n\n\n\n\n\nMIN\n\n\nReturns the minimum of the elements in the group\n\n\n\n\n\n\nMAX\n\n\nReturns the maximum of the elements in the group\n\n\n\n\n\n\nAVG\n\n\nComputes the average of the elements in the group\n\n\n\n\n\n\n\n\nThe following attributes are supported for GROUP:\n\n\nAttributes for GROUP:\n\n\n    \nattributes\n: {\n        \noperations\n: [\n            {\n                \ntype\n: \nCOUNT\n,\n                \nnewName\n: \nresultColumnName\n\n            },\n            {\n                \ntype\n: \nSUM\n,\n                \nfield\n: \nfieldName\n,\n                \nnewName\n: \nresultColumnName\n\n            },\n            {\n                \ntype\n: \nMIN\n,\n                \nfield\n: \nfieldName\n,\n                \nnewName\n: \nresultColumnName\n\n            },\n            {\n                \ntype\n: \nMAX\n,\n                \nfield\n: \nfieldName\n,\n                \nnewName\n: \nresultColumnName\n\n            },\n            {\n                \ntype\n: \nAVG\n,\n                \nfield\n: \nfieldName\n,\n                \nnewName\n: \nresultColumnName\n\n            }\n        ]\n    }\n\n\n\n\nYou can perform SUM, MIN, MAX, AVG on non-numeric fields. \nBullet will attempt to cast the field to a number first.\n If it cannot, that record with the field will be ignored for the operation. For the purposes of AVG, Bullet will\nperform the average across the numeric values for a field only.\n\n\nAttributes for COUNT DISTINCT:\n\n\n    \nattributes\n: {\n        \nnewName\n: \nthe name of the resulting count column\n\n    }\n\n\n\n\nNote that the new names you specify in the fields map for aggregations do not apply. You must use the attributes here to give your resulting output count column a name.\n\n\nResults\n\n\nBullet results are JSON objects with two fields:\n\n\n\n\n\n\n\n\nField\n\n\nContents\n\n\n\n\n\n\n\n\n\n\nrecords\n\n\nThis field contains the list of matching records\n\n\n\n\n\n\nmeta\n\n\nThis field is a map that contains meta information about the query, such as the time the query was received, error data, etc. These are configurable at launch time.\n\n\n\n\n\n\n\n\nFor a detailed description of how to perform these queries and see example results, see \nExamples\n.", 
            "title": "API"
        }, 
        {
            "location": "/ws/api/#api", 
            "text": "See the  UI Usage section  for using the UI to build Bullet queries. This section deals with examples of the JSON query format that the API currently exposes (and the UI uses underneath).", 
            "title": "API"
        }, 
        {
            "location": "/ws/api/#querying", 
            "text": "Bullet queries allow you to filter, project and aggregate data. It lets you fetch raw and aggregated data. Fields inside maps can be accessed using the '.' notation in queries. For example, myMap.key will access the key field inside the myMap map. There is no support for accessing fields inside Lists or inside nested Maps as of yet. Only the entire object can be operated on for now.  The three main sections of a Bullet query are:  {\n     filters : {},\n     projection : {},\n     aggregation : {}.\n     duration : 20000\n}  The duration represents how long the query runs for (a window from when you submit it to that many milliseconds into the future). See the  Filters ,  Projections  and  Aggregation  sections for their respective specifications. Each of those sections are objects.", 
            "title": "Querying"
        }, 
        {
            "location": "/ws/api/#filters", 
            "text": "Bullet supports two kinds of filters:   Logical filters  Relational filters", 
            "title": "Filters"
        }, 
        {
            "location": "/ws/api/#logical-filters", 
            "text": "Logical filters allow you to combine other filter clauses with logical operations like AND, OR and NOT.  The current logical operators allowed in filters are:     Logical Operator  Meaning      AND  All filters must be true. The first false filter evaluated left to right will short-circuit the computation.    OR  Any filter must be true. The first true filter evaluated left to right will short-circuit the computation.    NOT  Negates the value of the first filter clause. The filter is satisfied iff the value is true.     The format for a Logical filter is:  {\n    operation :  AND | OR | NOT \n    clauses : [\n      { operation :  ... , clauses: [{}, ...]},\n      { field :  ... ,  operation :  , values: [ ... ]},\n      { operation :  ... , clauses: [{}, ...]}\n      ...\n   ]\n}  Any other type of filter may be provided as a clause in clauses.", 
            "title": "Logical Filters"
        }, 
        {
            "location": "/ws/api/#relational-filters", 
            "text": "Relational filters allow you to specify conditions on a field, using a comparison operator and a list of values.  The current comparisons allowed in filters are:     Comparison  Meaning      ==  Equal to any value in values    !=  Not equal to any value in values    =  Less than or equal to any value in values    =  Greater than or equal to any value in values     Less than any value in values     Greater than any value in values    RLIKE  Matches using  Java Regex notation , any Regex value in values     These operators are all typed based on the type of the left hand side from the Bullet record. If the elements on the right hand side cannot be\ncasted to the types on the LHS, those items will be ignored for the comparison.  The format for a Relational filter is:  {\n     operation :  == | != |  = |  = |   |   | RLIKE \n     field :  record_field_name | map_field.subfield ,\n     values : [\n         string values ,\n         that go here ,\n         will be casted ,\n         to the ,\n         type of field \n    ]\n}  Multiple top level relational filters behave as if they are ANDed together.", 
            "title": "Relational Filters"
        }, 
        {
            "location": "/ws/api/#projections", 
            "text": "Projections allow you to pull out only the fields needed and rename them (renaming is being supported in order to give\nbetter names to fields pulled out from maps). If projections are not specified, the entire record is returned. If you are querying\nfor raw records, you can use projections to help reduce the load on the system and network.  {\n     projection : {\n         fieldA :  newNameA ,\n         fieldB :  newNameB \n    }\n}", 
            "title": "Projections"
        }, 
        {
            "location": "/ws/api/#aggregations", 
            "text": "Aggregations allow you to perform some operation on the collected records. They take an optional size to restrict\nthe size of the aggregation (this applies for aggregations high cardinality aggregations and raw records).  The current aggregation types that are supported are:     Aggregation  Meaning      GROUP  The resulting output would be a record containing the result of an operation for each unique group in the specified fields    COUNT DISTINCT  Computes the number of distinct elements in the fields. (May be approximate)    LIMIT  The resulting output would be at most the number specified in size.     The current format for an aggregation is ( note see above for what is supported at the moment ):  {\n     type :  GROUP | COUNT DISTINCT | TOP | PERCENTILE | RAW ,\n     size :  a limit on the number of resulting records ,\n     fields : {\n         fields :  newNameA ,\n         that go here :  newNameB ,\n         are what the :  newNameC ,\n         aggregation type applies to :  newNameD \n    },\n     attributes : {\n         these :  change ,\n         per : [\n            aggregation type \n        ]\n    }\n}  You can also use LIMIT as an alias for RAW. DISTINCT is also an alias for GROUP. These exist to make some queries read a bit better.  Currently we support GROUP aggregations on the following operations:     Operation  Meaning      COUNT  Computes the number of the elements in the group    SUM  Computes the sum of the elements in the group    MIN  Returns the minimum of the elements in the group    MAX  Returns the maximum of the elements in the group    AVG  Computes the average of the elements in the group     The following attributes are supported for GROUP:  Attributes for GROUP:       attributes : {\n         operations : [\n            {\n                 type :  COUNT ,\n                 newName :  resultColumnName \n            },\n            {\n                 type :  SUM ,\n                 field :  fieldName ,\n                 newName :  resultColumnName \n            },\n            {\n                 type :  MIN ,\n                 field :  fieldName ,\n                 newName :  resultColumnName \n            },\n            {\n                 type :  MAX ,\n                 field :  fieldName ,\n                 newName :  resultColumnName \n            },\n            {\n                 type :  AVG ,\n                 field :  fieldName ,\n                 newName :  resultColumnName \n            }\n        ]\n    }  You can perform SUM, MIN, MAX, AVG on non-numeric fields.  Bullet will attempt to cast the field to a number first.  If it cannot, that record with the field will be ignored for the operation. For the purposes of AVG, Bullet will\nperform the average across the numeric values for a field only.  Attributes for COUNT DISTINCT:       attributes : {\n         newName :  the name of the resulting count column \n    }  Note that the new names you specify in the fields map for aggregations do not apply. You must use the attributes here to give your resulting output count column a name.", 
            "title": "Aggregations"
        }, 
        {
            "location": "/ws/api/#results", 
            "text": "Bullet results are JSON objects with two fields:     Field  Contents      records  This field contains the list of matching records    meta  This field is a map that contains meta information about the query, such as the time the query was received, error data, etc. These are configurable at launch time.     For a detailed description of how to perform these queries and see example results, see  Examples .", 
            "title": "Results"
        }, 
        {
            "location": "/ws/examples/", 
            "text": "Examples\n\n\nRather than sourcing the examples from the \nQuick Start\n, these examples are real-world ones sourced from Bullet running on raw, user events generated by instrumentation on Yahoo sites.\n\n\n\n\nDisclaimer\n\n\nThe actual data shown here has been edited and is not how actual Yahoo user events look.\n\n\n\n\n\n\nSQL translation\n\n\nFor each query, we will also rewrite it to a SQL like syntax for readability. Eventually, we plan the API to support this format as input so you could just write this in place of the JSON syntax.\n\n\n\n\nSimplest Query\n\n\nThe simplest query you can write would be:\n\n\nBullet Query\n\n\n{}\n\n\n\n\nWhile not a very useful query - this will get any one event record (no filters means that any record would be matched, no projection gets the entire record, and the default aggregation is \nLIMIT\nor \nRAW\n with size 1, default duration 30000 ms), this can be used to quickly test your connection to Bullet.\n\n\nSQL\n\n\nSELECT * FROM WINDOW(30000) LIMIT 1;\n\n\n\n\n\n\nWINDOW?\n\n\nThere is only one unified data stream in Bullet, so for clarity the \nFROM\n clause is replaced with a non-existent \nWINDOW\n function to denote the look-forward time window for the Bullet query. For the \nSQL\n examples, we pretend that this function understands various time granularities as well.\n\n\n\n\nSimple Filtering\n\n\nBullet Query\n\n\n{\n   \nfilters\n:[\n       {\n           \nfield\n:\nid\n,\n           \noperation\n:\n==\n,\n           \nvalues\n:[\n               \nbtsg8l9b234ha\n\n           ]\n       }\n    ]\n}\n\n\n\n\nSQL\n\n\nSELECT * FROM WINDOW(30s) WHERE id = \nbtsg8l9b234ha\n LIMIT 1;\n\n\n\n\nBecause of the default constraints, this query would find at most 1 record with the id matching the value provided. The record would have all its fields.\n\n\nA sample response could be (it has been edited to remove PII and other Yahoo data). The response contains a single matching record, and the associated meta information.\n\n\n{\n   \nrecords\n:[\n       {\n           \nserver_name\n:\nEDITED\n,\n           \npage_uri\n:\n/\n,\n           \nis_page_view\n:true,\n           \ndevice\n:\ntablet\n,\n           \ndebug_codes\n:{\n               \nhttp_status_code\n:\n200\n\n           },\n           \nreferrer_domain\n:\nwww.yahoo.com\n,\n           \nis_logged_in\n:true,\n           \ntimestamp\n:1446842189000,\n           \nevent_family\n:\nview\n,\n           \nid\n:\nbtsg8l9b234ha\n,\n           \nos_name\n:\nmac os\n,\n           \ndemographics\n:{\n               \nage\n : \n25\n,\n               \ngender\n : \nm\n,\n            }\n       }\n    ],\n    \nmeta\n:{\n        \nrule_id\n:1167304238598842449,\n        \nrule_body\n:\n{}\n,\n        \nrule_finish_time\n:1480723799550,\n        \nrule_receive_time\n:1480723799540\n    }\n}\n\n\n\n\nRelational Filters and Projections\n\n\nBullet Query\n\n\n{\n    \nfilters\n:[\n        {\n            \nfield\n:\nid\n,\n            \noperation\n:\n==\n,\n            \nvalues\n:[\n                \nbtsg8l9b234ha\n\n            ]\n        },\n        {\n            \nfield\n:\npage_id\n,\n            \noperation\n:\n!=\n,\n            \nvalues\n:[\n                \nnull\n\n            ]\n        }\n    ],\n    \nprojection\n:{\n        \nfields\n:{\n            \ntimestamp\n:\nts\n,\n            \ndevice_timestamp\n:\ndevice_ts\n,\n            \nevent\n:\nevent\n,\n            \npage_domain\n:\ndomain\n,\n            \npage_id\n:\nid\n\n        }\n    },\n    \naggregation\n:{\n        \ntype\n:\nRAW\n,\n        \nsize\n:10\n    },\n    \nduration\n:20000\n}\n\n\n\n\nSQL\n\n\nSELECT timestamp AS ts, device_timestamp AS device_ts, event AS event, page_domain AS domain, page_id AS id\nFROM WINDOW(20s)\nWHERE id = \nbtsg8l9b234ha\n AND page_id IS NOT NULL\nLIMIT 10;\n\n\n\n\nThe above query finds all events with id set to 'btsg8l9b234ha' and page_id is not null, projects out the fields listed above with their new names (timestamp becomes ts etc) and limits the results to at most 10 such records. \nRAW\n indicates that the complete raw record fields will be returned, and more complicated aggregations such as \nCOUNT\n or \nSUM\n will not be performed. The duration would set the query to wait at most 20 seconds for records to show up.\n\n\nThe resulting response could look like (only 3 events were generated that matched the criteria):\n\n\n{\n    \nrecords\n: [\n        {\n            \ndomain\n: \nhttp://some.url.com\n,\n            \ndevice_ts\n: 1481152233788,\n            \nid\n: 2273844742998,\n            \nevent\n: \npage\n,\n            \nts\n: null\n        },\n        {\n            \ndomain\n: \nwww.yahoo.com\n,\n            \ndevice_ts\n: 1481152233788,\n            \nid\n: 227384472956,\n            \nevent\n: \nclick\n,\n            \nts\n: 1481152233888\n        },\n        {\n            \ndomain\n: \nhttps://news.yahoo.com\n,\n            \ndevice_ts\n: null,\n            \nid\n: 2273844742556,\n            \nevent\n: \npage\n,\n            \nts\n: null\n        }\n    ],\n    \nmeta\n: {\n        \nrule_id\n: -3239746252817510000,\n        \nrule_body\n: \nRULE_BODY_EDITED_OUT\n,\n        \nrule_finish_time\n: 1481152233799,\n        \nrule_receive_time\n: 1481152233796\n    }\n}\n\n\n\n\nLogical Filters and Projections\n\n\nBullet Query\n\n\n{\n \nfilters\n: [\n                {\n                    \noperation\n: \nOR\n,\n                    \nclauses\n: [\n                        {\n                            \noperation\n: \nAND\n,\n                            \nclauses\n: [\n                                {\n                                    \nfield\n: \nid\n,\n                                    \noperation\n: \n==\n,\n                                    \nvalues\n: [\nc14plm1begla7\n]\n                                },\n                                {\n                                    \noperation\n: \nOR\n,\n                                    \nclauses\n: [\n                                        {\n                                            \noperation\n: \nAND\n,\n                                            \nclauses\n: [\n                                                {\n                                                    \nfield\n: \nexperience\n,\n                                                    \noperation\n: \n==\n,\n                                                    \nvalues\n: [\nweb\n]\n                                                },\n                                                {\n                                                    \nfield\n: \npage_id\n,\n                                                    \noperation\n: \n==\n,\n                                                    \nvalues\n: [\n18025\n, \n47729\n]\n                                                }\n                                            ]\n                                        },\n                                        {\n                                            \nfield\n: \nlink_id\n,\n                                            \noperation\n: \nRLIKE\n,\n                                            \nvalues\n: [\n2.*\n]\n                                        }\n                                    ]\n                                }\n                            ]\n                        },\n                        {\n                            \noperation\n: \nAND\n,\n                            \nclauses\n: [\n                                {\n                                    \nfield\n: \ntags.player\n,\n                                    \noperation\n: \n==\n,\n                                    \nvalues\n: [\ntrue\n]\n                                },\n                                {\n                                    \nfield\n: \ndemographics.age\n,\n                                    \noperation\n: \n,\n                                    \nvalues\n: [\n65\n]\n                                }\n                            ]\n                        }\n                    ]\n                }\n            ],\n \nprojection\n : {\n    \nfields\n: {\n        \nid\n: \nid\n,\n        \nexperience\n: \nexperience\n,\n        \npage_id\n: \npid\n,\n        \nlink_id\n: \nlid\n,\n        \ntags\n: \ntags\n,\n        \ndemographics.age\n: \nage\n\n    }\n },\n \naggregation\n: {\ntype\n : \nRAW\n, \nsize\n : 1},\n \nduration\n: 60000\n}\n\n\n\n\nSQL\n\n\nSELECT id AS id, experience AS experience, page_id AS pid,\n       link_id AS lid, tags AS tags, demographics[\nage\n] AS age\nFROM WINDOW(1min)\nWHERE (id = \nc14plm1begla7\n AND ((experience = \nweb\n AND page_id IN [\n18025\n, \n47729\n])\n                                 OR link_id MATCHES \n2.*\n))\n      OR\n      (tags[\nplayer\n] AND demographics[\nage\n] \n \n65\n)\nLIMIT 1;\n\n\n\n\n\n\nTyping\n\n\nIf demographics[\"age\"] was of type Long, then Bullet will convert 85 to be an Long, but in this example, we are pretending that it is String.  So, no conversion is made. Similarly for link_id, id, experience and page_id. tags is a Map of String to Boolean so Bullet converts \n\"true\"\n to the Boolean \ntrue\n.\n\n\n\n\nThis query is looking for a single event with a specific id and either the page_id is in two specific pages on the \"web\" experience or with a link_id that starts with 2, or a player event where the age is greater than \"65\". In other words, it is looking for senior citizens who generate video player events or a particular person's (based on id) events on two specific pages or a group of pages that have link that have ids that start with 2. It then projects out only these fields with different names.\n\n\nA sample result could look like (it matched because of tags.player was true and demographics.age was \n 65):\n\n\n{\n    \nrecords\n: [\n        {\n            \npid\n:\n158\n,\n            \nid\n:\n0qcgofdbfqs9s\n,\n            \nexperience\n:\nweb\n,\n            \nlid\n:\n978500434\n,\n            \nage\n:\n66\n,\n            \ntags\n:{\nplayer\n:true}\n        }\n    ],\n    \nmeta\n: {\n        \nrule_id\n: 3239746252812284004,\n        \nrule_body\n: \nRULE_BODY_EDITED_OUT\n,\n        \nrule_finish_time\n: 1481152233805,\n        \nrule_receive_time\n: 1481152233881\n    }\n}\n\n\n\n\nGROUP ALL COUNT Aggregation\n\n\nAn example of a query performing a COUNT all records aggregation would look like:\n\n\nBullet Query\n\n\n{\n  \nfilters\n:[\n    {\n      \nfield\n: \ndemographics.age\n,\n      \noperation\n: \n,\n      \nvalues\n: [\n65\n]\n    }\n  ],\n  \naggregation\n:{\n    \ntype\n: \nGROUP\n,\n    \nattributes\n: {\n      \noperations\n: [\n        {\n          \ntype\n: \nCOUNT\n,\n          \nnewName\n: \nnumSeniors\n\n        }\n      ]\n    }\n  },\n  \nduration\n: 20000\n}\n\n\n\n\nSQL\n\n\nSELECT COUNT(*) AS numSeniors\nFROM WINDOW(20s)\nWHERE demographics[\nage\n] \n \n65\n;\n\n\n\n\nThis query will count the number events for which demographics.age \n 65. The aggregation type GROUP indicates that it is a group aggregation. To group by a key, the \nfields\n key needs to be set in the \naggregation\n part of the query. If \nfields\n is empty or is omitted (as it is in the query above) and the \ntype\n is \nGROUP\n, it is as if all the records are collapsed into a single group - a \nGROUP ALL\n. Adding a \nCOUNT\n in the \noperations\n part of the \nattributes\n indicates that the number of records in this group will be counted, and the \"newName\" key denotes the name the resulting column \"numSeniors\" in the result. Setting the duration to 20000 counts matching records for\nthis duration.\n\n\nA sample result would look like:\n\n\n{\n    \nrecords\n: [\n        {\n            \nnumSeniors\n: 363201\n        }\n    ],\n    \nmeta\n: {}\n}\n\n\n\n\nThis result indicates that 363,201 records were counted with demographics.age \n 65 during the 20s the query was running.\n\n\nGROUP ALL Multiple Aggregations\n\n\nCOUNT is the only GROUP operation for which you can omit a \"field\".\n\n\nBullet Query\n\n\n{\n   \nfilters\n:[\n      {\n         \nfield\n: \ndemographics.state\n,\n         \noperation\n: \n==\n,\n         \nvalues\n: [\ncalifornia\n]\n      }\n   ],\n   \naggregation\n:{\n      \ntype\n: \nGROUP\n,\n      \nattributes\n: {\n        \noperations\n: [\n          {\n            \ntype\n: \nCOUNT\n,\n            \nnewName\n: \nnumCalifornians\n\n          },\n          {\n            \ntype\n: \nAVG\n,\n            \nfield\n: \ndemographics.age\n,\n            \nnewName\n: \navgAge\n\n          },\n          {\n            \ntype\n: \nMIN\n,\n            \nfield\n: \ndemographics.age\n,\n            \nnewName\n: \nminAge\n\n          },\n          {\n            \ntype\n: \nMAX\n,\n            \nfield\n: \ndemographics.age\n,\n            \nnewName\n: \nmaxAge\n\n          }\n        ]\n      }\n   },\n   \nduration\n: 20000\n}\n\n\n\n\nSQL\n\n\nSELECT COUNT(*) AS numCalifornians, AVG(demographics[\nage\n]) AS avgAge,\n       MIN(demographics[\nage\n]) AS minAge, MAX(demographics[\nage\n]) AS maxAge,\nFROM WINDOW(20s)\nWHERE demographics[\nstate\n] = \ncalifornia\n;\n\n\n\n\nA sample result would look like:\n\n\n{\n    \nrecords\n: [\n        {\n            \nmaxAge\n: 94.0,\n            \nnumCalifornians\n: 188451,\n            \nminAge\n: 6.0,\n            \navgAge\n: 33.71828\n        }\n    ],\n    \nmeta\n: {\n        \nrule_id\n: 8051040987827161000,\n        \nrule_body\n: \nRULE_BODY_EDITED_OUT\n,\n        \nrule_finish_time\n: 1482371927435,\n        \nrule_receive_time\n: 1482371916625\n    }\n}\n\n\n\n\nThis result indicates that, among the records observed during the 20s this query ran, there were 188,451 users with demographics.state equal to \"california\". Among these users the average age was 33.71828, the max age observed was 94, and the minimum age observed was 6.\n\n\nExact COUNT DISTINCT Aggregation\n\n\nBullet Query\n\n\n{\n  \naggregation\n: {\n      \ntype\n: \nCOUNT DISTINCT\n,\n      \nfields\n: {\n        \nbrowser_name\n: \n,\n        \nbrowser_version\n: \n\n      }\n    }\n}\n\n\n\n\nSQL\n\n\nSELECT COUNT(*) AS \nCOUNT DISTINCT\n\nFROM (SELECT browser_name, browser_version\n      FROM WINDOW(30s)\n      GROUP BY browser_name, browser_version) tmp;\n\n\n\n\nThis gets the count of the unique browser names and versions in the next 30s (default duration). Note that we do not specify values for the keys in fields. This is because they are not relevant\n\n\n{\n    \nrecords\n: [\n        {\n            \nCOUNT DISTINCT\n: 158.0\n        }\n    ],\n    \nmeta\n: {\n        \nrule_id\n: 4451146261377394443,\n        \naggregation\n: {\n            \nstandardDeviations\n: {\n                \n1\n: {\n                    \nupperBound\n: 158.0,\n                    \nlowerBound\n: 158.0\n                },\n                \n2\n: {\n                    \nupperBound\n: 158.0,\n                    \nlowerBound\n: 158.0\n                },\n                \n3\n: {\n                    \nupperBound\n: 158.0,\n                    \nlowerBound\n: 158.0\n                }\n            },\n            \nwasEstimated\n: false,\n            \nsketchFamily\n: \nCOMPACT\n,\n            \nsketchTheta\n: 1.0,\n            \nsketchSize\n: 1280\n        },\n        \nrule_body\n: \nRULE_BODY_EDITED_OUT\n,\n        \nrule_finish_time\n: 1484084869073,\n        \nrule_receive_time\n: 1484084832684\n    }\n}\n\n\n\n\nThere were 158 unique combinations on browser names and versions in our dataset for those 30 s. Note the new \naggregation\n object in the meta. It has various metadata about the result and Sketches. In particular, the \nwasEstimated\n key denotes where the result\nwas estimated or not. The \nstandardDeviations\n key denotes the confidence at various sigmas: 1 (1 sigma = ~68% confidence, 2 sigma = ~95% confidence, 3 sigma = ~99% confidence). Since this result was not estimated, the result is the same as the upper and lower bounds for the result.\n\n\nApproximate COUNT DISTINCT Aggregation\n\n\nBullet Query\n\n\n{\n  \naggregation\n: {\n      \ntype\n: \nCOUNT DISTINCT\n,\n      \nfields\n: {\n        \nip_address\n: \n\n      },\n      \nattributes\n: {\n        \nnewName\n: \nuniqueIPs\n\n      }\n    },\n    \nduration\n: 10000\n}\n\n\n\n\nSQL\n\n\nSELECT COUNT(*) AS uniqueIPs\nFROM (SELECT ip_address\n      FROM WINDOW(10s)\n      GROUP BY ip_address) tmp;\n\n\n\n\nThis query gets us the unique IP addresses in the next 10 s. It renames the result column from \"COUNT DISTINCT\" to \"uniqueIPs\".\n\n\n{\n    \nrecords\n: [\n        {\n            \nuniqueIPs\n: 130551.07952805843\n        }\n    ],\n    \nmeta\n: {\n        \nrule_id\n: 5377782455857451480,\n        \naggregation\n: {\n            \nstandardDeviations\n: {\n                \n1\n: {\n                    \nupperBound\n: 131512.85413760383,\n                    \nlowerBound\n: 129596.30223107953\n                },\n                \n2\n: {\n                    \nupperBound\n: 132477.15103015225,\n                    \nlowerBound\n: 128652.93906100772\n                },\n                \n3\n: {\n                    \nupperBound\n: 133448.49248615955,\n                    \nlowerBound\n: 127716.46773622213\n                }\n            },\n            \nwasEstimated\n: true,\n            \nsketchFamily\n: \nCOMPACT\n,\n            \nsketchTheta\n: 0.12549877074343688,\n            \nsketchSize\n: 131096\n        },\n        \nrule_body\n: \nRULE_BODY_EDITED_OUT\n,\n        \nrule_finish_time\n: 1484090240812,\n        \nrule_receive_time\n: 1484090223351\n    }\n}\n\n\n\n\nThe number of unique IPs in our dataset was 130551 in those 10s (approximately) with the true value between (129596, 131512) at 68% confidence, (128652, 132477) at 95% confidence and (127716, 133448) at 99% confidence. In the \nworst\n case at 3 sigma (99% confidence),\nour error is 2.17%. The final result was computed with 131096 bytes or ~128 KiB as denoted by \nsketchSize\n. This happens to be maximum size the the COUNT DISTINCT sketch will take up at the default nominal entries, so even if we had billions of unique IPs, the size will be the same and the error may be higher (depends on the distribution). For example, the error when the same query was run for 30s was 2.28% at 99% confidence (actual unique IPs: 559428, upper bound: 572514). In fact, the worst the error can get at this\nSketch size is 2.34% as defined \nhere\n, \nregardless of the number of unique entries added to the Sketch!\n.\n\n\nDISTINCT Aggregation\n\n\nBullet Query\n\n\n{\n  \naggregation\n:{\n    \ntype\n: \nDISTINCT\n,\n    \nsize\n: 10,\n    \nfields\n: {\n      \nbrowser_name\n: \nbrowser\n\n    }\n  }\n}\n\n\n\n\nSQL\n\n\nSELECT browser_name AS browser\nFROM WINDOW(30s)\nGROUP BY browser_name\nLIMIT 10;\n\n\n\n\nThis query gets the distinct values for the browser_name field and limit the results to 10. It runs for 30 s.\n\n\n{\n    \nrecords\n:[\n        {\n            \nbrowser\n:\nopera\n\n        },\n        {\n            \nbrowser\n:\nflock\n\n        },\n        {\n            \nbrowser\n:\nlinks\n\n        },\n        {\n            \nbrowser\n:\nmozilla firefox\n\n        },\n        {\n            \nbrowser\n:\ndolfin\n\n        },\n        {\n            \nbrowser\n:\nlynx\n\n        },\n        {\n            \nbrowser\n:\nchrome\n\n        },\n        {\n            \nbrowser\n:\nmicrosoft internet explorer\n\n        },\n        {\n            \nbrowser\n:\naol browser\n\n        },\n        {\n            \nbrowser\n:\nedge\n\n        }\n    ],\n    \nmeta\n:{\n        \nrule_id\n:-4872093887360741287,\n        \naggregation\n:{\n            \nstandardDeviations\n:{\n                \n1\n:{\n                    \nupperBound\n:28.0,\n                    \nlowerBound\n:28.0\n                },\n                \n2\n:{\n                    \nupperBound\n:28.0,\n                    \nlowerBound\n:28.0\n                },\n                \n3\n:{\n                    \nupperBound\n:28.0,\n                    \nlowerBound\n:28.0\n                }\n            },\n            \nwasEstimated\n:false,\n            \nuniquesEstimate\n:28.0,\n            \nsketchTheta\n:1.0\n        },\n        \nrule_body\n: \nRULE_BODY_EDITED_OUT\n,\n        \nrule_finish_time\n:1485469087971,\n        \nrule_receive_time\n:1485469054070\n    }\n}\n\n\n\n\nThere were 28 unique results but we asked for 10, so the query returned a uniform sample across the 28 distinct values.\n\n\nDISTINCT is just an alias for GROUP. A GROUP by with no operations is exactly a DISTINCT.\n\n\nGROUP by Aggregation\n\n\nBullet Query\n\n\n{\n  \nfilters\n:[\n    {\n      \nfield\n: \ndemographics\n,\n      \noperation\n: \n!=\n,\n      \nvalues\n: [\nnull\n]\n    }\n  ],\n  \naggregation\n:{\n    \ntype\n: \nGROUP\n,\n    \nsize\n: 50,\n    \nfields\n: {\n      \ndemographics.country\n: \ncountry\n,\n      \ndevice\n: \n\n    },\n    \nattributes\n: {\n      \noperations\n: [\n        {\n          \ntype\n: \nCOUNT\n,\n          \nnewName\n: \ncount\n\n        },\n        {\n          \ntype\n: \nAVG\n,\n          \nfield\n: \ndemographics.age\n,\n          \nnewName\n: \naverageAge\n\n        },\n        {\n          \ntype\n: \nAVG\n,\n          \nfield\n: \ntimespent\n,\n          \nnewName\n: \naverageTimespent\n\n        }\n      ]\n    }\n  },\n  \nduration\n: 20000\n}\n\n\n\n\nSQL\n\n\nSELECT demographics[\ncountry\n] AS country, device AS device,\n       COUNT(*) AS count, AVG(demographics[\nage\n]) AS averageAge,\n       AVG(timespent) AS averageTimespent\nFROM WINDOW(20s)\nWHERE demographics IS NOT NULL\nGROUP BY demographics[\ncountry\n], device\nLIMIT 50;\n\n\n\n\nThis query groups by the country and the device and for each unique group gets the count, average age and time spent by the users for the next 20 seconds. It renames demographics.country to country and does not rename device. It limits the groups to 50. If there were more than\n50 groups, the results would be a uniform sampling of the groups (but each group in the result would have the correct result). These parameters can all be tweaked \nin the configuration\n.\n\n\n{\n    \nrecords\n:[\n      {\n          \ncountry\n:\nuk\n,\n          \ndevice\n:\ndesktop\n,\n          \ncount\n:203034,\n          \naverageAge\n:32.42523,\n          \naverageTimespent\n:1.342\n      },\n      {\n          \ncountry\n:\nus\n,\n          \ndevice\n:\ndesktop\n,\n          \ncount\n:1934030,\n          \naverageAge\n:29.42523,\n          \naverageTimespent\n:3.234520\n      },\n      \n...and 41 other such records here\n\n  ],\n  \nmeta\n:{\n      \nrule_id\n:1705911449584057747,\n      \naggregation\n:{\n          \nstandardDeviations\n:{\n              \n1\n:{\n                  \nupperBound\n:43.0,\n                  \nlowerBound\n:43.0\n              },\n              \n2\n:{\n                  \nupperBound\n:43.0,\n                  \nlowerBound\n:43.0\n              },\n              \n3\n:{\n                  \nupperBound\n:43.0,\n                  \nlowerBound\n:43.0\n              }\n          },\n        \nwasEstimated\n:false,\n        \nuniquesEstimate\n:43.0,\n        \nsketchTheta\n:1.0\n      },\n      \nrule_body\n:\nRULE_BODY_HERE\n,\n      \nrule_finish_time\n:1485217172780,\n      \nrule_receive_time\n:1485217148840\n  }\n}\n\n\n\n\nWe received 43 rows for this result. The maximum groups that was allowed for the instance of Bullet was 512. If there were more groups than the maximum specified by your configuration, \na uniform sample\n across them would be chosen\nfor the result. However, for each group, the values computed (average, count) would be exact. The standard deviations, whether the result was estimated and the number of approximate uniques in the metadata would reflect the change.\n\n\nIf you asked for 50 rows in the aggregation (as the query did above) but there were more than 50 in the result (but \n 512), the metadata would reflect the fact that the result was not estimated. You would still get a uniform sample\nbut by increasing your aggregation size higher, you could get the rest.\n\n\nFor readability, if you were just trying to get the unique values for a field or a set of fields, you could leave out the attributes section and specify your fields section. You could also call the type \nDISTINCT\n instead of\n\nGROUP\n to make that explicit. \nDISTINCT\n is just an alias for \nGROUP\n. See \nthe DISTINCT example\n.", 
            "title": "Examples"
        }, 
        {
            "location": "/ws/examples/#examples", 
            "text": "Rather than sourcing the examples from the  Quick Start , these examples are real-world ones sourced from Bullet running on raw, user events generated by instrumentation on Yahoo sites.   Disclaimer  The actual data shown here has been edited and is not how actual Yahoo user events look.    SQL translation  For each query, we will also rewrite it to a SQL like syntax for readability. Eventually, we plan the API to support this format as input so you could just write this in place of the JSON syntax.", 
            "title": "Examples"
        }, 
        {
            "location": "/ws/examples/#simplest-query", 
            "text": "The simplest query you can write would be:  Bullet Query  {}  While not a very useful query - this will get any one event record (no filters means that any record would be matched, no projection gets the entire record, and the default aggregation is  LIMIT or  RAW  with size 1, default duration 30000 ms), this can be used to quickly test your connection to Bullet.  SQL  SELECT * FROM WINDOW(30000) LIMIT 1;   WINDOW?  There is only one unified data stream in Bullet, so for clarity the  FROM  clause is replaced with a non-existent  WINDOW  function to denote the look-forward time window for the Bullet query. For the  SQL  examples, we pretend that this function understands various time granularities as well.", 
            "title": "Simplest Query"
        }, 
        {
            "location": "/ws/examples/#simple-filtering", 
            "text": "Bullet Query  {\n    filters :[\n       {\n            field : id ,\n            operation : == ,\n            values :[\n                btsg8l9b234ha \n           ]\n       }\n    ]\n}  SQL  SELECT * FROM WINDOW(30s) WHERE id =  btsg8l9b234ha  LIMIT 1;  Because of the default constraints, this query would find at most 1 record with the id matching the value provided. The record would have all its fields.  A sample response could be (it has been edited to remove PII and other Yahoo data). The response contains a single matching record, and the associated meta information.  {\n    records :[\n       {\n            server_name : EDITED ,\n            page_uri : / ,\n            is_page_view :true,\n            device : tablet ,\n            debug_codes :{\n                http_status_code : 200 \n           },\n            referrer_domain : www.yahoo.com ,\n            is_logged_in :true,\n            timestamp :1446842189000,\n            event_family : view ,\n            id : btsg8l9b234ha ,\n            os_name : mac os ,\n            demographics :{\n                age  :  25 ,\n                gender  :  m ,\n            }\n       }\n    ],\n     meta :{\n         rule_id :1167304238598842449,\n         rule_body : {} ,\n         rule_finish_time :1480723799550,\n         rule_receive_time :1480723799540\n    }\n}", 
            "title": "Simple Filtering"
        }, 
        {
            "location": "/ws/examples/#relational-filters-and-projections", 
            "text": "Bullet Query  {\n     filters :[\n        {\n             field : id ,\n             operation : == ,\n             values :[\n                 btsg8l9b234ha \n            ]\n        },\n        {\n             field : page_id ,\n             operation : != ,\n             values :[\n                 null \n            ]\n        }\n    ],\n     projection :{\n         fields :{\n             timestamp : ts ,\n             device_timestamp : device_ts ,\n             event : event ,\n             page_domain : domain ,\n             page_id : id \n        }\n    },\n     aggregation :{\n         type : RAW ,\n         size :10\n    },\n     duration :20000\n}  SQL  SELECT timestamp AS ts, device_timestamp AS device_ts, event AS event, page_domain AS domain, page_id AS id\nFROM WINDOW(20s)\nWHERE id =  btsg8l9b234ha  AND page_id IS NOT NULL\nLIMIT 10;  The above query finds all events with id set to 'btsg8l9b234ha' and page_id is not null, projects out the fields listed above with their new names (timestamp becomes ts etc) and limits the results to at most 10 such records.  RAW  indicates that the complete raw record fields will be returned, and more complicated aggregations such as  COUNT  or  SUM  will not be performed. The duration would set the query to wait at most 20 seconds for records to show up.  The resulting response could look like (only 3 events were generated that matched the criteria):  {\n     records : [\n        {\n             domain :  http://some.url.com ,\n             device_ts : 1481152233788,\n             id : 2273844742998,\n             event :  page ,\n             ts : null\n        },\n        {\n             domain :  www.yahoo.com ,\n             device_ts : 1481152233788,\n             id : 227384472956,\n             event :  click ,\n             ts : 1481152233888\n        },\n        {\n             domain :  https://news.yahoo.com ,\n             device_ts : null,\n             id : 2273844742556,\n             event :  page ,\n             ts : null\n        }\n    ],\n     meta : {\n         rule_id : -3239746252817510000,\n         rule_body :  RULE_BODY_EDITED_OUT ,\n         rule_finish_time : 1481152233799,\n         rule_receive_time : 1481152233796\n    }\n}", 
            "title": "Relational Filters and Projections"
        }, 
        {
            "location": "/ws/examples/#logical-filters-and-projections", 
            "text": "Bullet Query  {\n  filters : [\n                {\n                     operation :  OR ,\n                     clauses : [\n                        {\n                             operation :  AND ,\n                             clauses : [\n                                {\n                                     field :  id ,\n                                     operation :  == ,\n                                     values : [ c14plm1begla7 ]\n                                },\n                                {\n                                     operation :  OR ,\n                                     clauses : [\n                                        {\n                                             operation :  AND ,\n                                             clauses : [\n                                                {\n                                                     field :  experience ,\n                                                     operation :  == ,\n                                                     values : [ web ]\n                                                },\n                                                {\n                                                     field :  page_id ,\n                                                     operation :  == ,\n                                                     values : [ 18025 ,  47729 ]\n                                                }\n                                            ]\n                                        },\n                                        {\n                                             field :  link_id ,\n                                             operation :  RLIKE ,\n                                             values : [ 2.* ]\n                                        }\n                                    ]\n                                }\n                            ]\n                        },\n                        {\n                             operation :  AND ,\n                             clauses : [\n                                {\n                                     field :  tags.player ,\n                                     operation :  == ,\n                                     values : [ true ]\n                                },\n                                {\n                                     field :  demographics.age ,\n                                     operation :  ,\n                                     values : [ 65 ]\n                                }\n                            ]\n                        }\n                    ]\n                }\n            ],\n  projection  : {\n     fields : {\n         id :  id ,\n         experience :  experience ,\n         page_id :  pid ,\n         link_id :  lid ,\n         tags :  tags ,\n         demographics.age :  age \n    }\n },\n  aggregation : { type  :  RAW ,  size  : 1},\n  duration : 60000\n}  SQL  SELECT id AS id, experience AS experience, page_id AS pid,\n       link_id AS lid, tags AS tags, demographics[ age ] AS age\nFROM WINDOW(1min)\nWHERE (id =  c14plm1begla7  AND ((experience =  web  AND page_id IN [ 18025 ,  47729 ])\n                                 OR link_id MATCHES  2.* ))\n      OR\n      (tags[ player ] AND demographics[ age ]    65 )\nLIMIT 1;   Typing  If demographics[\"age\"] was of type Long, then Bullet will convert 85 to be an Long, but in this example, we are pretending that it is String.  So, no conversion is made. Similarly for link_id, id, experience and page_id. tags is a Map of String to Boolean so Bullet converts  \"true\"  to the Boolean  true .   This query is looking for a single event with a specific id and either the page_id is in two specific pages on the \"web\" experience or with a link_id that starts with 2, or a player event where the age is greater than \"65\". In other words, it is looking for senior citizens who generate video player events or a particular person's (based on id) events on two specific pages or a group of pages that have link that have ids that start with 2. It then projects out only these fields with different names.  A sample result could look like (it matched because of tags.player was true and demographics.age was   65):  {\n     records : [\n        {\n             pid : 158 ,\n             id : 0qcgofdbfqs9s ,\n             experience : web ,\n             lid : 978500434 ,\n             age : 66 ,\n             tags :{ player :true}\n        }\n    ],\n     meta : {\n         rule_id : 3239746252812284004,\n         rule_body :  RULE_BODY_EDITED_OUT ,\n         rule_finish_time : 1481152233805,\n         rule_receive_time : 1481152233881\n    }\n}", 
            "title": "Logical Filters and Projections"
        }, 
        {
            "location": "/ws/examples/#group-all-count-aggregation", 
            "text": "An example of a query performing a COUNT all records aggregation would look like:  Bullet Query  {\n   filters :[\n    {\n       field :  demographics.age ,\n       operation :  ,\n       values : [ 65 ]\n    }\n  ],\n   aggregation :{\n     type :  GROUP ,\n     attributes : {\n       operations : [\n        {\n           type :  COUNT ,\n           newName :  numSeniors \n        }\n      ]\n    }\n  },\n   duration : 20000\n}  SQL  SELECT COUNT(*) AS numSeniors\nFROM WINDOW(20s)\nWHERE demographics[ age ]    65 ;  This query will count the number events for which demographics.age   65. The aggregation type GROUP indicates that it is a group aggregation. To group by a key, the  fields  key needs to be set in the  aggregation  part of the query. If  fields  is empty or is omitted (as it is in the query above) and the  type  is  GROUP , it is as if all the records are collapsed into a single group - a  GROUP ALL . Adding a  COUNT  in the  operations  part of the  attributes  indicates that the number of records in this group will be counted, and the \"newName\" key denotes the name the resulting column \"numSeniors\" in the result. Setting the duration to 20000 counts matching records for\nthis duration.  A sample result would look like:  {\n     records : [\n        {\n             numSeniors : 363201\n        }\n    ],\n     meta : {}\n}  This result indicates that 363,201 records were counted with demographics.age   65 during the 20s the query was running.", 
            "title": "GROUP ALL COUNT Aggregation"
        }, 
        {
            "location": "/ws/examples/#group-all-multiple-aggregations", 
            "text": "COUNT is the only GROUP operation for which you can omit a \"field\".  Bullet Query  {\n    filters :[\n      {\n          field :  demographics.state ,\n          operation :  == ,\n          values : [ california ]\n      }\n   ],\n    aggregation :{\n       type :  GROUP ,\n       attributes : {\n         operations : [\n          {\n             type :  COUNT ,\n             newName :  numCalifornians \n          },\n          {\n             type :  AVG ,\n             field :  demographics.age ,\n             newName :  avgAge \n          },\n          {\n             type :  MIN ,\n             field :  demographics.age ,\n             newName :  minAge \n          },\n          {\n             type :  MAX ,\n             field :  demographics.age ,\n             newName :  maxAge \n          }\n        ]\n      }\n   },\n    duration : 20000\n}  SQL  SELECT COUNT(*) AS numCalifornians, AVG(demographics[ age ]) AS avgAge,\n       MIN(demographics[ age ]) AS minAge, MAX(demographics[ age ]) AS maxAge,\nFROM WINDOW(20s)\nWHERE demographics[ state ] =  california ;  A sample result would look like:  {\n     records : [\n        {\n             maxAge : 94.0,\n             numCalifornians : 188451,\n             minAge : 6.0,\n             avgAge : 33.71828\n        }\n    ],\n     meta : {\n         rule_id : 8051040987827161000,\n         rule_body :  RULE_BODY_EDITED_OUT ,\n         rule_finish_time : 1482371927435,\n         rule_receive_time : 1482371916625\n    }\n}  This result indicates that, among the records observed during the 20s this query ran, there were 188,451 users with demographics.state equal to \"california\". Among these users the average age was 33.71828, the max age observed was 94, and the minimum age observed was 6.", 
            "title": "GROUP ALL Multiple Aggregations"
        }, 
        {
            "location": "/ws/examples/#exact-count-distinct-aggregation", 
            "text": "Bullet Query  {\n   aggregation : {\n       type :  COUNT DISTINCT ,\n       fields : {\n         browser_name :  ,\n         browser_version :  \n      }\n    }\n}  SQL  SELECT COUNT(*) AS  COUNT DISTINCT \nFROM (SELECT browser_name, browser_version\n      FROM WINDOW(30s)\n      GROUP BY browser_name, browser_version) tmp;  This gets the count of the unique browser names and versions in the next 30s (default duration). Note that we do not specify values for the keys in fields. This is because they are not relevant  {\n     records : [\n        {\n             COUNT DISTINCT : 158.0\n        }\n    ],\n     meta : {\n         rule_id : 4451146261377394443,\n         aggregation : {\n             standardDeviations : {\n                 1 : {\n                     upperBound : 158.0,\n                     lowerBound : 158.0\n                },\n                 2 : {\n                     upperBound : 158.0,\n                     lowerBound : 158.0\n                },\n                 3 : {\n                     upperBound : 158.0,\n                     lowerBound : 158.0\n                }\n            },\n             wasEstimated : false,\n             sketchFamily :  COMPACT ,\n             sketchTheta : 1.0,\n             sketchSize : 1280\n        },\n         rule_body :  RULE_BODY_EDITED_OUT ,\n         rule_finish_time : 1484084869073,\n         rule_receive_time : 1484084832684\n    }\n}  There were 158 unique combinations on browser names and versions in our dataset for those 30 s. Note the new  aggregation  object in the meta. It has various metadata about the result and Sketches. In particular, the  wasEstimated  key denotes where the result\nwas estimated or not. The  standardDeviations  key denotes the confidence at various sigmas: 1 (1 sigma = ~68% confidence, 2 sigma = ~95% confidence, 3 sigma = ~99% confidence). Since this result was not estimated, the result is the same as the upper and lower bounds for the result.", 
            "title": "Exact COUNT DISTINCT Aggregation"
        }, 
        {
            "location": "/ws/examples/#approximate-count-distinct-aggregation", 
            "text": "Bullet Query  {\n   aggregation : {\n       type :  COUNT DISTINCT ,\n       fields : {\n         ip_address :  \n      },\n       attributes : {\n         newName :  uniqueIPs \n      }\n    },\n     duration : 10000\n}  SQL  SELECT COUNT(*) AS uniqueIPs\nFROM (SELECT ip_address\n      FROM WINDOW(10s)\n      GROUP BY ip_address) tmp;  This query gets us the unique IP addresses in the next 10 s. It renames the result column from \"COUNT DISTINCT\" to \"uniqueIPs\".  {\n     records : [\n        {\n             uniqueIPs : 130551.07952805843\n        }\n    ],\n     meta : {\n         rule_id : 5377782455857451480,\n         aggregation : {\n             standardDeviations : {\n                 1 : {\n                     upperBound : 131512.85413760383,\n                     lowerBound : 129596.30223107953\n                },\n                 2 : {\n                     upperBound : 132477.15103015225,\n                     lowerBound : 128652.93906100772\n                },\n                 3 : {\n                     upperBound : 133448.49248615955,\n                     lowerBound : 127716.46773622213\n                }\n            },\n             wasEstimated : true,\n             sketchFamily :  COMPACT ,\n             sketchTheta : 0.12549877074343688,\n             sketchSize : 131096\n        },\n         rule_body :  RULE_BODY_EDITED_OUT ,\n         rule_finish_time : 1484090240812,\n         rule_receive_time : 1484090223351\n    }\n}  The number of unique IPs in our dataset was 130551 in those 10s (approximately) with the true value between (129596, 131512) at 68% confidence, (128652, 132477) at 95% confidence and (127716, 133448) at 99% confidence. In the  worst  case at 3 sigma (99% confidence),\nour error is 2.17%. The final result was computed with 131096 bytes or ~128 KiB as denoted by  sketchSize . This happens to be maximum size the the COUNT DISTINCT sketch will take up at the default nominal entries, so even if we had billions of unique IPs, the size will be the same and the error may be higher (depends on the distribution). For example, the error when the same query was run for 30s was 2.28% at 99% confidence (actual unique IPs: 559428, upper bound: 572514). In fact, the worst the error can get at this\nSketch size is 2.34% as defined  here ,  regardless of the number of unique entries added to the Sketch! .", 
            "title": "Approximate COUNT DISTINCT Aggregation"
        }, 
        {
            "location": "/ws/examples/#distinct-aggregation", 
            "text": "Bullet Query  {\n   aggregation :{\n     type :  DISTINCT ,\n     size : 10,\n     fields : {\n       browser_name :  browser \n    }\n  }\n}  SQL  SELECT browser_name AS browser\nFROM WINDOW(30s)\nGROUP BY browser_name\nLIMIT 10;  This query gets the distinct values for the browser_name field and limit the results to 10. It runs for 30 s.  {\n     records :[\n        {\n             browser : opera \n        },\n        {\n             browser : flock \n        },\n        {\n             browser : links \n        },\n        {\n             browser : mozilla firefox \n        },\n        {\n             browser : dolfin \n        },\n        {\n             browser : lynx \n        },\n        {\n             browser : chrome \n        },\n        {\n             browser : microsoft internet explorer \n        },\n        {\n             browser : aol browser \n        },\n        {\n             browser : edge \n        }\n    ],\n     meta :{\n         rule_id :-4872093887360741287,\n         aggregation :{\n             standardDeviations :{\n                 1 :{\n                     upperBound :28.0,\n                     lowerBound :28.0\n                },\n                 2 :{\n                     upperBound :28.0,\n                     lowerBound :28.0\n                },\n                 3 :{\n                     upperBound :28.0,\n                     lowerBound :28.0\n                }\n            },\n             wasEstimated :false,\n             uniquesEstimate :28.0,\n             sketchTheta :1.0\n        },\n         rule_body :  RULE_BODY_EDITED_OUT ,\n         rule_finish_time :1485469087971,\n         rule_receive_time :1485469054070\n    }\n}  There were 28 unique results but we asked for 10, so the query returned a uniform sample across the 28 distinct values.  DISTINCT is just an alias for GROUP. A GROUP by with no operations is exactly a DISTINCT.", 
            "title": "DISTINCT Aggregation"
        }, 
        {
            "location": "/ws/examples/#group-by-aggregation", 
            "text": "Bullet Query  {\n   filters :[\n    {\n       field :  demographics ,\n       operation :  != ,\n       values : [ null ]\n    }\n  ],\n   aggregation :{\n     type :  GROUP ,\n     size : 50,\n     fields : {\n       demographics.country :  country ,\n       device :  \n    },\n     attributes : {\n       operations : [\n        {\n           type :  COUNT ,\n           newName :  count \n        },\n        {\n           type :  AVG ,\n           field :  demographics.age ,\n           newName :  averageAge \n        },\n        {\n           type :  AVG ,\n           field :  timespent ,\n           newName :  averageTimespent \n        }\n      ]\n    }\n  },\n   duration : 20000\n}  SQL  SELECT demographics[ country ] AS country, device AS device,\n       COUNT(*) AS count, AVG(demographics[ age ]) AS averageAge,\n       AVG(timespent) AS averageTimespent\nFROM WINDOW(20s)\nWHERE demographics IS NOT NULL\nGROUP BY demographics[ country ], device\nLIMIT 50;  This query groups by the country and the device and for each unique group gets the count, average age and time spent by the users for the next 20 seconds. It renames demographics.country to country and does not rename device. It limits the groups to 50. If there were more than\n50 groups, the results would be a uniform sampling of the groups (but each group in the result would have the correct result). These parameters can all be tweaked  in the configuration .  {\n     records :[\n      {\n           country : uk ,\n           device : desktop ,\n           count :203034,\n           averageAge :32.42523,\n           averageTimespent :1.342\n      },\n      {\n           country : us ,\n           device : desktop ,\n           count :1934030,\n           averageAge :29.42523,\n           averageTimespent :3.234520\n      },\n       ...and 41 other such records here \n  ],\n   meta :{\n       rule_id :1705911449584057747,\n       aggregation :{\n           standardDeviations :{\n               1 :{\n                   upperBound :43.0,\n                   lowerBound :43.0\n              },\n               2 :{\n                   upperBound :43.0,\n                   lowerBound :43.0\n              },\n               3 :{\n                   upperBound :43.0,\n                   lowerBound :43.0\n              }\n          },\n         wasEstimated :false,\n         uniquesEstimate :43.0,\n         sketchTheta :1.0\n      },\n       rule_body : RULE_BODY_HERE ,\n       rule_finish_time :1485217172780,\n       rule_receive_time :1485217148840\n  }\n}  We received 43 rows for this result. The maximum groups that was allowed for the instance of Bullet was 512. If there were more groups than the maximum specified by your configuration,  a uniform sample  across them would be chosen\nfor the result. However, for each group, the values computed (average, count) would be exact. The standard deviations, whether the result was estimated and the number of approximate uniques in the metadata would reflect the change.  If you asked for 50 rows in the aggregation (as the query did above) but there were more than 50 in the result (but   512), the metadata would reflect the fact that the result was not estimated. You would still get a uniform sample\nbut by increasing your aggregation size higher, you could get the rest.  For readability, if you were just trying to get the unique values for a field or a set of fields, you could leave out the attributes section and specify your fields section. You could also call the type  DISTINCT  instead of GROUP  to make that explicit.  DISTINCT  is just an alias for  GROUP . See  the DISTINCT example .", 
            "title": "GROUP by Aggregation"
        }, 
        {
            "location": "/ui/setup/", 
            "text": "The UI Layer\n\n\nThe Bullet UI lets you easily create and work with Bullet queries and results for your custom data. It stores all created queries, results and other metadata in the local browser storage or \nLocalStorage\n.\n\n\nWhile LocalStorage is sufficient for simple usage, UI users can run out of space when a lot of queries and results are being stored. We are looking into more robust solutions like \nLocalForage\n. See \n#9\n. This should be landing soon\n.\n\n\n\n\nReally!? LocalStorage only!?\n\n\nWe're serious about the no persistence thing with Bullet! And while we're at it, we are also not interested in supporting old browsers. Joking aside though, we wanted to keep Bullet as light and simple as possible to start with. We can look into extending support from the server-side by adding a database or the like if needed. In practice, we have found that this isn't as important as it initially seems.\n\n\n\n\nPrerequisites\n\n\nIn order for your UI to work with Bullet, you should have:\n\n\n\n\nAn instance of the \nbackend\n set up\n\n\nAn instance of the \nWeb Service\n set up\n\n\nYou should also have a Web Service serving your schema (either by using the \nfile based serving\n from the Web Service or your own somewhere else)\n\n\n\n\nInstallation\n\n\nWe are considering various packaging options at the moment like Docker etc. In the meantime, the following two options are available:\n\n\nGitHub Releases\n\n\n\n\nHead to the \nReleases page\n and grab the latest release\n\n\nDownload the bullet-ui-vX.X.X.tar.gz archive\n\n\nUnarchive it into your web server where you wish to run the UI.\n\n\nInstall \nNode\n (recommend using \nnvm\n to manage Node versions) on the web server\n\n\n\n\nBuild from source\n\n\n\n\nInstall \nNode\n (recommend using \nnvm\n to manage Node versions).\n\n\nInstall \nBower\n. Use NPM to install it with \nsudo npm install -g bower\n\n\nInstall \nEmber\n. \nsudo npm install -g ember-cli\n (sudo required only if not using nvm)\n\n\ngit clone git@github.com:yahoo/bullet-ui.git\n\n\ncd bullet-ui\n\n\nnpm install\n\n\nbower install\n\n\nember build --environment production\n\n\n\n\nThe entire application with all its assets and dependencies are compiled and placed into dist/. You could point a web server directly at this folder but you will \nonly\n be able to use the default configuration (see \nbelow\n).\n\n\nRunning\n\n\nThere is a Node.js server endpoint defined at \nserver/index.js\n to serve the UI. This dynamically injects the settings (see configuration \nbelow\n) into the served UI based on the environment variable NODE_ENV. You should not need to worry about if you only have one environment.\n\n\nThe entry-point for the UI is the \nExpress\n endpoint defined as the main in package.json that simply adds the server/index.js as a middleware.\n\n\nRegardless of which \ninstallation\n option you chose, you need the following folder structure in order to run the UI:\n\n\ndist/*\nconfig/env-settings.json\nserver/index.js\nexpress-server.js\n\n\n\n\nYou can use node to launch the UI from the top-level of the folder structure above.\n\n\nTo launch the UI with the default settings (without specifying proper API endpoints you will not be able to create or run a query):\n\n\nPORT=8800 node express-server.js\n\n\n\n\nTo launch with custom settings:\n\n\nNODE_ENV=\nyour_property_name_from_env-settings.json\n PORT=8800 node express-server.js\n\n\n\n\nVisit localhost:8800 to see your UI that should be configured with the right settings.\n\n\nConfiguration\n\n\nAll of the configuration for the UI is \nenvironment-specific\n. This lets you have different instances of Bullet for different environments (e.g. CI, Staging, Production). These settings can be found in \nenv-settings.json\n.\n\n\nEach property in the env-settings.json file will contain the settings that will be used when running a custom instance of the UI (see \nabove\n).\n\n\nThe \ndefault\n property shows the default settings for the UI that can be selectively overridden based on which host you are running on. The file does not specify the \ndefaultFilter\n setting shown below.\n\n\ndefault\n: {\n  \ndrpcHost\n: \nhttp://foo.bar.com:4080\n,\n  \ndrpcNamespace\n: \nbullet/api\n,\n  \ndrpcPath\n: \ndrpc\n,\n  \nschemaHost\n: \nhttp://foo.bar.com:4080\n,\n  \nschemaNamespace\n: \nbullet/api\n,\n  \nhelpLinks\n: [\n    {\n      \nname\n: \nExample Docs Page\n,\n      \nlink\n: \n\n    }\n  ],\n  \ndefaultFilter\n: {\n      \nclauses\n: [\n          {\n              \nfield\n: \nprimary_key\n,\n              \nvalues\n:[\n123123123321321\n],\n              \noperation\n:\n==\n\n          }\n      ],\n      \noperation\n:\nAND\n\n  },\n  \naggregateDataDefaultSize\n: 512,\n  \nmodelVersion\n: 1\n}\n\n\n\n\nYou can add more configuration at the top level for each host you have the UI running on.\n\n\ndrpcHost\n is the end point (port included) of your Web Service machine that is proxying to the Bullet topology.\n\n\ndrpcNamespace\n is the fragment of the path to your Web Service on the \ndrpcHost\n.\n\n\nschemaHost\n is the end point (port included) of your Web Service machine that is serving your schema in the JSON API format (see the \nWeb Service setup\n for details.)\n\n\nschemaNamespace\n is the fragment of the path to your schema Web Service on the \nschemaHost\n. There is no \nschemaPath\n because it \nmust\n be \"columns\" in order for the UI to be able fetch the column resource (columns in your schema).\n\n\nmodelVersion\n is a way for you to control your UI users' Ember models saved in LocalStorage. If there is a need for you to purge all your user's created queries, results and other data stored in their LocalStorage, then you should increment this number. The UI, on startup, will compare this number with what it has seen before (your old version) and purge the LocalStorage.\n\n\nhelpLinks\n is a list of objects, where each object is a help link. These links drive the drop-down list when you click the \"Help\" button on the UI's top navbar. You can use this to point to your particular help links. For example, you could use this to point your users toward a page that\nhelps them understand your data (that this UI is operating on).\n\n\ndefaultFilter\n can either be an \nAPI Filter\n or a URL from which one could be fetched dynamically. The UI adds this filter to every newly created Query. You could use this as a way to have user specific (for example, cookie based) filters created for your users when they create a new query in the UI.\n\n\nbugLink\n is a URL that by default points to the issues page for the UI GitHub repository (this). You can change it to point to your own custom JIRA queue or the like if you want to.\n\n\naggregateDataDefaultSize\n is the aggregation size for all queries that are not pulling raw data. In order to keep the aggregation size from being ambiguous for UI users when doing a Count Distinct or a Distinct or a Group By query, this is the size that is used. You should set this to your max size that you have configured for your non-raw aggregations in your topology configuration.\n\n\n\n\nCORS\n\n\nAll your Web Service endpoints must support CORS (return the right headers) in order for the UI to be able to communicate with it. The Bullet Web Service already does this for the DRPC and columns endpoints.\n\n\n\n\nTo cement all this, if you wanted an instance of the UI in your CI environment, you could add this to the env-settings.json file.\n\n\n{\n  \ndefault\n: {\n      \ndrpcHost\n: \n,\n      \ndrpcNamespace\n: \nbullet/api\n,\n      \ndrpcPath\n: \ndrpc\n,\n      \nschemaHost\n: \n,\n      \nschemaNamespace\n: \nbullet/api\n,\n      \nhelpLinks\n: [\n        {\n          \nname\n: \nData Documentation\n,\n          \nlink\n: \nhttp://data.docs.domain.com\n\n        }\n      ],\n      \nbugLink\n: \nhttp://your.issues.page.com\n,\n      \naggregateDataDefaultSize\n: 500,\n      \nmodelVersion\n: 1\n  },\n   \nci\n: {\n        \ndrpcHost\n: \nhttp://bullet-ws.development.domain.com:4080\n,\n        \nschemaHost\n: \nhttp://bullet-ws.development.domain.com:4080\n,\n        \ndefaultFilter\n: \nhttp://bullet-ws.development.domain.com:4080/custom-endpoint/api/defaultQuery\n\n      }\n}\n\n\n\n\nYour UI on your CI environment will:\n\n\n\n\nPOST to \nhttp://bullet-ws.development.domain.com:4080/bullet/api/drpc\n for UI created Bullet queries\n\n\nGET the schema from \nhttp://bullet-ws.development.domain.com:4080/bullet/api/columns\n\n\nPopulate an additional link on the Help drop-down pointing to \nhttp://data.docs.domain.com\n\n\nGET and cache a defaultFilter from \nhttp://bullet-ws.development.domain.com:4080/custom-endpoint/api/defaultQuery", 
            "title": "Setup"
        }, 
        {
            "location": "/ui/setup/#the-ui-layer", 
            "text": "The Bullet UI lets you easily create and work with Bullet queries and results for your custom data. It stores all created queries, results and other metadata in the local browser storage or  LocalStorage .  While LocalStorage is sufficient for simple usage, UI users can run out of space when a lot of queries and results are being stored. We are looking into more robust solutions like  LocalForage . See  #9 . This should be landing soon .   Really!? LocalStorage only!?  We're serious about the no persistence thing with Bullet! And while we're at it, we are also not interested in supporting old browsers. Joking aside though, we wanted to keep Bullet as light and simple as possible to start with. We can look into extending support from the server-side by adding a database or the like if needed. In practice, we have found that this isn't as important as it initially seems.", 
            "title": "The UI Layer"
        }, 
        {
            "location": "/ui/setup/#prerequisites", 
            "text": "In order for your UI to work with Bullet, you should have:   An instance of the  backend  set up  An instance of the  Web Service  set up  You should also have a Web Service serving your schema (either by using the  file based serving  from the Web Service or your own somewhere else)", 
            "title": "Prerequisites"
        }, 
        {
            "location": "/ui/setup/#installation", 
            "text": "We are considering various packaging options at the moment like Docker etc. In the meantime, the following two options are available:", 
            "title": "Installation"
        }, 
        {
            "location": "/ui/setup/#github-releases", 
            "text": "Head to the  Releases page  and grab the latest release  Download the bullet-ui-vX.X.X.tar.gz archive  Unarchive it into your web server where you wish to run the UI.  Install  Node  (recommend using  nvm  to manage Node versions) on the web server", 
            "title": "GitHub Releases"
        }, 
        {
            "location": "/ui/setup/#build-from-source", 
            "text": "Install  Node  (recommend using  nvm  to manage Node versions).  Install  Bower . Use NPM to install it with  sudo npm install -g bower  Install  Ember .  sudo npm install -g ember-cli  (sudo required only if not using nvm)  git clone git@github.com:yahoo/bullet-ui.git  cd bullet-ui  npm install  bower install  ember build --environment production   The entire application with all its assets and dependencies are compiled and placed into dist/. You could point a web server directly at this folder but you will  only  be able to use the default configuration (see  below ).", 
            "title": "Build from source"
        }, 
        {
            "location": "/ui/setup/#running", 
            "text": "There is a Node.js server endpoint defined at  server/index.js  to serve the UI. This dynamically injects the settings (see configuration  below ) into the served UI based on the environment variable NODE_ENV. You should not need to worry about if you only have one environment.  The entry-point for the UI is the  Express  endpoint defined as the main in package.json that simply adds the server/index.js as a middleware.  Regardless of which  installation  option you chose, you need the following folder structure in order to run the UI:  dist/*\nconfig/env-settings.json\nserver/index.js\nexpress-server.js  You can use node to launch the UI from the top-level of the folder structure above.  To launch the UI with the default settings (without specifying proper API endpoints you will not be able to create or run a query):  PORT=8800 node express-server.js  To launch with custom settings:  NODE_ENV= your_property_name_from_env-settings.json  PORT=8800 node express-server.js  Visit localhost:8800 to see your UI that should be configured with the right settings.", 
            "title": "Running"
        }, 
        {
            "location": "/ui/setup/#configuration", 
            "text": "All of the configuration for the UI is  environment-specific . This lets you have different instances of Bullet for different environments (e.g. CI, Staging, Production). These settings can be found in  env-settings.json .  Each property in the env-settings.json file will contain the settings that will be used when running a custom instance of the UI (see  above ).  The  default  property shows the default settings for the UI that can be selectively overridden based on which host you are running on. The file does not specify the  defaultFilter  setting shown below.  default : {\n   drpcHost :  http://foo.bar.com:4080 ,\n   drpcNamespace :  bullet/api ,\n   drpcPath :  drpc ,\n   schemaHost :  http://foo.bar.com:4080 ,\n   schemaNamespace :  bullet/api ,\n   helpLinks : [\n    {\n       name :  Example Docs Page ,\n       link :  \n    }\n  ],\n   defaultFilter : {\n       clauses : [\n          {\n               field :  primary_key ,\n               values :[ 123123123321321 ],\n               operation : == \n          }\n      ],\n       operation : AND \n  },\n   aggregateDataDefaultSize : 512,\n   modelVersion : 1\n}  You can add more configuration at the top level for each host you have the UI running on.  drpcHost  is the end point (port included) of your Web Service machine that is proxying to the Bullet topology.  drpcNamespace  is the fragment of the path to your Web Service on the  drpcHost .  schemaHost  is the end point (port included) of your Web Service machine that is serving your schema in the JSON API format (see the  Web Service setup  for details.)  schemaNamespace  is the fragment of the path to your schema Web Service on the  schemaHost . There is no  schemaPath  because it  must  be \"columns\" in order for the UI to be able fetch the column resource (columns in your schema).  modelVersion  is a way for you to control your UI users' Ember models saved in LocalStorage. If there is a need for you to purge all your user's created queries, results and other data stored in their LocalStorage, then you should increment this number. The UI, on startup, will compare this number with what it has seen before (your old version) and purge the LocalStorage.  helpLinks  is a list of objects, where each object is a help link. These links drive the drop-down list when you click the \"Help\" button on the UI's top navbar. You can use this to point to your particular help links. For example, you could use this to point your users toward a page that\nhelps them understand your data (that this UI is operating on).  defaultFilter  can either be an  API Filter  or a URL from which one could be fetched dynamically. The UI adds this filter to every newly created Query. You could use this as a way to have user specific (for example, cookie based) filters created for your users when they create a new query in the UI.  bugLink  is a URL that by default points to the issues page for the UI GitHub repository (this). You can change it to point to your own custom JIRA queue or the like if you want to.  aggregateDataDefaultSize  is the aggregation size for all queries that are not pulling raw data. In order to keep the aggregation size from being ambiguous for UI users when doing a Count Distinct or a Distinct or a Group By query, this is the size that is used. You should set this to your max size that you have configured for your non-raw aggregations in your topology configuration.   CORS  All your Web Service endpoints must support CORS (return the right headers) in order for the UI to be able to communicate with it. The Bullet Web Service already does this for the DRPC and columns endpoints.   To cement all this, if you wanted an instance of the UI in your CI environment, you could add this to the env-settings.json file.  {\n   default : {\n       drpcHost :  ,\n       drpcNamespace :  bullet/api ,\n       drpcPath :  drpc ,\n       schemaHost :  ,\n       schemaNamespace :  bullet/api ,\n       helpLinks : [\n        {\n           name :  Data Documentation ,\n           link :  http://data.docs.domain.com \n        }\n      ],\n       bugLink :  http://your.issues.page.com ,\n       aggregateDataDefaultSize : 500,\n       modelVersion : 1\n  },\n    ci : {\n         drpcHost :  http://bullet-ws.development.domain.com:4080 ,\n         schemaHost :  http://bullet-ws.development.domain.com:4080 ,\n         defaultFilter :  http://bullet-ws.development.domain.com:4080/custom-endpoint/api/defaultQuery \n      }\n}  Your UI on your CI environment will:   POST to  http://bullet-ws.development.domain.com:4080/bullet/api/drpc  for UI created Bullet queries  GET the schema from  http://bullet-ws.development.domain.com:4080/bullet/api/columns  Populate an additional link on the Help drop-down pointing to  http://data.docs.domain.com  GET and cache a defaultFilter from  http://bullet-ws.development.domain.com:4080/custom-endpoint/api/defaultQuery", 
            "title": "Configuration"
        }, 
        {
            "location": "/ui/usage/", 
            "text": "Navigating the UI\n\n\nThe UI should (hopefully) be self-explanatory. Any particular section that requires additional information has the \n icon next to it. Clicking this will display information relevant to that section. The interactions in this page are running on the topology that was set up in the \nQuick Start\n. Recall that the example backend is configured to produce \n20 data records every 101 ms.\n.\n\n\nLanding page\n\n\nLoading the UI takes you to a page that shows all the queries and past results. You can edit, delete and copy your existing queries here. You can also view or clear your past results for the queries.\n\n\nThe help links you \nconfigure  for the UI\n are shown in the Help menu.\n\n\nSchema\n\n\nThe schema you \nplug into the UI\n is shown here so the users can better understand what the columns mean. Enumerated map fails can be expanded and their nested fields are also described.\n\n\nExample: The landing and schema pages\n\n\n\n  \n\n  Your browser does not support the video tag.\n\n\n\n\n\n\nAbout tables\n\n\nAll tables in the UI, the table for the existing queries, the existing results, the schema or for a query result are all infinite-scroll type tables - they only show a fixed amount but if you scroll to the end, they automatically load more. The tables can all be sorted by clicking on the column name.\n\n\n\n\nA simple first query\n\n\nIf you create a new query, it defaults to getting a raw data record. This query returns immediately even though the maximum duration is set to 20s because the \nQuick Start topology\n produces about 200 records/s and we are looking for one record with no filters.\n\n\nResults\n\n\nSince the entire record was asked to be returned instead of particular fields, the result defaults to a JSON view of the data. You can click the Show as Table button to switch the mode. In this mode, you can click on each cell to get a popover showing the data formatted.\n\n\nYou can also download the results in JSON, CSV or flattened CSV (fields inside maps and lists are exploded). Any metadata returned for the query is collapsed by default. Any relevant metadata for the query \nas configured\n is shown here. As always the help icons display help messages for each section.\n\n\nExample: Picking a random record from the stream\n\n\n\n  \n\n  Your browser does not support the video tag.\n\n\n\n\n\n\n__receive_timestamp\n\n\nThis was enabled as part of the configuration for the example backend. This was the timestamp when Bullet first saw this record. If you have timestamps in your data (as this example does), you will be able to tell exactly when your data was received by Bullet. This coupled with the timestamps in the Result Metadata for when your query was submitted and terminated, you will be able to tell why or why not a particular record was or was not seen in Bullet.\n\n\n\n\nFiltering and projecting data\n\n\nThe Filters section in the UI features a querybuilder (a modified version of the \njQuery-QueryBuilder\n) that you can use to add filters. These allow you to \npick at the slice of data\n from your stream that is relevant to you.\n\n\nThe Output Data section lets you aggregate or choose to see raw data records. You can either get all the data as \nabove\n or you can select a subset of fields (and optionally rename them) that you would like to see.\n\n\nExample: Finding and picking out fields from events that have probability \n 0.5\n\n\n\n  \n\n  Your browser does not support the video tag\n\n\n\n\n\n\nDefault result display\n\n\nIf you choose the Show All Fields selection in the Output Data option, the results will default to the JSON data view. Otherwise, it defaults to the table.\n\n\n\n\nComplex Filtering\n\n\nThe querybuilder also lets you easily create nested filters. You can add rules to add basic relational filters or group a set of filters by connecting them with ANDs and ORs. You can also drag and drop rules and groups.\n\n\nThe querybuilder is also type aware. The operations you can perform change based on the type. Numeric fields only allow numeric values. String fields allow you to apply regular expressions to them or specify multiple values at the same time using a \n,\n. Boolean fields only allow you to choose a radio button etc.\n\n\nExample: Finding and picking out the first and second events in each period that also have probability \n 0.5\n\n\n\n  \n\n  Your browser does not support the video tag.\n\n\n\n\n\n\nWhat's the .* next to a field?\n\n\nIf you have a map that is not enumerated (the keys are not known upfront), there will be \ntwo\n selections for the field in the dropdowns. If you want to apply operations to the nested keys, you can choose the field with the \n.*\n. This will display a free-form subfield selection input where you can specify the key. If you want to apply operations on the entire map, you will need to choose the field without the \n.*\n\n\n\n\nCount Distinct\n\n\nExact\n\n\nThe settings you had \nconfigured when launching\n the backend determines the number of unique values that Bullet can \ncount exactly\n. The example UI shown here used the default configuration value of \n16384\n that the example provided, so for all Count Distinct queries where the cardinality of the field combination is less than this number, the result is exact. The metadata also reflects this.\n\n\nYou can also optionally rename the result.\n\n\nExample: Counting unique UUIDs for 20s\n\n\n\n  \n\n  Your browser does not support the video tag.\n\n\n\n\n\n\nShouldn't the count be slightly more in the last example?\n\n\nShort answer:\n Yes and it's because of the synthetic nature of the data generation.\n\n\nLong answer:\n We should have had \n20000 ms / 101 ms\n or \n198\n periods or \n198 periods * 20 tuples/period\n or \n3960\n tuples with unique values for the\nuuid\n field. The example spout generates data in bursts of 20 at the start of every period (101 ms). However, the delay isn't exactly 101 ms between periods; it's a bit more depending on when Storm decided to run the emission code. As a result, every period will slowly add a delay of a few ms. Eventually, this can lead us to missing an entire period. This increases the longer the query runs. Even a delay of 1 ms every period (a very likely scenario) can add up to 101 ms or 1 period in as short a time as a 101 periods or \n101 periods * 101 ms/period\n or \n~10 s\n. A good rule of thumb is that for every 10 s your query runs, you are missing 20 tuples. You might also miss another 20 tuples at the beginning or the end of the window since the spout is bursty.\n\n\nIn most real streaming scenarios, data should be constantly flowing and there shouldn't delays building like this. Even so, for a distributed, streaming system like Bullet, you should always remember that data can be missed at either end of your query window due to inherent skews and timing issues.\n\n\n\n\n\n\nWhy did the Maximum Records input disappear?\n\n\nMaximum Records as a query stopping criteria only makes sense when you are picking out raw records. While the API still supports using it as a limiting mechanism on the number of records that are returned to you, the UI eschews this and sets it to a value that you can \nconfigure\n. It is also particularly confusing to see a Maximum Records when you are doing a Count Distinct operation, while it makes sense when you are Grouping data. You should ideally set this to the same value as your maximum aggregation size that you configure when launching your backend.\n\n\n\n\nApproximate\n\n\nWhen the result is approximate, it is shown as a decimal value. The Result Metadata section will reflect that the result was estimated and provide you standard deviations for the true value. The errors are derived from \nDataSketches here\n. Note the line for \n16384\n, which was what we configured for the maximum unique values for the Count Distinct operation. In the example below, this means if we want 99.73% confidence for the result, the \n3\n standard deviation entry says that the true count could vary from \n38194\n to \n39590\n.\n\n\nExample: Counting unique UUIDs for 200s\n\n\n\n  \n\n  Your browser does not support the video tag.\n\n\n\n\n\n\nSo why is the approximate count what it is?\n\n\nThe backend should have produced \n20 * 200000/101\n or \n39603\n tuples with unique uuids. Due to the synthetic nature of the data generation and the building delays mentioned above, we estimated that we should subtract about 20 tuples for every 10 s the query runs. Since this query ran for \n200 s\n, this makes the actual uuids generated to be at best \n39603 - (200/10) * 20\n or \n39203\n. The result from Bullet was \n38886\n, which is an error of \n~0.8 %\n. The real error is probably about a \nthird\n of that because we assumed the delay between periods to be 1 ms. It is more on the order of 2 or 3 ms, which makes the number of uuids actually generated even less.\n\n\n\n\nGroup all\n\n\nWhen choosing the Grouped Data option, you can choose to add fields to group by. If you do not and you add metrics, they will apply to all the data that matches your filters (or the whole data set if you don't have any).\n\n\nExample: Counting, summing and averaging on the whole dataset\n\n\nThe metrics you apply on fields are all numeric presently. If you apply a metric on a non-numeric field, Bullet will try to \ntype-cast\n your field into number and if it's not possible, the result will be \nnull\n. The result will also be \nnull\n if the field was not present or no data matched your filters.\n\n\n\n  \n\n  Your browser does not support the video tag.\n\n\n\n\n\n\nErrors when building queries\n\n\nErrors that can be readily displayed are shown immediately. Some errors like the ones in the example above are only shown when you try to run or save the query.\n\n\n\n\n\n\nAre Grouped Data metrics approximate?\n\n\nNo, the results are all exact. See below to see what is approximated when you have too many unique group combinations.\n\n\n\n\nGroup by\n\n\nYou can also choose Group fields and perform metrics per group. If you do not add any Metric fields, you will be \nperforming a distinct operation\n on your group fields.\n\n\nExample: Grouping by tuple_number\n\n\nIn this example, we group by \ntuple_number\n. Recall that this is the number assigned to a tuple within a period. They range from 0 to 19. If we group by this, we expect to have 20 unique groups. In 5s, we have \n5000/101\n or \n49\n periods. Each period has one of each \ntuple_number\n. We expect \n49\n as the count for each group, and this what we see. The building delays mentioned \nin the note above\n has not really started affecting the data yet. Note that the average is also roughly \n0.50\n since the \nprobability\n field is a uniformly distributed value between 0 and 1.\n\n\n\n  \n\n  Your browser does not support the video tag.\n\n\n\n\n\n\nWhat happens if I group by uuid?\n\n\nTry it out! Nothing bad should happen. If the number of unique group values exceeds the \nmaximum configured\n (we used 1024 for this example), you will receive a \nuniform sample\n across your unique group values. The results for your metrics however, are \nnot sampled\n. It is the groups that are sampled on. This means that is \nno\n guarantee of order if you were expecting the \nmost popular\n groups or similar. We are working on adding a \nTOP K\n query that can support these kinds of use-cases.\n\n\n\n\n\n\nWhy no Count Distinct after Grouping\n\n\nAt this time, we do not support counting distinct values per field because with the current implementation of Grouping, it would involve storing DataSketches within DataSketches. We are considering this in a future release however.\n\n\n\n\n\n\nAha, sorting by tuple_number didn't sort properly!\n\n\nGood job, eagle eyes! Unfortunately, whenever we group on fields, those fields become strings under the current implementation. Rather than convert them back at the end, we have currently decided to leave it as is. This means that in your results, if you try and sort by a grouped field, it will perform a lexicographical sort even if it was originally a number.\n\n\nHowever, this also means that you can actually group by any field - including non primitives such as maps and lists! The field will be converted to a string and that string will be used as the field's representation for uniqueness and grouping purposes.", 
            "title": "Usage"
        }, 
        {
            "location": "/ui/usage/#navigating-the-ui", 
            "text": "The UI should (hopefully) be self-explanatory. Any particular section that requires additional information has the   icon next to it. Clicking this will display information relevant to that section. The interactions in this page are running on the topology that was set up in the  Quick Start . Recall that the example backend is configured to produce  20 data records every 101 ms. .", 
            "title": "Navigating the UI"
        }, 
        {
            "location": "/ui/usage/#landing-page", 
            "text": "Loading the UI takes you to a page that shows all the queries and past results. You can edit, delete and copy your existing queries here. You can also view or clear your past results for the queries.  The help links you  configure  for the UI  are shown in the Help menu.", 
            "title": "Landing page"
        }, 
        {
            "location": "/ui/usage/#schema", 
            "text": "The schema you  plug into the UI  is shown here so the users can better understand what the columns mean. Enumerated map fails can be expanded and their nested fields are also described.  Example: The landing and schema pages  \n   \n  Your browser does not support the video tag.   About tables  All tables in the UI, the table for the existing queries, the existing results, the schema or for a query result are all infinite-scroll type tables - they only show a fixed amount but if you scroll to the end, they automatically load more. The tables can all be sorted by clicking on the column name.", 
            "title": "Schema"
        }, 
        {
            "location": "/ui/usage/#a-simple-first-query", 
            "text": "If you create a new query, it defaults to getting a raw data record. This query returns immediately even though the maximum duration is set to 20s because the  Quick Start topology  produces about 200 records/s and we are looking for one record with no filters.", 
            "title": "A simple first query"
        }, 
        {
            "location": "/ui/usage/#results", 
            "text": "Since the entire record was asked to be returned instead of particular fields, the result defaults to a JSON view of the data. You can click the Show as Table button to switch the mode. In this mode, you can click on each cell to get a popover showing the data formatted.  You can also download the results in JSON, CSV or flattened CSV (fields inside maps and lists are exploded). Any metadata returned for the query is collapsed by default. Any relevant metadata for the query  as configured  is shown here. As always the help icons display help messages for each section.  Example: Picking a random record from the stream  \n   \n  Your browser does not support the video tag.   __receive_timestamp  This was enabled as part of the configuration for the example backend. This was the timestamp when Bullet first saw this record. If you have timestamps in your data (as this example does), you will be able to tell exactly when your data was received by Bullet. This coupled with the timestamps in the Result Metadata for when your query was submitted and terminated, you will be able to tell why or why not a particular record was or was not seen in Bullet.", 
            "title": "Results"
        }, 
        {
            "location": "/ui/usage/#filtering-and-projecting-data", 
            "text": "The Filters section in the UI features a querybuilder (a modified version of the  jQuery-QueryBuilder ) that you can use to add filters. These allow you to  pick at the slice of data  from your stream that is relevant to you.  The Output Data section lets you aggregate or choose to see raw data records. You can either get all the data as  above  or you can select a subset of fields (and optionally rename them) that you would like to see.  Example: Finding and picking out fields from events that have probability   0.5  \n   \n  Your browser does not support the video tag   Default result display  If you choose the Show All Fields selection in the Output Data option, the results will default to the JSON data view. Otherwise, it defaults to the table.", 
            "title": "Filtering and projecting data"
        }, 
        {
            "location": "/ui/usage/#complex-filtering", 
            "text": "The querybuilder also lets you easily create nested filters. You can add rules to add basic relational filters or group a set of filters by connecting them with ANDs and ORs. You can also drag and drop rules and groups.  The querybuilder is also type aware. The operations you can perform change based on the type. Numeric fields only allow numeric values. String fields allow you to apply regular expressions to them or specify multiple values at the same time using a  , . Boolean fields only allow you to choose a radio button etc.  Example: Finding and picking out the first and second events in each period that also have probability   0.5  \n   \n  Your browser does not support the video tag.   What's the .* next to a field?  If you have a map that is not enumerated (the keys are not known upfront), there will be  two  selections for the field in the dropdowns. If you want to apply operations to the nested keys, you can choose the field with the  .* . This will display a free-form subfield selection input where you can specify the key. If you want to apply operations on the entire map, you will need to choose the field without the  .*", 
            "title": "Complex Filtering"
        }, 
        {
            "location": "/ui/usage/#count-distinct", 
            "text": "", 
            "title": "Count Distinct"
        }, 
        {
            "location": "/ui/usage/#exact", 
            "text": "The settings you had  configured when launching  the backend determines the number of unique values that Bullet can  count exactly . The example UI shown here used the default configuration value of  16384  that the example provided, so for all Count Distinct queries where the cardinality of the field combination is less than this number, the result is exact. The metadata also reflects this.  You can also optionally rename the result.  Example: Counting unique UUIDs for 20s  \n   \n  Your browser does not support the video tag.   Shouldn't the count be slightly more in the last example?  Short answer:  Yes and it's because of the synthetic nature of the data generation.  Long answer:  We should have had  20000 ms / 101 ms  or  198  periods or  198 periods * 20 tuples/period  or  3960  tuples with unique values for the uuid  field. The example spout generates data in bursts of 20 at the start of every period (101 ms). However, the delay isn't exactly 101 ms between periods; it's a bit more depending on when Storm decided to run the emission code. As a result, every period will slowly add a delay of a few ms. Eventually, this can lead us to missing an entire period. This increases the longer the query runs. Even a delay of 1 ms every period (a very likely scenario) can add up to 101 ms or 1 period in as short a time as a 101 periods or  101 periods * 101 ms/period  or  ~10 s . A good rule of thumb is that for every 10 s your query runs, you are missing 20 tuples. You might also miss another 20 tuples at the beginning or the end of the window since the spout is bursty.  In most real streaming scenarios, data should be constantly flowing and there shouldn't delays building like this. Even so, for a distributed, streaming system like Bullet, you should always remember that data can be missed at either end of your query window due to inherent skews and timing issues.    Why did the Maximum Records input disappear?  Maximum Records as a query stopping criteria only makes sense when you are picking out raw records. While the API still supports using it as a limiting mechanism on the number of records that are returned to you, the UI eschews this and sets it to a value that you can  configure . It is also particularly confusing to see a Maximum Records when you are doing a Count Distinct operation, while it makes sense when you are Grouping data. You should ideally set this to the same value as your maximum aggregation size that you configure when launching your backend.", 
            "title": "Exact"
        }, 
        {
            "location": "/ui/usage/#approximate", 
            "text": "When the result is approximate, it is shown as a decimal value. The Result Metadata section will reflect that the result was estimated and provide you standard deviations for the true value. The errors are derived from  DataSketches here . Note the line for  16384 , which was what we configured for the maximum unique values for the Count Distinct operation. In the example below, this means if we want 99.73% confidence for the result, the  3  standard deviation entry says that the true count could vary from  38194  to  39590 .  Example: Counting unique UUIDs for 200s  \n   \n  Your browser does not support the video tag.   So why is the approximate count what it is?  The backend should have produced  20 * 200000/101  or  39603  tuples with unique uuids. Due to the synthetic nature of the data generation and the building delays mentioned above, we estimated that we should subtract about 20 tuples for every 10 s the query runs. Since this query ran for  200 s , this makes the actual uuids generated to be at best  39603 - (200/10) * 20  or  39203 . The result from Bullet was  38886 , which is an error of  ~0.8 % . The real error is probably about a  third  of that because we assumed the delay between periods to be 1 ms. It is more on the order of 2 or 3 ms, which makes the number of uuids actually generated even less.", 
            "title": "Approximate"
        }, 
        {
            "location": "/ui/usage/#group-all", 
            "text": "When choosing the Grouped Data option, you can choose to add fields to group by. If you do not and you add metrics, they will apply to all the data that matches your filters (or the whole data set if you don't have any).  Example: Counting, summing and averaging on the whole dataset  The metrics you apply on fields are all numeric presently. If you apply a metric on a non-numeric field, Bullet will try to  type-cast  your field into number and if it's not possible, the result will be  null . The result will also be  null  if the field was not present or no data matched your filters.  \n   \n  Your browser does not support the video tag.   Errors when building queries  Errors that can be readily displayed are shown immediately. Some errors like the ones in the example above are only shown when you try to run or save the query.    Are Grouped Data metrics approximate?  No, the results are all exact. See below to see what is approximated when you have too many unique group combinations.", 
            "title": "Group all"
        }, 
        {
            "location": "/ui/usage/#group-by", 
            "text": "You can also choose Group fields and perform metrics per group. If you do not add any Metric fields, you will be  performing a distinct operation  on your group fields.  Example: Grouping by tuple_number  In this example, we group by  tuple_number . Recall that this is the number assigned to a tuple within a period. They range from 0 to 19. If we group by this, we expect to have 20 unique groups. In 5s, we have  5000/101  or  49  periods. Each period has one of each  tuple_number . We expect  49  as the count for each group, and this what we see. The building delays mentioned  in the note above  has not really started affecting the data yet. Note that the average is also roughly  0.50  since the  probability  field is a uniformly distributed value between 0 and 1.  \n   \n  Your browser does not support the video tag.   What happens if I group by uuid?  Try it out! Nothing bad should happen. If the number of unique group values exceeds the  maximum configured  (we used 1024 for this example), you will receive a  uniform sample  across your unique group values. The results for your metrics however, are  not sampled . It is the groups that are sampled on. This means that is  no  guarantee of order if you were expecting the  most popular  groups or similar. We are working on adding a  TOP K  query that can support these kinds of use-cases.    Why no Count Distinct after Grouping  At this time, we do not support counting distinct values per field because with the current implementation of Grouping, it would involve storing DataSketches within DataSketches. We are considering this in a future release however.    Aha, sorting by tuple_number didn't sort properly!  Good job, eagle eyes! Unfortunately, whenever we group on fields, those fields become strings under the current implementation. Rather than convert them back at the end, we have currently decided to leave it as is. This means that in your results, if you try and sort by a grouped field, it will perform a lexicographical sort even if it was originally a number.  However, this also means that you can actually group by any field - including non primitives such as maps and lists! The field will be converted to a string and that string will be used as the field's representation for uniqueness and grouping purposes.", 
            "title": "Group by"
        }, 
        {
            "location": "/about/releases/", 
            "text": "Releases\n\n\nThis sections gathers all the relevant releases of the three components of Bullet in one place. It may not include the very few initial releases of these components if they were largely irrelevant. Full release notes can be found by clicking on the actual releases.\n\n\nBullet is still in active development. We welcome all contributions. Feel free to raise any issues/questions/bugs and whatever else on the relevant issues section for each component. Please include as many details as you can.\n\n\nBullet Storm\n\n\nThe implementation of Bullet on Storm. Due to major API changes between Storm \n= 0.10 and Storm 1.0, Bullet Storm \nbuilds two artifacts\n. The \nartifactId\n changes from \nbullet-storm\n (for 1.0+) to \nbullet-storm-0.10\n.\nAll releases include migration and testing of the code on \nboth\n versions. Both versions are built simultaneously. Feature parity depends on what was new in Storm 1.0. For example, the Resource Aware Scheduler or RAS, is only present in Storm 1.0+. So, bullet-storm-0.10 removes\ncertain CPU and memory related settings specific to RAS in its configuration. There are also minor changes to the Metrics API in Storm. In terms of Bullet itself, there should be no differences.\n\n\n\n\nFuture support\n\n\nWe will support Storm 0.10 for a bit longer till Storm 2.0 is up and stable. Storm 1.0+ have a lot of performance fixes and features that you should be running with.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nStorm-1.0+ Repository\n\n\nhttps://github.com/yahoo/bullet-storm\n\n\n\n\n\n\nStorm-0.10- Repository\n\n\nhttps://github.com/yahoo/bullet-storm/tree/storm-0.10\n\n\n\n\n\n\nIssues\n\n\nhttps://github.com/yahoo/bullet-storm/issues\n\n\n\n\n\n\nLast Tag\n\n\n\n\n\n\n\n\nLatest Artifact\n\n\n\n\n\n\n\n\n\n\nReleases\n\n\n\n\n\n\n\n\nDate\n\n\nStorm 1.0\n\n\nStorm 0.10\n\n\nHighlights\n\n\n\n\n\n\n\n\n\n\n2017-03-13\n\n\n0.3.1\n\n\n0.3.1\n\n\nExtra records accepted after query expiry bug fix\n\n\n\n\n\n\n2017-02-27\n\n\n0.3.0\n\n\n0.3.0\n\n\nMetrics interface, config namespace, NPE bug fix\n\n\n\n\n\n\n2017-02-15\n\n\n0.2.1\n\n\n0.2.1\n\n\nAcking support, Max size and other bug fixes\n\n\n\n\n\n\n2017-01-26\n\n\n0.2.0\n\n\n0.2.0\n\n\nGROUP (DISTINCT, SUM, COUNT, MIN, MAX, AVG)\n\n\n\n\n\n\n2017-01-09\n\n\n0.1.0\n\n\n0.1.0\n\n\nCOUNT DISTINCT and micro-batching\n\n\n\n\n\n\n\n\nBullet Web Service\n\n\nThe Web Service implementation that can serve a static schema from a file and talk to the Storm backend.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRepository\n\n\nhttps://github.com/yahoo/bullet-service\n\n\n\n\n\n\nIssues\n\n\nhttps://github.com/yahoo/bullet-service/issues\n\n\n\n\n\n\nLast Tag\n\n\n\n\n\n\n\n\nLatest Artifact\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDate\n\n\nRelease\n\n\nHighlights\n\n\n\n\n\n\n\n\n\n\n2016-12-16\n\n\n0.0.1\n\n\nThe first release with support for DRPC and the file-based schema\n\n\n\n\n\n\n\n\nBullet UI\n\n\nThe Bullet UI that lets you build, run, save and visualize results from Bullet.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRepository\n\n\nhttps://github.com/yahoo/bullet-ui\n\n\n\n\n\n\nIssues\n\n\nhttps://github.com/yahoo/bullet-ui/issues\n\n\n\n\n\n\nLast Tag\n\n\n\n\n\n\n\n\nLatest Artifact\n\n\n\n\n\n\n\n\n\n\nReleases\n\n\n\n\n\n\n\n\nDate\n\n\nRelease\n\n\nHighlights\n\n\n\n\n\n\n\n\n\n\n2016-02-21\n\n\n0.1.0\n\n\nThe first release with support for all features included in Bullet Storm 0.2.1+\n\n\n\n\n\n\n\n\nBullet Record\n\n\nThe AVRO container that you need to convert your data into to be consumed by Bullet.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRepository\n\n\nhttps://github.com/yahoo/bullet-record\n\n\n\n\n\n\nIssues\n\n\nhttps://github.com/yahoo/bullet-record/issues\n\n\n\n\n\n\nLast Tag\n\n\n\n\n\n\n\n\nLatest Artifact\n\n\n\n\n\n\n\n\n\n\nReleases\n\n\n\n\n\n\n\n\nDate\n\n\nRelease\n\n\nHighlights\n\n\n\n\n\n\n\n\n\n\n2017-02-09\n\n\n0.1.0\n\n\nMap constructor", 
            "title": "Past Releases"
        }, 
        {
            "location": "/about/releases/#releases", 
            "text": "This sections gathers all the relevant releases of the three components of Bullet in one place. It may not include the very few initial releases of these components if they were largely irrelevant. Full release notes can be found by clicking on the actual releases.  Bullet is still in active development. We welcome all contributions. Feel free to raise any issues/questions/bugs and whatever else on the relevant issues section for each component. Please include as many details as you can.", 
            "title": "Releases"
        }, 
        {
            "location": "/about/releases/#bullet-storm", 
            "text": "The implementation of Bullet on Storm. Due to major API changes between Storm  = 0.10 and Storm 1.0, Bullet Storm  builds two artifacts . The  artifactId  changes from  bullet-storm  (for 1.0+) to  bullet-storm-0.10 .\nAll releases include migration and testing of the code on  both  versions. Both versions are built simultaneously. Feature parity depends on what was new in Storm 1.0. For example, the Resource Aware Scheduler or RAS, is only present in Storm 1.0+. So, bullet-storm-0.10 removes\ncertain CPU and memory related settings specific to RAS in its configuration. There are also minor changes to the Metrics API in Storm. In terms of Bullet itself, there should be no differences.   Future support  We will support Storm 0.10 for a bit longer till Storm 2.0 is up and stable. Storm 1.0+ have a lot of performance fixes and features that you should be running with.            Storm-1.0+ Repository  https://github.com/yahoo/bullet-storm    Storm-0.10- Repository  https://github.com/yahoo/bullet-storm/tree/storm-0.10    Issues  https://github.com/yahoo/bullet-storm/issues    Last Tag     Latest Artifact", 
            "title": "Bullet Storm"
        }, 
        {
            "location": "/about/releases/#releases_1", 
            "text": "Date  Storm 1.0  Storm 0.10  Highlights      2017-03-13  0.3.1  0.3.1  Extra records accepted after query expiry bug fix    2017-02-27  0.3.0  0.3.0  Metrics interface, config namespace, NPE bug fix    2017-02-15  0.2.1  0.2.1  Acking support, Max size and other bug fixes    2017-01-26  0.2.0  0.2.0  GROUP (DISTINCT, SUM, COUNT, MIN, MAX, AVG)    2017-01-09  0.1.0  0.1.0  COUNT DISTINCT and micro-batching", 
            "title": "Releases"
        }, 
        {
            "location": "/about/releases/#bullet-web-service", 
            "text": "The Web Service implementation that can serve a static schema from a file and talk to the Storm backend.           Repository  https://github.com/yahoo/bullet-service    Issues  https://github.com/yahoo/bullet-service/issues    Last Tag     Latest Artifact         Date  Release  Highlights      2016-12-16  0.0.1  The first release with support for DRPC and the file-based schema", 
            "title": "Bullet Web Service"
        }, 
        {
            "location": "/about/releases/#bullet-ui", 
            "text": "The Bullet UI that lets you build, run, save and visualize results from Bullet.           Repository  https://github.com/yahoo/bullet-ui    Issues  https://github.com/yahoo/bullet-ui/issues    Last Tag     Latest Artifact", 
            "title": "Bullet UI"
        }, 
        {
            "location": "/about/releases/#releases_2", 
            "text": "Date  Release  Highlights      2016-02-21  0.1.0  The first release with support for all features included in Bullet Storm 0.2.1+", 
            "title": "Releases"
        }, 
        {
            "location": "/about/releases/#bullet-record", 
            "text": "The AVRO container that you need to convert your data into to be consumed by Bullet.           Repository  https://github.com/yahoo/bullet-record    Issues  https://github.com/yahoo/bullet-record/issues    Last Tag     Latest Artifact", 
            "title": "Bullet Record"
        }, 
        {
            "location": "/about/releases/#releases_3", 
            "text": "Date  Release  Highlights      2017-02-09  0.1.0  Map constructor", 
            "title": "Releases"
        }, 
        {
            "location": "/about/contributing/", 
            "text": "Contributing\n\n\nWe welcome all contributions! We also welcome all usage experiences, stories, annoyances and whatever else you want to say. Head on over to our \nContact Us page\n and let us know!\n\n\nContributor License Agreement (CLA)\n\n\nBullet is hosted under the \nYahoo Github Organization\n. In order to contribute to any Yahoo project, you will need to submit a CLA. When you submit a Pull Request to any Bullet repository, a CLABot will ask  you to sign the CLA if you haven't signed one already.\n\n\nFuture plans\n\n\nHere is a list of features we are currently considering/working on. If the Status column is empty, we are still discussing how to prioritize/approach/break them down. They will be updated as they are solidified. Feel free to \ncontact us\n with any ideas/suggestions/PRs!\n\n\nThis list is neither comprehensive nor in any particular order.\n\n\n\n\n\n\n\n\nFeature\n\n\nComponents\n\n\nDescription\n\n\nStatus\n\n\n\n\n\n\n\n\n\n\nTOP K\n\n\nUI, BE\n\n\nA TOP K implementation using DataSketches: FrequentItems\n\n\nIn progress\n\n\n\n\n\n\nDISTRIBUTION\n\n\nUI, BE\n\n\nA query to get the distribution/quantiles of a numeric field using DataSketches: Quantiles\n\n\nIn progress\n\n\n\n\n\n\nPub-Sub Queue\n\n\nBE, WS, UI\n\n\nWS and BE talk through the pub/sub. Bullet Storm uses Storm DRPC for this, which is strictly request-response. This will let us work on other Stream Processors and support incremental updates through WebSockets or SSEs\n\n\n\n\n\n\n\n\nIncremental updates\n\n\nBE, WS, UI\n\n\nPush results back to users as soon as they arrive. Monoidal operations implies additive, so progressive results can be streamed back. Micro-batching and other features come into play\n\n\n\n\n\n\n\n\nSQL API\n\n\nBE, WS\n\n\nWS supports an endpoint that converts a SQL-like query into Bullet queries\n\n\n\n\n\n\n\n\nLocalForage\n\n\nUI\n\n\nMigration to LocalForage to distance ourselves from the relatively small LocalStorage space\n\n\n#9\n\n\n\n\n\n\nUI Packaging\n\n\nUI\n\n\nGithub releases and building from source are the only two options. Docker or something similar may be more apt\n\n\n\n\n\n\n\n\nSimple Settings\n\n\nUI, WS\n\n\nThere are several settings in the UI and WS that are directly tied to the BE. They should be configurable and optimally configurable from one location", 
            "title": "Contributing"
        }, 
        {
            "location": "/about/contributing/#contributing", 
            "text": "We welcome all contributions! We also welcome all usage experiences, stories, annoyances and whatever else you want to say. Head on over to our  Contact Us page  and let us know!", 
            "title": "Contributing"
        }, 
        {
            "location": "/about/contributing/#contributor-license-agreement-cla", 
            "text": "Bullet is hosted under the  Yahoo Github Organization . In order to contribute to any Yahoo project, you will need to submit a CLA. When you submit a Pull Request to any Bullet repository, a CLABot will ask  you to sign the CLA if you haven't signed one already.", 
            "title": "Contributor License Agreement (CLA)"
        }, 
        {
            "location": "/about/contributing/#future-plans", 
            "text": "Here is a list of features we are currently considering/working on. If the Status column is empty, we are still discussing how to prioritize/approach/break them down. They will be updated as they are solidified. Feel free to  contact us  with any ideas/suggestions/PRs!  This list is neither comprehensive nor in any particular order.     Feature  Components  Description  Status      TOP K  UI, BE  A TOP K implementation using DataSketches: FrequentItems  In progress    DISTRIBUTION  UI, BE  A query to get the distribution/quantiles of a numeric field using DataSketches: Quantiles  In progress    Pub-Sub Queue  BE, WS, UI  WS and BE talk through the pub/sub. Bullet Storm uses Storm DRPC for this, which is strictly request-response. This will let us work on other Stream Processors and support incremental updates through WebSockets or SSEs     Incremental updates  BE, WS, UI  Push results back to users as soon as they arrive. Monoidal operations implies additive, so progressive results can be streamed back. Micro-batching and other features come into play     SQL API  BE, WS  WS supports an endpoint that converts a SQL-like query into Bullet queries     LocalForage  UI  Migration to LocalForage to distance ourselves from the relatively small LocalStorage space  #9    UI Packaging  UI  Github releases and building from source are the only two options. Docker or something similar may be more apt     Simple Settings  UI, WS  There are several settings in the UI and WS that are directly tied to the BE. They should be configurable and optimally configurable from one location", 
            "title": "Future plans"
        }, 
        {
            "location": "/about/contact/", 
            "text": "Contact Us\n\n\nIssues\n\n\nIf you have any issues with any of the particular Bullet sub-components, feel free to create issues.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nStorm\n\n\nhttps://github.com/yahoo/bullet-storm/issues\n\n\n\n\n\n\nWeb Service\n\n\nhttps://github.com/yahoo/bullet-service/issues\n\n\n\n\n\n\nUI\n\n\nhttps://github.com/yahoo/bullet-ui/issues\n\n\n\n\n\n\nRecord\n\n\nhttps://github.com/yahoo/bullet-record/issues\n\n\n\n\n\n\nDocumentation\n\n\nhttps://github.com/yahoo/bullet-docs/issues\n\n\n\n\n\n\n\n\nMailing Lists\n\n\nIf you have a general question, comment, or observation meant for general visibility, reach out to the Users list. If you want to keep it to just the developers, reach to that list instead.\n\n\n\n\n\n\n\n\n\n\nMail\n\n\nForum\n\n\n\n\n\n\n\n\n\n\nUsers\n\n\nbullet-users@googlegroups.com\n\n\nhttps://groups.google.com/d/forum/bullet-users\n\n\n\n\n\n\nDevelopers\n\n\nbullet-dev@googlegroups.com\n\n\nhttps://groups.google.com/d/forum/bullet-dev", 
            "title": "Contact Us"
        }, 
        {
            "location": "/about/contact/#contact-us", 
            "text": "", 
            "title": "Contact Us"
        }, 
        {
            "location": "/about/contact/#issues", 
            "text": "If you have any issues with any of the particular Bullet sub-components, feel free to create issues.           Storm  https://github.com/yahoo/bullet-storm/issues    Web Service  https://github.com/yahoo/bullet-service/issues    UI  https://github.com/yahoo/bullet-ui/issues    Record  https://github.com/yahoo/bullet-record/issues    Documentation  https://github.com/yahoo/bullet-docs/issues", 
            "title": "Issues"
        }, 
        {
            "location": "/about/contact/#mailing-lists", 
            "text": "If you have a general question, comment, or observation meant for general visibility, reach out to the Users list. If you want to keep it to just the developers, reach to that list instead.      Mail  Forum      Users  bullet-users@googlegroups.com  https://groups.google.com/d/forum/bullet-users    Developers  bullet-dev@googlegroups.com  https://groups.google.com/d/forum/bullet-dev", 
            "title": "Mailing Lists"
        }
    ]
}